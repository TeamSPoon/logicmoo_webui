<!DOCTYPE html>
<!-- saved from url=(0030)https://worldmodels.github.io/ -->
<html lang="en" class="gr__worldmodels_github_io"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<style>
body {
  overflow-x: hidden;
}
#cover_sketch {padding: 0; 
    margin: 0;
}
#cover_overlay_wrap {padding: 0; 
    margin: 0;
    position: absolute;
    top: 0;
    left: 0;
}
#cover_overlay {padding: 0; 
    margin: 0;
    position: absolute;
    top: 0;
    left: 0;
    background: #000000;
    opacity: 0.2;
    width: 100%;
    height: 100%;
}
#cover_title {padding: 0; 
    margin: 0;
    position: absolute;
    top: 0;
    left: 0;
}
#cover_title_svg {padding: 0; 
    margin: 0;
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
}
#loading_text {
  margin: 0;
  top: 0;
  left: 0;
  height: 100vh;
  width: 100vw;
}
.cover-instruction {
  width: 100%;
  height: 60px;
  bottom: 10%;
  position: absolute;
  font-family: "Roboto","Helvetica Neue",Helvetica,Arial,sans-serif;
  font-size: 16px;
  font-weight: 300;
  color: #FFFFFF;
}
.scroll-down {
  width: 80px;
  height: 40px;
  right: 10px;
  bottom: 10px;
  position: absolute;
  font-family: "Roboto","Helvetica Neue",Helvetica,Arial,sans-serif;
  font-size: 12px;
  font-weight: 300;
  color: #FFFFFF;
  opacity: 0;
  -webkit-transition: opacity 2s ease-in;
  -moz-transition: opacity 2s ease-in;
  -o-transition: opacity 2s ease-in;
  -ms-transition: opacity 2s ease-in;
  transition: opacity 2s ease-in;
}
.scroll-down span {
  margin-top: 5px;
  position: absolute;
  left: 50%;
  transform: translate(-100%, 0) rotate(45deg);
  transform-origin: 100% 100%;
  height: 2px;
  width: 10px;
  background: #FFFFFF;
}
.scroll-down span:nth-of-type(2) {
  transform-origin: 0 100%;
  transform: translate(0, 0) rotate(-45deg);
}
.spinner {
  position: absolute;
  height: 160px;
  width: 160px;
  -webkit-animation: rotation .6s infinite linear;
  -moz-animation: rotation .6s infinite linear;
  -o-animation: rotation .6s infinite linear;
  animation: rotation .6s infinite linear;
  border-left: 6px solid rgba(0, 174, 239, .15);
  border-right: 6px solid rgba(0, 174, 239, .15);
  border-bottom: 6px solid rgba(0, 174, 239, .15);
  border-top: 6px solid rgba(0, 174, 239, .8);
  border-radius: 100%;
  top: calc(50% - 100px);
  left: calc(50% - 80px);
  right: auto;
  bottom: auto;
}

@-webkit-keyframes rotation {
  from {
    -webkit-transform: rotate(0deg);
  }
  to {
    -webkit-transform: rotate(359deg);
  }
}
.transparent {
  opacity: 0;
}

.figload {
  font-family: Helvetica,Arial,sans-serif;
  font-weight: 400;
  color: rgba(0, 174, 239, .8);
  font-size: 24px;
  line-height: 1.5em;
  display: block;
  width: 100%;
  text-align: center;
  position: absolute;
  top: calc(50% - 80px + 190px);
}

dt-article figcaption {
  padding: 0.5em;
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
  text-align: left;
}

dt-article figcaption a {
  color: rgba(0, 0, 0, 0.6);
}

dt-article figcaption b {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

*.unselectable {
    -moz-user-select: -moz-none;
    -khtml-user-select: none;
    -webkit-user-select: none;
    -o-user-select: none;
    user-select: none;
}
*.svgunselectable {
    -moz-user-select: -moz-none;
    -khtml-user-select: none;
    -webkit-user-select: none;
    -o-user-select: none;
    user-select: none;
    background: none;
    pointer-events: none;
}
</style>

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <!-- roboto font -->
  <link href="./World Models_files/css" rel="stylesheet" type="text/css">
  <!-- icons -->
  <link rel="apple-touch-icon" sizes="57x57" href="https://worldmodels.github.io/logo/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="https://worldmodels.github.io/logo/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="https://worldmodels.github.io/logo/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="https://worldmodels.github.io/logo/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="https://worldmodels.github.io/logo/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="https://worldmodels.github.io/logo/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="https://worldmodels.github.io/logo/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="https://worldmodels.github.io/logo/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="https://worldmodels.github.io/logo/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192" href="https://worldmodels.github.io/logo/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="https://worldmodels.github.io/logo/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="https://worldmodels.github.io/logo/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="https://worldmodels.github.io/logo/favicon-16x16.png">
  <link rel="manifest" href="https://worldmodels.github.io/logo/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="logo/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script type="text/javascript" async="" src="./World Models_files/analytics.js.download"></script><script async="" src="./World Models_files/js"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-116311758-1');
  </script>
  <!-- SEO -->
  <meta property="og:title" content="World Models">
  <meta property="og:type" content="article">
  <meta property="og:description" content="Can agents learn inside of their own dreams?">
  <meta property="og:image" content="https://worldmodels.github.io/assets/world_models_card_both.png">
  <meta property="og:url" content="https://worldmodels.github.io/">
  <!-- Twitter Card data -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="World Models">
  <meta name="twitter:description" content="Can agents learn inside of their own dreams?">
  <meta property="og:site_name" content="World Models">
  <meta name="twitter:image" content="https://worldmodels.github.io/assets/world_models_card_single.png">
    <!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
  <meta name="citation_title" content="World Models">
  <meta name="citation_doi" content="10.5281/zenodo.1207631">
  <meta name="citation_volume" content="1">
  <meta name="citation_issue" content="1">
  <meta name="citation_firstpage" content="e10">
  <meta name="citation_fulltext_world_readable" content="">
  <meta name="citation_fulltext_html_url" content="https://worldmodels.github.io/">
  <meta name="citation_online_date" content="2018/03/27">
  <meta name="citation_publication_date" content="2018/03/27">
  <meta name="citation_author" content="Ha, David">
  <meta name="citation_author_institution" content="Google Brain">
  <meta name="citation_author" content="Schmidhuber, Jürgen">
  <meta name="citation_author_institution" content="NNAISENSE">
  <meta name="citation_journal_title" content="World Models">
  <meta name="citation_journal_abbrev" content="World Models">
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/1803.10122.pdf">
<link rel="stylesheet" href="./World Models_files/katex.min.css"><script src="./World Models_files/jquery-1.12.4.min.js.download"></script><script src="./World Models_files/mobile-detect.min.js.download"></script><script src="./World Models_files/template.v1.js.download"></script><style>html {
  font: 400 16px/1.55em -apple-system, BlinkMacSystemFont, "Roboto", Helvetica, sans-serif;
  /*background-color: hsl(223, 9%, 25%);*/
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
  text-size-adjust: 100%;
}
body {
  margin: 0;
  /*background-color: hsl(223, 9%, 25%);*/
}

a {
  color: #004276;
}

figure {
  margin: 0;
}

h1 {
  font-family: Cochin, Georgia, serif;
}

/*
html, body, div, span, applet, object, iframe,
h1, h2, h3, h4, h5, h6, p, blockquote, pre,
a, abbr, acronym, address, big, cite, code,
del, dfn, em, img, ins, kbd, q, s, samp,
small, strike, strong, sub, sup, tt, var,
b, u, i, center,
dl, dt, dd, ol, ul, li,
fieldset, form, label, legend,
table, caption, tbody, tfoot, thead, tr, th, td,
article, aside, canvas, details, embed,
figure, figcaption, footer, header, hgroup,
menu, nav, output, ruby, section, summary,
time, mark, audio, video {
    margin: 0;
    padding: 0;
    border: 0;
    font-size: 100%;
    font: inherit;
    vertical-align: baseline;
}
article, aside, details, figcaption, figure,
footer, header, hgroup, menu, nav, section {
    display: block;
}
body {
    line-height: 1;
}
ol, ul {
    list-style: none;
}
blockquote, q {
    quotes: none;
}
blockquote:before, blockquote:after,
q:before, q:after {
    content: '';
    content: none;
}
table {
    border-collapse: collapse;
    border-spacing: 0;
}*/
/*
  Column: 60px
  Gutter: 24px

  Body: 648px
    - 8 columns
    - 7 gutters
  Middle: 816px
  Page: 984px
    - 12 columns
    - 11 gutters
*/

.l-body,
.l-body-outset,
.l-page,
.l-page-outset,
.l-middle,
.l-middle-outset,
dt-article > div,
dt-article > p,
dt-article > h1,
dt-article > h2,
dt-article > h3,
dt-article > h4,
dt-article > figure,
dt-article > table,
dt-article > ol,
dt-article > ul,
dt-article > dt-byline,
dt-article > dt-math,
dt-article > dt-code,
dt-article section > div,
dt-article section > p,
dt-article section > h1,
dt-article section > h2,
dt-article section > h3,
dt-article section > h4,
dt-article section > figure,
dt-article section > table,
dt-article section > ol,
dt-article section > ul,
dt-article section > dt-byline,
dt-article section > dt-code {
  width: auto;
  margin-left: 24px;
  margin-right: 24px;
  box-sizing: border-box;
}

@media(min-width: 768px) {
  .l-body,
  .l-body-outset,
  .l-page,
  .l-page-outset,
  .l-middle,
  .l-middle-outset,
  dt-article > div,
  dt-article > p,
  dt-article > h1,
  dt-article > h2,
  dt-article > h3,
  dt-article > h4,
  dt-article > figure,
  dt-article > table,
  dt-article > ol,
  dt-article > ul,
  dt-article > dt-byline,
  dt-article > dt-math,
  dt-article > dt-code,
  dt-article section > div,
  dt-article section > p,
  dt-article section > h1,
  dt-article section > h2,
  dt-article section > h3,
  dt-article section > h4,
  dt-article section > figure,
  dt-article section > table,
  dt-article section > ol,
  dt-article section > ul,
  dt-article section > dt-byline,
  dt-article section > dt-code {
    margin-left: 72px;
    margin-right: 72px;
  }
}

@media(min-width: 1080px) {
  .l-body,
  dt-article > div,
  dt-article > p,
  dt-article > h2,
  dt-article > h3,
  dt-article > h4,
  dt-article > figure,
  dt-article > table,
  dt-article > ol,
  dt-article > ul,
  dt-article > dt-byline,
  dt-article > dt-math,
  dt-article > dt-code,
  dt-article section > div,
  dt-article section > p,
  dt-article section > h2,
  dt-article section > h3,
  dt-article section > h4,
  dt-article section > figure,
  dt-article section > table,
  dt-article section > ol,
  dt-article section > ul,
  dt-article section > dt-byline,
  dt-article section > dt-code {
    margin-left: calc(50% - 984px / 2);
    width: 648px;
  }
  .l-body-outset,
  dt-article .l-body-outset {
    margin-left: calc(50% - 984px / 2 - 96px/2);
    width: calc(648px + 96px);
  }
  .l-middle,
  dt-article .l-middle {
    width: 816px;
    margin-left: calc(50% - 984px / 2);
    margin-right: auto;
  }
  .l-middle-outset,
  dt-article .l-middle-outset {
    width: calc(816px + 96px);
    margin-left: calc(50% - 984px / 2 - 48px);
    margin-right: auto;
  }
  dt-article > h1,
  dt-article section > h1,
  .l-page,
  dt-article .l-page,
  dt-article.centered .l-page {
    width: 984px;
    margin-left: auto;
    margin-right: auto;
  }
  .l-page-outset,
  dt-article .l-page-outset,
  dt-article.centered .l-page-outset {
    width: 1080px;
    margin-left: auto;
    margin-right: auto;
  }
  .l-screen,
  dt-article .l-screen,
  dt-article.centered .l-screen {
    margin-left: auto;
    margin-right: auto;
    width: auto;
  }
  .l-screen-inset,
  dt-article .l-screen-inset,
  dt-article.centered .l-screen-inset {
    margin-left: 24px;
    margin-right: 24px;
    width: auto;
  }
  .l-gutter,
  dt-article .l-gutter {
    clear: both;
    float: right;
    margin-top: 0;
    margin-left: 24px;
    margin-right: calc((100vw - 984px) / 2 + 168px);
    width: calc((984px - 648px) / 2 - 24px);
  }

  /* Side */
  .side.l-body,
  dt-article .side.l-body {
    clear: both;
    float: right;
    margin-top: 0;
    margin-left: 48px;
    margin-right: calc((100vw - 984px + 648px) / 2);
    width: calc(648px / 2 - 24px - 84px);
  }
  .side.l-body-outset,
  dt-article .side.l-body-outset {
    clear: both;
    float: right;
    margin-top: 0;
    margin-left: 48px;
    margin-right: calc((100vw - 984px + 648px - 48px) / 2);
    width: calc(648px / 2 - 48px + 24px);
  }
  .side.l-middle,
  dt-article .side.l-middle {
    clear: both;
    float: right;
    width: calc(456px - 84px);
    margin-left: 48px;
    margin-right: calc((100vw - 984px) / 2 + 168px);
  }
  .side.l-middle-outset,
  dt-article .side.l-middle-outset {
    clear: both;
    float: right;
    width: 456px;
    margin-left: 48px;
    margin-right: calc((100vw - 984px) / 2 + 168px);
  }
  .side.l-page,
  dt-article .side.l-page {
    clear: both;
    float: right;
    margin-left: 48px;
    width: calc(624px - 84px);
    margin-right: calc((100vw - 984px) / 2);
  }
  .side.l-page-outset,
  dt-article .side.l-page-outset {
    clear: both;
    float: right;
    width: 624px;
    margin-right: calc((100vw - 984px) / 2);
  }
}

/* Centered */

@media(min-width: 1080px) {
  .centered .l-body,
  .centered.l-body,
  dt-article.centered > div,
  dt-article.centered > p,
  dt-article.centered > h2,
  dt-article.centered > h3,
  dt-article.centered > h4,
  dt-article.centered > figure,
  dt-article.centered > table,
  dt-article.centered > ol,
  dt-article.centered > ul,
  dt-article.centered > dt-byline,
  dt-article.centered > dt-code,
  dt-article.centered section > div,
  dt-article.centered section > p,
  dt-article.centered section > h2,
  dt-article.centered section > h3,
  dt-article.centered section > h4,
  dt-article.centered section > figure,
  dt-article.centered section > table,
  dt-article.cebtered section > ol,
  dt-article.centered section > ul,
  dt-article.centered section > dt-byline,
  dt-article.centered section > dt-code,
  dt-article section.centered > div,
  dt-article section.centered > p,
  dt-article section.centered > h2,
  dt-article section.centered > h3,
  dt-article section.centered > h4,
  dt-article section.centered > figure,
  dt-article section.centered > table,
  dt-article section.centered > ol,
  dt-article section.centered > ul,
  dt-article section.centered > dt-byline,
  dt-article section.centered > dt-code {
    margin-left: auto;
    margin-right: auto;
    width: 648px;
  }
  .centered .l-body-outset,
  .centered.l-body-outset,
  dt-article.centered .l-body-outset {
    margin-left: auto;
    margin-right: auto;
    width: calc(648px + 96px);
  }
  dt-article.centered > h1,
  dt-article.centered section > h1,
  dt-article section.centered > h1,
  .centered .l-middle,
  .centered.l-middle,
  dt-article.centered .l-middle {
    width: 816px;
    margin-left: auto;
    margin-right: auto;
  }

  .centered .l-middle-outset,
  .centered.l-middle-outset,
  dt-article.centered .l-middle-outset {
    width: calc(816px + 96px);
    margin-left: auto;
    margin-right: auto;
  }

  /* page and screen are already centered */

  /* Side */

  .centered .side.l-body,
  .centered dt-article .side.l-body {
    clear: both;
    float: right;
    margin-top: 0;
    margin-left: 48px;
    margin-right: calc((100vw - 648px) / 2);
    width: calc(4 * 60px + 3 * 24px);
  }
  .centered .side.l-body-outset,
  .centered dt-article .side.l-body-outset {
    clear: both;
    float: right;
    margin-top: 0;
    margin-left: 48px;
    margin-right: calc((100vw - 648px) / 2);
    width: calc(4 * 60px + 3 * 24px);
  }
  .centered .side.l-middle,
  .centered dt-article .side.l-middle {
    clear: both;
    float: right;
    width: 396px;
    margin-left: 48px;
    margin-right: calc((100vw - 984px) / 2 + 168px / 2);
  }
  .centered .side.l-middle-outset,
  .centered dt-article .side.l-middle-outset {
    clear: both;
    float: right;
    width: 456px;
    margin-left: 48px;
    margin-right: calc((100vw - 984px) / 2 + 168px);
  }
  .centered .side.l-page,
  .centered dt-article .side.l-page {
    clear: both;
    float: right;
    width: 480px;
    margin-right: calc((100vw - 984px) / 2);
  }
  .centered .side.l-page-outset,
  .centered dt-article .side.l-page-outset {
    clear: both;
    float: right;
    width: 480px;
    margin-right: calc((100vw - 984px) / 2);
  }
  .centered .l-gutter,
  .centered.l-gutter,
  dt-article.centered .l-gutter {
    clear: both;
    float: right;
    margin-top: 0;
    margin-left: 24px;
    margin-right: calc((100vw - 984px) / 2);
    width: calc((984px - 648px) / 2 - 24px);
  }

}

/* Rows and Columns */

.row {
  display: flex;
}
.column {
  flex: 1;
  box-sizing: border-box;
  margin-right: 24px;
  margin-left: 24px;
}
.row > .column:first-of-type {
  margin-left: 0;
}
.row > .column:last-of-type {
  margin-right: 0;
}
dt-article {
  display: block;
  color: rgba(0, 0, 0, 0.8);
  font: 17px/1.55em -apple-system, BlinkMacSystemFont, "Roboto", sans-serif;
  padding-bottom: 72px;
  background: white;
}

@media(min-width: 1024px) {
  dt-article {
    font-size: 20px;
  }
}

/* H1 */

dt-article h1 {
  margin-top: 18px;
  font-weight: 400;
  font-size: 40px;
  line-height: 1em;
  font-family: HoeflerText-Regular, Cochin, Georgia, serif;
}
@media(min-width: 768px) {
  dt-article h1 {
    font-size: 46px;
    margin-top: 48px;
    margin-bottom: 12px;
  }
}

@media(min-width: 1080px) {
  .centered h1 {
    text-align: center;
  }

  dt-article h1 {
    font-size: 50px;
    letter-spacing: -0.02em;
  }

  dt-article > h1:first-of-type,
  dt-article section > h1:first-of-type {
    margin-top: 80px;
  }
}


@media(min-width: 1200px) {
  dt-article h1 {
    font-size: 56px;
  }

  dt-article > h1:first-of-type {
    margin-top: 100px;
  }
}

/* H2 */

dt-article h2 {
  font-family: HoeflerText-Regular, Cochin, Georgia, serif;
  font-weight: 400;
  font-size: 26px;
  line-height: 1.25em;
  margin-top: 36px;
  margin-bottom: 24px;
}

@media(min-width: 1024px) {
  dt-article h2 {
    margin-top: 48px;
    font-size: 30px;
  }
}

dt-article h1 + h2 {
  font-weight: 300;
  font-size: 20px;
  line-height: 1.4em;
  margin-top: 8px;
  font-style: normal;
}


@media(min-width: 1080px) {
  .centered h1 + h2 {
    text-align: center;
  }
  dt-article h1 + h2 {
    margin-top: 12px;
    font-size: 24px;
  }
}

/* H3 */

dt-article h3 {
  font-family: HoeflerText-Regular, Georgia, serif;
  font-weight: 400;
  font-size: 20px;
  line-height: 1.4em;
  margin-top: 36px;
  margin-bottom: 18px;
  font-style: italic;
}

dt-article h1 + h3 {
  margin-top: 48px;
}

@media(min-width: 1024px) {
  dt-article h3 {
    font-size: 26px;
  }
}

/* H4 */

dt-article h4 {
  font-weight: 600;
  text-transform: uppercase;
  font-size: 14px;
  line-height: 1.4em;
}

dt-article a {
  color: inherit;
}

dt-article p,
dt-article ul,
dt-article ol {
  margin-bottom: 24px;
  font-family: Georgia, serif;
}

dt-article p b,
dt-article ul b,
dt-article ol b {
  -webkit-font-smoothing: antialiased;
}

dt-article a {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  text-decoration: none;
}

dt-article a:hover {
  border-bottom: 1px solid rgba(0, 0, 0, 0.8);
}

dt-article .link {
  text-decoration: underline;
  cursor: pointer;
}

dt-article ul,
dt-article ol {
  padding-left: 24px;
}

dt-article li {
  margin-bottom: 24px;
  margin-left: 0;
  padding-left: 0;
}

dt-article pre {
  font-size: 14px;
  margin-bottom: 20px;
}


dt-article hr {
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
  margin-top: 60px;
  margin-bottom: 60px;
}

dt-article section {
  margin-top: 60px;
  margin-bottom: 60px;
}

/* Tables */

dt-article table {
  border-collapse: collapse;
}

dt-article table th {
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

dt-article table td {
  border-bottom: 1px solid rgba(0, 0, 0, 0.05);
}

dt-article table th,
dt-article table td {
  font-size: 15px;
  padding: 2px 0;
}

/* Figure */

dt-article figure {
  position: relative;
  margin-top: 30px;
  margin-bottom: 30px;
}

@media(min-width: 1024px) {
  dt-article figure {
    margin-top: 48px;
    margin-bottom: 48px;
  }
}

dt-article figure img {
  width: 100%;
}

dt-article figure svg text,
dt-article figure svg tspan {
}

dt-article figure figcaption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}
@media(min-width: 1024px) {
  dt-article figure figcaption {
    font-size: 13px;
  }
}

dt-article figure.external img {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

dt-article figure figcaption a {
  color: rgba(0, 0, 0, 0.6);
}

dt-article span.equation-mimic {
  font-family: georgia;
  font-size: 115%;
  font-style: italic;
}

dt-article figure figcaption b {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

dt-article > dt-code,
dt-article section > dt-code  {
  display: block;
}

dt-article .citation {
  color: #668;
  cursor: pointer;
}

dt-include {
  width: auto;
  display: block;
}
/**
 * prism.js default theme for JavaScript, CSS and HTML
 * Based on dabblet (http://dabblet.com)
 * @author Lea Verou
 */

code {
  white-space: nowrap;
  background: rgba(0, 0, 0, 0.04);
  border-radius: 2px;
  padding: 4px 7px;
  font-size: 15px;
  color: rgba(0, 0, 0, 0.6);
}

pre code {
  display: block;
  background: white;
  border-left: 3px solid rgba(0, 0, 0, 0.05);
  padding: 0 0 0 24px;
}


code[class*="language-"],
pre[class*="language-"] {
  text-shadow: 0 1px white;
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

pre[class*="language-"]::-moz-selection, pre[class*="language-"] ::-moz-selection,
code[class*="language-"]::-moz-selection, code[class*="language-"] ::-moz-selection {
  text-shadow: none;
  background: #b3d4fc;
}

pre[class*="language-"]::selection, pre[class*="language-"] ::selection,
code[class*="language-"]::selection, code[class*="language-"] ::selection {
  text-shadow: none;
  background: #b3d4fc;
}

@media print {
  code[class*="language-"],
  pre[class*="language-"] {
  text-shadow: none;
  }
}

/* Code blocks */
pre[class*="language-"] {
  overflow: auto;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
}

/* Inline code */
:not(pre) > code[class*="language-"] {
  white-space: normal;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
  color: slategray;
}

.token.punctuation {
  color: #999;
}

.namespace {
  opacity: .7;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
  color: #905;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
  color: #690;
}

.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string {
  color: #a67f59;
  background: hsla(0, 0%, 100%, .5);
}

.token.atrule,
.token.attr-value,
.token.keyword {
  color: #07a;
}

.token.function {
  color: #DD4A68;
}

.token.regex,
.token.important,
.token.variable {
  color: #e90;
}

.token.important,
.token.bold {
  font-weight: bold;
}
.token.italic {
  font-style: italic;
}

.token.entity {
  cursor: help;
}

@media print {
  @page {
    size: 8in 11in;
  }
  html {
  }
  p, code {
    page-break-inside: avoid;
  }
  h2, h3 {
    page-break-after: avoid;
  }
  dt-header {
    visibility: hidden;
  }
  dt-footer {
    display: none!important;
  }
}
</style><script src="./World Models_files/controller.js.download"></script><script type="text/front-matter">
  title: "World Models"
  description: "Can an agent to learn inside of its own dream?"
</script>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <link rel="icon" type="image/png" href="data:image/png;base64,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">
    <link href="https://worldmodels.github.io/rss.xml" rel="alternate" type="application/rss+xml" title="Articles from Distill">
    <link rel="canonical" href="http://distill.pub/">
    <title>World Models</title>
  
    <!--  https://schema.org/Article -->
    <meta property="article:published" itemprop="datePublished" content="NaN-NaN-NaN">
    <meta property="article:created" itemprop="dateCreated" content="Invalid Date">
    <meta property="article:modified" itemprop="dateModified" content="Invalid Date">
  <script src="chrome-extension://jgghnecdoiloelcogfmgjgcacadpaejf/inject.js"></script><style id="ace_editor.css">.ace_editor {position: relative;overflow: hidden;font: 12px/normal 'Monaco', 'Menlo', 'Ubuntu Mono', 'Consolas', 'source-code-pro', monospace;direction: ltr;}.ace_scroller {position: absolute;overflow: hidden;top: 0;bottom: 0;background-color: inherit;-ms-user-select: none;-moz-user-select: none;-webkit-user-select: none;user-select: none;cursor: text;}.ace_content {position: absolute;-moz-box-sizing: border-box;-webkit-box-sizing: border-box;box-sizing: border-box;min-width: 100%;}.ace_dragging .ace_scroller:before{position: absolute;top: 0;left: 0;right: 0;bottom: 0;content: '';background: rgba(250, 250, 250, 0.01);z-index: 1000;}.ace_dragging.ace_dark .ace_scroller:before{background: rgba(0, 0, 0, 0.01);}.ace_selecting, .ace_selecting * {cursor: text !important;}.ace_gutter {position: absolute;overflow : hidden;width: auto;top: 0;bottom: 0;left: 0;cursor: default;z-index: 4;-ms-user-select: none;-moz-user-select: none;-webkit-user-select: none;user-select: none;}.ace_gutter-active-line {position: absolute;left: 0;right: 0;}.ace_scroller.ace_scroll-left {box-shadow: 17px 0 16px -16px rgba(0, 0, 0, 0.4) inset;}.ace_gutter-cell {padding-left: 19px;padding-right: 6px;background-repeat: no-repeat;}.ace_gutter-cell.ace_error {background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAMAAAAoLQ9TAAABOFBMVEX/////////QRswFAb/Ui4wFAYwFAYwFAaWGAfDRymzOSH/PxswFAb/SiUwFAYwFAbUPRvjQiDllog5HhHdRybsTi3/Tyv9Tir+Syj/UC3////XurebMBIwFAb/RSHbPx/gUzfdwL3kzMivKBAwFAbbvbnhPx66NhowFAYwFAaZJg8wFAaxKBDZurf/RB6mMxb/SCMwFAYwFAbxQB3+RB4wFAb/Qhy4Oh+4QifbNRcwFAYwFAYwFAb/QRzdNhgwFAYwFAbav7v/Uy7oaE68MBK5LxLewr/r2NXewLswFAaxJw4wFAbkPRy2PyYwFAaxKhLm1tMwFAazPiQwFAaUGAb/QBrfOx3bvrv/VC/maE4wFAbRPBq6MRO8Qynew8Dp2tjfwb0wFAbx6eju5+by6uns4uH9/f36+vr/GkHjAAAAYnRSTlMAGt+64rnWu/bo8eAA4InH3+DwoN7j4eLi4xP99Nfg4+b+/u9B/eDs1MD1mO7+4PHg2MXa347g7vDizMLN4eG+Pv7i5evs/v79yu7S3/DV7/498Yv24eH+4ufQ3Ozu/v7+y13sRqwAAADLSURBVHjaZc/XDsFgGIBhtDrshlitmk2IrbHFqL2pvXf/+78DPokj7+Fz9qpU/9UXJIlhmPaTaQ6QPaz0mm+5gwkgovcV6GZzd5JtCQwgsxoHOvJO15kleRLAnMgHFIESUEPmawB9ngmelTtipwwfASilxOLyiV5UVUyVAfbG0cCPHig+GBkzAENHS0AstVF6bacZIOzgLmxsHbt2OecNgJC83JERmePUYq8ARGkJx6XtFsdddBQgZE2nPR6CICZhawjA4Fb/chv+399kfR+MMMDGOQAAAABJRU5ErkJggg==");background-repeat: no-repeat;background-position: 2px center;}.ace_gutter-cell.ace_warning {background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAMAAAAoLQ9TAAAAmVBMVEX///8AAAD///8AAAAAAABPSzb/5sAAAAB/blH/73z/ulkAAAAAAAD85pkAAAAAAAACAgP/vGz/rkDerGbGrV7/pkQICAf////e0IsAAAD/oED/qTvhrnUAAAD/yHD/njcAAADuv2r/nz//oTj/p064oGf/zHAAAAA9Nir/tFIAAAD/tlTiuWf/tkIAAACynXEAAAAAAAAtIRW7zBpBAAAAM3RSTlMAABR1m7RXO8Ln31Z36zT+neXe5OzooRDfn+TZ4p3h2hTf4t3k3ucyrN1K5+Xaks52Sfs9CXgrAAAAjklEQVR42o3PbQ+CIBQFYEwboPhSYgoYunIqqLn6/z8uYdH8Vmdnu9vz4WwXgN/xTPRD2+sgOcZjsge/whXZgUaYYvT8QnuJaUrjrHUQreGczuEafQCO/SJTufTbroWsPgsllVhq3wJEk2jUSzX3CUEDJC84707djRc5MTAQxoLgupWRwW6UB5fS++NV8AbOZgnsC7BpEAAAAABJRU5ErkJggg==");background-position: 2px center;}.ace_gutter-cell.ace_info {background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAAAAAA6mKC9AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAJ0Uk5TAAB2k804AAAAPklEQVQY02NgIB68QuO3tiLznjAwpKTgNyDbMegwisCHZUETUZV0ZqOquBpXj2rtnpSJT1AEnnRmL2OgGgAAIKkRQap2htgAAAAASUVORK5CYII=");background-position: 2px center;}.ace_dark .ace_gutter-cell.ace_info {background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQBAMAAADt3eJSAAAAJFBMVEUAAAChoaGAgIAqKiq+vr6tra1ZWVmUlJSbm5s8PDxubm56enrdgzg3AAAAAXRSTlMAQObYZgAAAClJREFUeNpjYMAPdsMYHegyJZFQBlsUlMFVCWUYKkAZMxZAGdxlDMQBAG+TBP4B6RyJAAAAAElFTkSuQmCC");}.ace_scrollbar {position: absolute;right: 0;bottom: 0;z-index: 6;}.ace_scrollbar-inner {position: absolute;cursor: text;left: 0;top: 0;}.ace_scrollbar-v{overflow-x: hidden;overflow-y: scroll;top: 0;}.ace_scrollbar-h {overflow-x: scroll;overflow-y: hidden;left: 0;}.ace_print-margin {position: absolute;height: 100%;}.ace_text-input {position: absolute;z-index: 0;width: 0.5em;height: 1em;opacity: 0;background: transparent;-moz-appearance: none;appearance: none;border: none;resize: none;outline: none;overflow: hidden;font: inherit;padding: 0 1px;margin: 0 -1px;text-indent: -1em;-ms-user-select: text;-moz-user-select: text;-webkit-user-select: text;user-select: text;white-space: pre!important;}.ace_text-input.ace_composition {background: inherit;color: inherit;z-index: 1000;opacity: 1;text-indent: 0;}.ace_layer {z-index: 1;position: absolute;overflow: hidden;word-wrap: normal;white-space: pre;height: 100%;width: 100%;-moz-box-sizing: border-box;-webkit-box-sizing: border-box;box-sizing: border-box;pointer-events: none;}.ace_gutter-layer {position: relative;width: auto;text-align: right;pointer-events: auto;}.ace_text-layer {font: inherit !important;}.ace_cjk {display: inline-block;text-align: center;}.ace_cursor-layer {z-index: 4;}.ace_cursor {z-index: 4;position: absolute;-moz-box-sizing: border-box;-webkit-box-sizing: border-box;box-sizing: border-box;border-left: 2px solid;transform: translatez(0);}.ace_slim-cursors .ace_cursor {border-left-width: 1px;}.ace_overwrite-cursors .ace_cursor {border-left-width: 0;border-bottom: 1px solid;}.ace_hidden-cursors .ace_cursor {opacity: 0.2;}.ace_smooth-blinking .ace_cursor {-webkit-transition: opacity 0.18s;transition: opacity 0.18s;}.ace_editor.ace_multiselect .ace_cursor {border-left-width: 1px;}.ace_marker-layer .ace_step, .ace_marker-layer .ace_stack {position: absolute;z-index: 3;}.ace_marker-layer .ace_selection {position: absolute;z-index: 5;}.ace_marker-layer .ace_bracket {position: absolute;z-index: 6;}.ace_marker-layer .ace_active-line {position: absolute;z-index: 2;}.ace_marker-layer .ace_selected-word {position: absolute;z-index: 4;-moz-box-sizing: border-box;-webkit-box-sizing: border-box;box-sizing: border-box;}.ace_line .ace_fold {-moz-box-sizing: border-box;-webkit-box-sizing: border-box;box-sizing: border-box;display: inline-block;height: 11px;margin-top: -2px;vertical-align: middle;background-image:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABEAAAAJCAYAAADU6McMAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAJpJREFUeNpi/P//PwOlgAXGYGRklAVSokD8GmjwY1wasKljQpYACtpCFeADcHVQfQyMQAwzwAZI3wJKvCLkfKBaMSClBlR7BOQikCFGQEErIH0VqkabiGCAqwUadAzZJRxQr/0gwiXIal8zQQPnNVTgJ1TdawL0T5gBIP1MUJNhBv2HKoQHHjqNrA4WO4zY0glyNKLT2KIfIMAAQsdgGiXvgnYAAAAASUVORK5CYII="),url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAA3CAYAAADNNiA5AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAACJJREFUeNpi+P//fxgTAwPDBxDxD078RSX+YeEyDFMCIMAAI3INmXiwf2YAAAAASUVORK5CYII=");background-repeat: no-repeat, repeat-x;background-position: center center, top left;color: transparent;border: 1px solid black;border-radius: 2px;cursor: pointer;pointer-events: auto;}.ace_dark .ace_fold {}.ace_fold:hover{background-image:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABEAAAAJCAYAAADU6McMAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAJpJREFUeNpi/P//PwOlgAXGYGRklAVSokD8GmjwY1wasKljQpYACtpCFeADcHVQfQyMQAwzwAZI3wJKvCLkfKBaMSClBlR7BOQikCFGQEErIH0VqkabiGCAqwUadAzZJRxQr/0gwiXIal8zQQPnNVTgJ1TdawL0T5gBIP1MUJNhBv2HKoQHHjqNrA4WO4zY0glyNKLT2KIfIMAAQsdgGiXvgnYAAAAASUVORK5CYII="),url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAA3CAYAAADNNiA5AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAACBJREFUeNpi+P//fz4TAwPDZxDxD5X4i5fLMEwJgAADAEPVDbjNw87ZAAAAAElFTkSuQmCC");}.ace_tooltip {background-color: #FFF;background-image: -webkit-linear-gradient(top, transparent, rgba(0, 0, 0, 0.1));background-image: linear-gradient(to bottom, transparent, rgba(0, 0, 0, 0.1));border: 1px solid gray;border-radius: 1px;box-shadow: 0 1px 2px rgba(0, 0, 0, 0.3);color: black;max-width: 100%;padding: 3px 4px;position: fixed;z-index: 999999;-moz-box-sizing: border-box;-webkit-box-sizing: border-box;box-sizing: border-box;cursor: default;white-space: pre;word-wrap: break-word;line-height: normal;font-style: normal;font-weight: normal;letter-spacing: normal;pointer-events: none;}.ace_folding-enabled > .ace_gutter-cell {padding-right: 13px;}.ace_fold-widget {-moz-box-sizing: border-box;-webkit-box-sizing: border-box;box-sizing: border-box;margin: 0 -12px 0 1px;display: none;width: 11px;vertical-align: top;background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUAAAAFCAYAAACNbyblAAAANElEQVR42mWKsQ0AMAzC8ixLlrzQjzmBiEjp0A6WwBCSPgKAXoLkqSot7nN3yMwR7pZ32NzpKkVoDBUxKAAAAABJRU5ErkJggg==");background-repeat: no-repeat;background-position: center;border-radius: 3px;border: 1px solid transparent;cursor: pointer;}.ace_folding-enabled .ace_fold-widget {display: inline-block;   }.ace_fold-widget.ace_end {background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUAAAAFCAYAAACNbyblAAAANElEQVR42m3HwQkAMAhD0YzsRchFKI7sAikeWkrxwScEB0nh5e7KTPWimZki4tYfVbX+MNl4pyZXejUO1QAAAABJRU5ErkJggg==");}.ace_fold-widget.ace_closed {background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAAGCAYAAAAG5SQMAAAAOUlEQVR42jXKwQkAMAgDwKwqKD4EwQ26sSOkVWjgIIHAzPiCgaqiqnJHZnKICBERHN194O5b9vbLuAVRL+l0YWnZAAAAAElFTkSuQmCCXA==");}.ace_fold-widget:hover {border: 1px solid rgba(0, 0, 0, 0.3);background-color: rgba(255, 255, 255, 0.2);box-shadow: 0 1px 1px rgba(255, 255, 255, 0.7);}.ace_fold-widget:active {border: 1px solid rgba(0, 0, 0, 0.4);background-color: rgba(0, 0, 0, 0.05);box-shadow: 0 1px 1px rgba(255, 255, 255, 0.8);}.ace_dark .ace_fold-widget {background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUAAAAFCAYAAACNbyblAAAAHklEQVQIW2P4//8/AzoGEQ7oGCaLLAhWiSwB146BAQCSTPYocqT0AAAAAElFTkSuQmCC");}.ace_dark .ace_fold-widget.ace_end {background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUAAAAFCAYAAACNbyblAAAAH0lEQVQIW2P4//8/AxQ7wNjIAjDMgC4AxjCVKBirIAAF0kz2rlhxpAAAAABJRU5ErkJggg==");}.ace_dark .ace_fold-widget.ace_closed {background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAAFCAYAAACAcVaiAAAAHElEQVQIW2P4//+/AxAzgDADlOOAznHAKgPWAwARji8UIDTfQQAAAABJRU5ErkJggg==");}.ace_dark .ace_fold-widget:hover {box-shadow: 0 1px 1px rgba(255, 255, 255, 0.2);background-color: rgba(255, 255, 255, 0.1);}.ace_dark .ace_fold-widget:active {box-shadow: 0 1px 1px rgba(255, 255, 255, 0.2);}.ace_fold-widget.ace_invalid {background-color: #FFB4B4;border-color: #DE5555;}.ace_fade-fold-widgets .ace_fold-widget {-webkit-transition: opacity 0.4s ease 0.05s;transition: opacity 0.4s ease 0.05s;opacity: 0;}.ace_fade-fold-widgets:hover .ace_fold-widget {-webkit-transition: opacity 0.05s ease 0.05s;transition: opacity 0.05s ease 0.05s;opacity:1;}.ace_underline {text-decoration: underline;}.ace_bold {font-weight: bold;}.ace_nobold .ace_bold {font-weight: normal;}.ace_italic {font-style: italic;}.ace_error-marker {background-color: rgba(255, 0, 0,0.2);position: absolute;z-index: 9;}.ace_highlight-marker {background-color: rgba(255, 255, 0,0.2);position: absolute;z-index: 8;}.ace_br1 {border-top-left-radius    : 3px;}.ace_br2 {border-top-right-radius   : 3px;}.ace_br3 {border-top-left-radius    : 3px; border-top-right-radius:    3px;}.ace_br4 {border-bottom-right-radius: 3px;}.ace_br5 {border-top-left-radius    : 3px; border-bottom-right-radius: 3px;}.ace_br6 {border-top-right-radius   : 3px; border-bottom-right-radius: 3px;}.ace_br7 {border-top-left-radius    : 3px; border-top-right-radius:    3px; border-bottom-right-radius: 3px;}.ace_br8 {border-bottom-left-radius : 3px;}.ace_br9 {border-top-left-radius    : 3px; border-bottom-left-radius:  3px;}.ace_br10{border-top-right-radius   : 3px; border-bottom-left-radius:  3px;}.ace_br11{border-top-left-radius    : 3px; border-top-right-radius:    3px; border-bottom-left-radius:  3px;}.ace_br12{border-bottom-right-radius: 3px; border-bottom-left-radius:  3px;}.ace_br13{border-top-left-radius    : 3px; border-bottom-right-radius: 3px; border-bottom-left-radius:  3px;}.ace_br14{border-top-right-radius   : 3px; border-bottom-right-radius: 3px; border-bottom-left-radius:  3px;}.ace_br15{border-top-left-radius    : 3px; border-top-right-radius:    3px; border-bottom-right-radius: 3px; border-bottom-left-radius: 3px;}
/*# sourceURL=ace/css/ace_editor.css */</style><style id="ace-tm">.ace-tm .ace_gutter {background: #f0f0f0;color: #333;}.ace-tm .ace_print-margin {width: 1px;background: #e8e8e8;}.ace-tm .ace_fold {background-color: #6B72E6;}.ace-tm {background-color: #FFFFFF;color: black;}.ace-tm .ace_cursor {color: black;}.ace-tm .ace_invisible {color: rgb(191, 191, 191);}.ace-tm .ace_storage,.ace-tm .ace_keyword {color: blue;}.ace-tm .ace_constant {color: rgb(197, 6, 11);}.ace-tm .ace_constant.ace_buildin {color: rgb(88, 72, 246);}.ace-tm .ace_constant.ace_language {color: rgb(88, 92, 246);}.ace-tm .ace_constant.ace_library {color: rgb(6, 150, 14);}.ace-tm .ace_invalid {background-color: rgba(255, 0, 0, 0.1);color: red;}.ace-tm .ace_support.ace_function {color: rgb(60, 76, 114);}.ace-tm .ace_support.ace_constant {color: rgb(6, 150, 14);}.ace-tm .ace_support.ace_type,.ace-tm .ace_support.ace_class {color: rgb(109, 121, 222);}.ace-tm .ace_keyword.ace_operator {color: rgb(104, 118, 135);}.ace-tm .ace_string {color: rgb(3, 106, 7);}.ace-tm .ace_comment {color: rgb(76, 136, 107);}.ace-tm .ace_comment.ace_doc {color: rgb(0, 102, 255);}.ace-tm .ace_comment.ace_doc.ace_tag {color: rgb(128, 159, 191);}.ace-tm .ace_constant.ace_numeric {color: rgb(0, 0, 205);}.ace-tm .ace_variable {color: rgb(49, 132, 149);}.ace-tm .ace_xml-pe {color: rgb(104, 104, 91);}.ace-tm .ace_entity.ace_name.ace_function {color: #0000A2;}.ace-tm .ace_heading {color: rgb(12, 7, 255);}.ace-tm .ace_list {color:rgb(185, 6, 144);}.ace-tm .ace_meta.ace_tag {color:rgb(0, 22, 142);}.ace-tm .ace_string.ace_regex {color: rgb(255, 0, 0)}.ace-tm .ace_marker-layer .ace_selection {background: rgb(181, 213, 255);}.ace-tm.ace_multiselect .ace_selection.ace_start {box-shadow: 0 0 3px 0px white;}.ace-tm .ace_marker-layer .ace_step {background: rgb(252, 255, 0);}.ace-tm .ace_marker-layer .ace_stack {background: rgb(164, 229, 101);}.ace-tm .ace_marker-layer .ace_bracket {margin: -1px 0 0 -1px;border: 1px solid rgb(192, 192, 192);}.ace-tm .ace_marker-layer .ace_active-line {background: rgba(0, 0, 0, 0.07);}.ace-tm .ace_gutter-active-line {background-color : #dcdcdc;}.ace-tm .ace_marker-layer .ace_selected-word {background: rgb(250, 250, 255);border: 1px solid rgb(200, 200, 250);}.ace-tm .ace_indent-guide {background: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAACCAYAAACZgbYnAAAAE0lEQVQImWP4////f4bLly//BwAmVgd1/w11/gAAAABJRU5ErkJggg==") right repeat-y;}
/*# sourceURL=ace/css/ace-tm */</style><style>    .error_widget_wrapper {        background: inherit;        color: inherit;        border:none    }    .error_widget {        border-top: solid 2px;        border-bottom: solid 2px;        margin: 5px 0;        padding: 10px 40px;        white-space: pre-wrap;    }    .error_widget.ace_error, .error_widget_arrow.ace_error{        border-color: #ff5a5a    }    .error_widget.ace_warning, .error_widget_arrow.ace_warning{        border-color: #F1D817    }    .error_widget.ace_info, .error_widget_arrow.ace_info{        border-color: #5a5a5a    }    .error_widget.ace_ok, .error_widget_arrow.ace_ok{        border-color: #5aaa5a    }    .error_widget_arrow {        position: absolute;        border: solid 5px;        border-top-color: transparent!important;        border-right-color: transparent!important;        border-left-color: transparent!important;        top: -5px;    }</style><style>.ace_snippet-marker {    -moz-box-sizing: border-box;    box-sizing: border-box;    background: rgba(194, 193, 208, 0.09);    border: 1px dotted rgba(211, 208, 235, 0.62);    position: absolute;}</style><style>.ace_editor.ace_autocomplete .ace_marker-layer .ace_active-line {    background-color: #CAD6FA;    z-index: 1;}.ace_editor.ace_autocomplete .ace_line-hover {    border: 1px solid #abbffe;    margin-top: -1px;    background: rgba(233,233,253,0.4);}.ace_editor.ace_autocomplete .ace_line-hover {    position: absolute;    z-index: 2;}.ace_editor.ace_autocomplete .ace_scroller {   background: none;   border: none;   box-shadow: none;}.ace_rightAlignedText {    color: gray;    display: inline-block;    position: absolute;    right: 4px;    text-align: right;    z-index: -1;}.ace_editor.ace_autocomplete .ace_completion-highlight{    color: #000;    text-shadow: 0 0 0.01em;}.ace_editor.ace_autocomplete {    width: 280px;    z-index: 200000;    background: #fbfbfb;    color: #444;    border: 1px lightgray solid;    position: fixed;    box-shadow: 2px 3px 5px rgba(0,0,0,.2);    line-height: 1.4;}</style><style id="ace_searchbox">.ace_search {background-color: #ddd;border: 1px solid #cbcbcb;border-top: 0 none;max-width: 325px;overflow: hidden;margin: 0;padding: 4px;padding-right: 6px;padding-bottom: 0;position: absolute;top: 0px;z-index: 99;white-space: normal;}.ace_search.left {border-left: 0 none;border-radius: 0px 0px 5px 0px;left: 0;}.ace_search.right {border-radius: 0px 0px 0px 5px;border-right: 0 none;right: 0;}.ace_search_form, .ace_replace_form {border-radius: 3px;border: 1px solid #cbcbcb;float: left;margin-bottom: 4px;overflow: hidden;}.ace_search_form.ace_nomatch {outline: 1px solid red;}.ace_search_field {background-color: white;border-right: 1px solid #cbcbcb;border: 0 none;-webkit-box-sizing: border-box;-moz-box-sizing: border-box;box-sizing: border-box;float: left;height: 22px;outline: 0;padding: 0 7px;width: 214px;margin: 0;}.ace_searchbtn,.ace_replacebtn {background: #fff;border: 0 none;border-left: 1px solid #dcdcdc;cursor: pointer;float: left;height: 22px;margin: 0;position: relative;}.ace_searchbtn:last-child,.ace_replacebtn:last-child {border-top-right-radius: 3px;border-bottom-right-radius: 3px;}.ace_searchbtn:disabled {background: none;cursor: default;}.ace_searchbtn {background-position: 50% 50%;background-repeat: no-repeat;width: 27px;}.ace_searchbtn.prev {background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAADFJREFUeNpiSU1NZUAC/6E0I0yACYskCpsJiySKIiY0SUZk40FyTEgCjGgKwTRAgAEAQJUIPCE+qfkAAAAASUVORK5CYII=);    }.ace_searchbtn.next {background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAADRJREFUeNpiTE1NZQCC/0DMyIAKwGJMUAYDEo3M/s+EpvM/mkKwCQxYjIeLMaELoLMBAgwAU7UJObTKsvAAAAAASUVORK5CYII=);    }.ace_searchbtn_close {background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAcCAYAAABRVo5BAAAAZ0lEQVR42u2SUQrAMAhDvazn8OjZBilCkYVVxiis8H4CT0VrAJb4WHT3C5xU2a2IQZXJjiQIRMdkEoJ5Q2yMqpfDIo+XY4k6h+YXOyKqTIj5REaxloNAd0xiKmAtsTHqW8sR2W5f7gCu5nWFUpVjZwAAAABJRU5ErkJggg==) no-repeat 50% 0;border-radius: 50%;border: 0 none;color: #656565;cursor: pointer;float: right;font: 16px/16px Arial;height: 14px;margin: 5px 1px 9px 5px;padding: 0;text-align: center;width: 14px;}.ace_searchbtn_close:hover {background-color: #656565;background-position: 50% 100%;color: white;}.ace_replacebtn.prev {width: 54px}.ace_replacebtn.next {width: 27px}.ace_button {margin-left: 2px;cursor: pointer;-webkit-user-select: none;-moz-user-select: none;-o-user-select: none;-ms-user-select: none;user-select: none;overflow: hidden;opacity: 0.7;border: 1px solid rgba(100,100,100,0.23);padding: 1px;-moz-box-sizing: border-box;box-sizing:    border-box;color: black;}.ace_button:hover {background-color: #eee;opacity:1;}.ace_button:active {background-color: #ddd;}.ace_button.checked {border-color: #3399ff;opacity:1;}.ace_search_options{margin-bottom: 3px;text-align: right;-webkit-user-select: none;-moz-user-select: none;-o-user-select: none;-ms-user-select: none;user-select: none;}
/*# sourceURL=ace/css/ace_searchbox */</style><style>#ace_settingsmenu, #kbshortcutmenu {background-color: #F7F7F7;color: black;box-shadow: -5px 4px 5px rgba(126, 126, 126, 0.55);padding: 1em 0.5em 2em 1em;overflow: auto;position: absolute;margin: 0;bottom: 0;right: 0;top: 0;z-index: 9991;cursor: default;}.ace_dark #ace_settingsmenu, .ace_dark #kbshortcutmenu {box-shadow: -20px 10px 25px rgba(126, 126, 126, 0.25);background-color: rgba(255, 255, 255, 0.6);color: black;}.ace_optionsMenuEntry:hover {background-color: rgba(100, 100, 100, 0.1);-webkit-transition: all 0.5s;transition: all 0.3s}.ace_closeButton {background: rgba(245, 146, 146, 0.5);border: 1px solid #F48A8A;border-radius: 50%;padding: 7px;position: absolute;right: -8px;top: -8px;z-index: 1000;}.ace_closeButton{background: rgba(245, 146, 146, 0.9);}.ace_optionsMenuKey {color: darkslateblue;font-weight: bold;}.ace_optionsMenuCommand {color: darkcyan;font-weight: normal;}</style></head>


<!-- used google storage instead: https://storage.googleapis.com/quickdraw-models/sketchRNN/world_models/ -->




<!--
<script src="demo/lib/jquery-1.12.4.min.js"></script>
<script src="demo/lib/mobile-detect.min.js"></script>
<script src="demo/lib/template.v1.js"></script>
-->




<body data-gr-c-s-loaded="true" about="https://worldmodels.github.io/" class=" hasGoogleVoiceExt" showlongdescborders="">
    <dt-article id="dtbody">
<div id="cover_sketch" class="unselectable" style="text-align: center;"><canvas id="defaultCanvas0" width="1826" height="896" style="width: 1826px; height: 896px;"></canvas></div>
<div id="cover_overlay_wrap" class="unselectable" style="width: 1842px; height: 896px;">
  <div id="cover_instruction" class="cover-instruction" style="text-align: center; display: none;">
    <div id="cover_instruction_text">Interactive demo. Tap screen or use arrow keys to override the agent's decisions.</div>
  </div>
  <div id="scrolldowntag" class="scroll-down" style="text-align: center; opacity: 1;">
    <div>scroll down</div>
    <span></span>
    <span></span>
  </div>
</div>
<div id="cover_title" class="unselectable" style="width: 896px; height: 896px;">
  <!--<img src="https://storage.googleapis.com/quickdraw-models/sketchRNN/world_models/assets/cover_title.svg" id="cover_title_svg" class="unselectable"/>-->
  <img src="./World Models_files/cover_title.svg" id="cover_title_svg" class="svgunselectable">
</div>
<!--<div><figcaption>An agent playing inside of a game environment hallucinated by a recurrent neural network.</figcaption></div>-->
<dt-byline class="l-page transparent">
<style>
  dt-byline {
    font-size: 12px;
    line-height: 18px;
    display: block;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    color: rgba(0, 0, 0, 0.5);
    padding-top: 12px;
    padding-bottom: 12px;
  }
  dt-article.centered dt-byline {
    text-align: center;

  }
  dt-byline a,
  dt-article dt-byline a {
    text-decoration: none;
    border-bottom: none;
  }
  dt-article dt-byline a:hover {
    text-decoration: underline;
    border-bottom: none;
  }
  dt-byline .authors {
    text-align: left;
  }
  dt-byline .name {
    display: inline;
    text-transform: uppercase;
  }
  dt-byline .affiliation {
    display: inline;
  }
  dt-byline .date {
    display: block;
    text-align: left;
  }
  dt-byline .year, dt-byline .month {
    display: inline;
  }
  dt-byline .citation {
    display: block;
    text-align: left;
  }
  dt-byline .citation div {
    display: inline;
  }

  @media(min-width: 768px) {
    dt-byline {
    }
  }

  @media(min-width: 1080px) {
    dt-byline {
      border-bottom: none;
      margin-bottom: 70px;
    }

    dt-byline a:hover {
      color: rgba(0, 0, 0, 0.9);
    }

    dt-byline .authors {
      display: inline-block;
    }

    dt-byline .author {
      display: inline-block;
      margin-right: 12px;
      /*padding-left: 20px;*/
      /*border-left: 1px solid #ddd;*/
    }

    dt-byline .affiliation {
      display: block;
    }

    dt-byline .author:last-child {
      margin-right: 0;
    }

    dt-byline .name {
      display: block;
    }

    dt-byline .date {
      border-left: 1px solid rgba(0, 0, 0, 0.1);
      padding-left: 15px;
      margin-left: 15px;
      display: inline-block;
    }
    dt-byline .year, dt-byline .month {
      display: block;
    }

    dt-byline .citation {
      border-left: 1px solid rgba(0, 0, 0, 0.15);
      padding-left: 15px;
      margin-left: 15px;
      display: inline-block;
    }
    dt-byline .citation div {
      display: block;
    }
  }
</style>


<div class="byline">
  <div class="authors">
  </div>
</div>
</dt-byline>
<div style="text-align: center;">
  <p>　</p>
</div>
<div style="text-align: center;">
  <p>　</p>
</div>
<h1>World Models</h1>
<p></p>
<h2>Can agents learn inside of their own dreams?</h2>
<dt-byline class="l-page" id="authors_section" hidden="">
<div class="byline">
  <div class="authors">
    <div class="author">
        <a class="name" href="https://twitter.com/hardmaru">David Ha</a>
        <a class="affiliation" href="https://g.co/brain">Google Brain</a>
        <a class="affiliation" href="http://blog.otoro.net/">&nbsp;</a>
    </div>
    <div class="author">
        <a class="name" href="http://people.idsia.ch/~juergen/">Jürgen Schmidhuber</a>
        <a class="affiliation" href="https://nnaisense.com/">NNAISENSE</a>
        <a class="affiliation" href="http://www.idsia.ch/">Swiss AI Lab, IDSIA (USI &amp; SUPSI)</a>
    </div>
  </div>
  <div class="date">
    <div class="month">March 27</div>
    <div class="year">2018</div>
    <div class="year">&nbsp;</div>
  </div>
  <div class="date">
    <div class="month">Citation</div>
    <div class="year" style="color: #668;"><a href="https://worldmodels.github.io/#citation">doi:10.5281/zenodo.1207631</a></div>
    <div class="year">&nbsp;</div>
  </div>
  <div class="date">
    <div class="month">Download</div>
    <div class="year" style="color: #FF6C00;"><a href="https://arxiv.org/abs/1803.10122" target="_blank">PDF</a></div>
    <div class="year">&nbsp;</div>
  </div>
</div>
</dt-byline><h2>Abstract</h2>
<p>We explore building generative neural network models of popular reinforcement learning environments<dt-cite key="openai_gym"><span id="citation-0" data-hover-ref="dt-cite-hover-box-0"><span class="citation-number">[1]</span></span></dt-cite>. Our <em>world model</em> can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment.</p>
<hr>
<h2>Introduction</h2>
<div style="text-align: left;">
<img src="./World Models_files/world_model_comic.jpeg" style="display: block; margin: auto; width: 100%;">
<figcaption>A World Model, from Scott McCloud’s <i>Understanding Comics</i>.<dt-cite key="understandingcomics,understandingcomics_blog"><span id="citation-1" data-hover-ref="dt-cite-hover-box-1"><span class="citation-number">[2, 3]</span></span></dt-cite></figcaption>
</div>
<p>Humans develop a mental model of the world based on what they are able to perceive with their limited senses. The decisions and actions we make are based on this internal model. Jay Wright Forrester, the father of system dynamics, defined a mental model as:</p>
<p>“<em>The image of the world around us, which we carry in our head, is just a model. Nobody in his head imagines all the world, government or country. He has only selected concepts, and relationships between them, and uses those to represent the real system.</em>”
<dt-cite key="forrester"><span id="citation-2" data-hover-ref="dt-cite-hover-box-2"><span class="citation-number">[4]</span></span></dt-cite></p>
<p>To handle the vast amount of information that flows through our daily lives, our brain learns an abstract representation of both spatial and temporal aspects of this information. We are able to observe a scene and remember an abstract description thereof <dt-cite key="facial_identity_primate_brain,single_neuron_viz"><span id="citation-3" data-hover-ref="dt-cite-hover-box-3"><span class="citation-number">[5, 6]</span></span></dt-cite>. Evidence also suggests that what we perceive at any given moment is governed by our brain’s prediction of the future based on our internal model<dt-cite key="primary_viz_cortex_past_present,mt_motion"><span id="citation-4" data-hover-ref="dt-cite-hover-box-4"><span class="citation-number">[7, 8]</span></span></dt-cite>.</p>
<div style="text-align: left;">
<img src="./World Models_files/kitaoka.jpeg" style="display: block; margin: auto; width: 100%;">
<figcaption>What we see is based on our brain’s prediction of the future.<dt-cite key="kitaoka,pdi,Watanabe2018"><span id="citation-5" data-hover-ref="dt-cite-hover-box-5"><span class="citation-number">[9, 10, 11]</span></span></dt-cite></figcaption>
</div>
<p>One way of understanding the predictive model in our brains is that it might not be about just predicting the future in general, but predicting future sensory data given current motor actions<dt-cite key="Keller2012,Leinweber2017"><span id="citation-6" data-hover-ref="dt-cite-hover-box-6"><span class="citation-number">[12, 13]</span></span></dt-cite>. We are able to instinctively act on this predictive model and perform fast reflexive behaviours when we face danger<dt-cite key="survival_optimization"><span id="citation-7" data-hover-ref="dt-cite-hover-box-7"><span class="citation-number">[14]</span></span></dt-cite>, without the need to consciously plan out a course of action.</p>
<p>Take baseball for example<dt-cite key="baseball_player_drawings"><span id="citation-8" data-hover-ref="dt-cite-hover-box-8"><span class="citation-number">[15]</span></span></dt-cite>. A baseball batter has milliseconds to decide how they should swing the bat — shorter than the time it takes for visual signals from our eyes to reach our brain. The reason we are able to hit a 115mph fastball is due to our ability to instinctively predict when and where the ball will go. For professional players, this all happens subconsciously. Their muscles reflexively swing the bat at the right time and location in line with their internal models’ prediction<dt-cite key="mt_motion"><span id="citation-9" data-hover-ref="dt-cite-hover-box-9"><span class="citation-number">[8]</span></span></dt-cite>. They can quickly act on their predictions of the future without the need to consciously roll out possible future scenarios to form a plan<dt-cite key="mt_motion_article"><span id="citation-10" data-hover-ref="dt-cite-hover-box-10"><span class="citation-number">[16]</span></span></dt-cite>.</p>
<div style="text-align: center;">
<img src="./World Models_files/baseball.svg" style="display: block; margin: auto; width: 100%;">
</div>
<p>In many reinforcement learning (RL)<dt-cite key="Kaelbling:96,sutton_barto,wiering2012"><span id="citation-11" data-hover-ref="dt-cite-hover-box-11"><span class="citation-number">[17, 18, 19]</span></span></dt-cite> problems, an artificial agent also benefits from having a good representation of past and present states, and a good predictive model of the future <dt-cite key="Werbos87specifications,dyna_slides"><span id="citation-12" data-hover-ref="dt-cite-hover-box-12"><span class="citation-number">[20, 21]</span></span></dt-cite>, preferably a powerful predictive model implemented on a general purpose computer such as a recurrent neural network (RNN) <dt-cite key="s05_making_the_world_differentiable,s05a_cm,s05b_rl"><span id="citation-13" data-hover-ref="dt-cite-hover-box-13"><span class="citation-number">[22, 23, 24]</span></span></dt-cite>.</p>
<!--A simple example is training an agent to play Flappy Bird<dt-cite key="flappy_bird"></dt-cite> from observing only pixel inputs. If the agent had already learned a compressed, low-dimensional representation of each image frame it sees, and also a predictive model of what the frames will look like in the future, then it would have almost all of the information needed to solve this task<dt-cite key="keras_flappybird"></dt-cite>.

<div style="text-align: center;">
<img src="assets/flappy_bird_multi.jpeg" style="display: block; margin: auto; width: 100%;"/>
<i>Flappy Bird Clone</i><dt-cite key="flappy_bird_ne"></dt-cite>
</div>-->
<p>Large RNNs are highly expressive models that can learn rich spatial and temporal representations of data. However, many <em>model-free</em> RL methods in the literature often only use small neural networks with few parameters. The RL algorithm is often bottlenecked by the <em>credit assignment problem</em><dt-fn><sup><span id="fn-1" data-hover-ref="dt-fn-hover-box-0" style="cursor:pointer">1</span></sup></dt-fn>, which makes it hard for traditional RL algorithms to learn millions of weights of a large model, hence in practice, smaller networks are used as they iterate faster to a good policy during training.</p>
<p>Ideally, we would like to be able to efficiently train large RNN-based agents. The backpropagation algorithm <dt-cite key="Linnainmaa:1970,Kelley:1960,werbos1982sensitivity"><span id="citation-14" data-hover-ref="dt-cite-hover-box-14"><span class="citation-number">[25, 26, 27]</span></span></dt-cite> can be used to train large neural networks efficiently. In this work we look at training a large neural network<dt-fn><sup><span id="fn-2" data-hover-ref="dt-fn-hover-box-1" style="cursor:pointer">2</span></sup></dt-fn> to tackle RL tasks, by dividing the agent into a large world model and a small controller model. We first train a large neural network to learn a model of the agent’s world in an unsupervised manner, and then train the smaller controller model to learn to perform a task using this world model. A small controller lets the training algorithm focus on the credit assignment problem on a small search space, while not sacrificing capacity and expressiveness via the larger world model. By training the agent through the lens of its world model, we show that it can learn a highly compact policy to perform its task.</p>
<p>Although there is a large body of research relating to <em>model-based</em> reinforcement learning, this article is not meant to be a review <dt-cite key="rl_survey,s03_overview"><span id="citation-15" data-hover-ref="dt-cite-hover-box-15"><span class="citation-number">[28, 29]</span></span></dt-cite> of the current state of the field. Instead, the goal of this article is to distill several key concepts from a series of papers 1990—2015 on combinations of RNN-based world models and controllers <dt-cite key="s05_making_the_world_differentiable,s05a_cm,s05b_rl,s05c_boredom,learning_to_think"><span id="citation-16" data-hover-ref="dt-cite-hover-box-16"><span class="citation-number">[22, 23, 24, 30, 31]</span></span></dt-cite>. We will also discuss other related works in the literature that share similar ideas of learning a world model and training an agent using this model.</p>
<p>In this article, we present a simplified framework that we can use to experimentally demonstrate some of the key concepts from these papers, and also suggest further insights to effectively apply these ideas to various RL environments. We use similar terminology and notation as <em>Learning to Think</em><dt-cite key="learning_to_think"><span id="citation-17" data-hover-ref="dt-cite-hover-box-17"><span class="citation-number">[31]</span></span></dt-cite> when describing our methodology and experiments.</p>
<hr>
<h2>Agent Model</h2>
<p>We present a simple model inspired by our own cognitive system. In this model, our agent has a visual sensory component that compresses what it sees into a small representative code. It also has a memory component that makes predictions about future codes based on historical information. Finally, our agent has a decision-making component that decides what actions to take based only on the representations created by its vision and memory components.</p>
<div id="overview_diagram_div" style="text-align: center;">
<img id="overview_diagram" src="./World Models_files/world_model_overview.svg" style="display: block; margin: auto; width: 720px;">
</div>
<p>Our agent consists of three components that work closely together: Vision (V), Memory (M), and Controller (C).</p>
<h3>VAE (V) Model</h3>
<p>The environment provides our agent with a high dimensional input observation at each time step. This input is usually a 2D image frame in a video sequence. The role of the V model is to learn an abstract, compressed representation of each observed input frame.</p>
<div style="text-align: left;">
<img src="./World Models_files/vae.svg" style="display: block; margin: auto; width: 100%;">
<br>
<figcaption>Flow diagram of a Variational Autoencoder.<dt-cite key="vae,vae_dm"><span id="citation-18" data-hover-ref="dt-cite-hover-box-18"><span class="citation-number">[32, 33]</span></span></dt-cite></figcaption>
</div>
<p>We use a Variational Autoencoder (VAE)<dt-cite key="vae,vae_dm"><span id="citation-19" data-hover-ref="dt-cite-hover-box-19"><span class="citation-number">[32, 33]</span></span></dt-cite> as the V model in our experiments. In the following demo, we show how the V model compresses each frame it receives at time step <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.61508em;"></span><span class="strut bottom" style="height:0.61508em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span></span></span></span> into a low dimensional <em>latent vector</em> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>. This compressed representation can be used to reconstruct the original image.</p>
<div style="text-align: left;">
<div id="doomvae_sketch" class="unselectable"><button style="position: absolute; left: 431px; top: 7525.64px;">Load Random Screenshot</button><button style="position: absolute; left: 751px; top: 7525.64px;">Randomize Z</button><canvas id="defaultCanvas4" width="760" height="420" style="width: 760px; height: 420px;"></canvas><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 7190.64px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 7211.97px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 7233.31px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 7254.64px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 7275.97px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 7297.31px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 7318.64px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 7339.97px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 7361.31px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 7382.64px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 7403.97px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 7425.31px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 7446.64px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 7467.97px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 7489.31px;"></div>
<figcaption style="color:#FF6C00;">Interactive Demo</figcaption>
<figcaption>A VAE trained on screenshots obtained from a VizDoom<dt-cite key="vizdoom,takecover"><span id="citation-20" data-hover-ref="dt-cite-hover-box-20"><span class="citation-number">[34, 35]</span></span></dt-cite> environment. You can load randomly chosen screenshots to be encoded into a small latent vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span>, which is used to reconstruct the original screenshot. You can also experiment with adjusting the values of the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span> vector using the slider bars to see how it affects the reconstruction, or randomize <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span> to observe the space of possible screenshots learned by our VAE.</figcaption>
</div>
<h3>MDN-RNN (M) Model</h3>
<p>While it is the role of the V model to compress what the agent sees at each time frame, we also want to compress what happens over time. For this purpose, the role of the M model is to predict the future. The M model serves as a predictive model of the future <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span> vectors that V is expected to produce. Because many complex environments are stochastic in nature, we train our RNN to output a probability density function <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>(</mo><mi>z</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">p(z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mclose">)</span></span></span></span> instead of a deterministic prediction <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span>.</p>
<div style="text-align: center;">
<img src="./World Models_files/mdn_rnn_new.svg" style="display: block; margin: auto; width: 80%;">
<figcaption>RNN with a Mixture Density Network output layer. The MDN outputs the parameters of a mixture of Gaussian distribution used to sample a prediction of the next latent vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span>.</figcaption>
</div>
<p>In our approach, we approximate <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>(</mo><mi>z</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">p(z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mclose">)</span></span></span></span> as a mixture of Gaussian distribution, and train the RNN to output the probability distribution of the next latent vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">z_{t+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> given the current and past information made available to it.</p>
<p>More specifically, the RNN will model <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mspace width="0.277778em"></mspace><mi mathvariant="normal">∣</mi><mspace width="0.277778em"></mspace><msub><mi>a</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>z</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>h</mi><mi>t</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">P(z_{t+1} \; | \; a_t, z_t, h_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mspace thickspace"></span><span class="mord mathrm">∣</span><span class="mord mspace thickspace"></span><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span>, where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">a_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> is the action taken at time <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.61508em;"></span><span class="strut bottom" style="height:0.61508em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> is the <em>hidden state</em> of the RNN at time <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.61508em;"></span><span class="strut bottom" style="height:0.61508em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span></span></span></span>. During sampling, we can adjust a <em>temperature</em> parameter <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span></span></span> to control model uncertainty, as done in <dt-cite key="sketchrnn"><span id="citation-21" data-hover-ref="dt-cite-hover-box-21"><span class="citation-number">[36]</span></span></dt-cite> — we will find adjusting <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span></span></span> to be useful for training our controller later on.</p>
<div style="text-align: center;">
<video autoplay="" muted="" playsinline="" loop="" style="display: block; margin: auto; width: 100%;"><source src="https://storage.googleapis.com/quickdraw-models/sketchRNN/world_models/assets/mp4/sketch_rnn_insect.mp4" type="video/mp4"></video>
<figcaption>SketchRNN<dt-cite key="sketchrnndemo"><span id="citation-22" data-hover-ref="dt-cite-hover-box-22"><span class="citation-number">[37]</span></span></dt-cite> is an example of a MDN-RNN used to predict the next pen strokes of a sketch drawing. We use a similar model to predict the next latent vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span>.</figcaption>
</div>
<p>This approach is known as a Mixture Density Network<dt-cite key="bishop,mdntf"><span id="citation-23" data-hover-ref="dt-cite-hover-box-23"><span class="citation-number">[38, 39]</span></span></dt-cite> combined with a RNN (MDN-RNN)<dt-cite key="graves_rnn,mdnrnn_tutorial"><span id="citation-24" data-hover-ref="dt-cite-hover-box-24"><span class="citation-number">[40, 41]</span></span></dt-cite>, and has been used successfully in the past for sequence generation problems such as generating handwriting<dt-cite key="graves_rnn,carter2016experiments"><span id="citation-25" data-hover-ref="dt-cite-hover-box-25"><span class="citation-number">[40, 42]</span></span></dt-cite> and sketches<dt-cite key="sketchrnn"><span id="citation-26" data-hover-ref="dt-cite-hover-box-26"><span class="citation-number">[36]</span></span></dt-cite>.</p>
<h3>Controller (C) Model</h3>
<p>The Controller (C) model is responsible for determining the course of actions to take in order to maximize the expected cumulative reward of the agent during a rollout in the environment. <!--To test our hypothesis that the representations inside our world model (V and M) contain most of the information required to solve a problem, we deliberately make C as simple and small as possible.--> In our experiments, we deliberately make C as simple and small as possible, and trained separately from V and M, so that most of our agent’s complexity resides in the world model (V and M).</p>
<p>C is a simple single layer linear model that maps <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> directly to action <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">a_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> at each time step:</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>=</mo><msub><mi>W</mi><mi>c</mi></msub><mspace width="0.277778em"></mspace><mo>[</mo><msub><mi>z</mi><mi>t</mi></msub><mspace width="0.277778em"></mspace><msub><mi>h</mi><mi>t</mi></msub><mo>]</mo><mspace width="0.277778em"></mspace><mo>+</mo><msub><mi>b</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">a_t = W_c \; [z_t \; h_t]\; + b_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">c</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mspace thickspace"></span><span class="mopen">[</span><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mspace thickspace"></span><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">]</span><span class="mord mspace thickspace"></span><span class="mbin">+</span><span class="mord"><span class="mord mathit">b</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">c</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></p>
<p>In this linear model, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">W_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">c</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>b</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">b_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">b</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">c</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> are the weight matrix and bias vector that maps the concatenated input vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>[</mo><msub><mi>z</mi><mi>t</mi></msub><mspace width="0.277778em"></mspace><msub><mi>h</mi><mi>t</mi></msub><mo>]</mo></mrow><annotation encoding="application/x-tex">[z_t \; h_t]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">[</span><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mspace thickspace"></span><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">]</span></span></span></span> to the output action vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">a_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>.<dt-fn><sup><span id="fn-3" data-hover-ref="dt-fn-hover-box-2" style="cursor:pointer">3</span></sup></dt-fn></p>
<h3>Putting Everything Together</h3>
<p>The following flow diagram illustrates how V, M, and C interacts with the environment:</p>
<!--During a rollout, it is important to note that C does not directly see the actual observations given to the agent, in this case, a 2D grid of pixels. The actual observation is first processed by V at each timestep $t$ to produce $z_t$. The inputs into C is this latent vector $z_t$ concatenated with M's hidden state $h_t$ at each time step. C will then output an action vector $a_t$ for motor control. M will then take the current $z_t$ and action $a_t$ as an input to update its own hidden state to produce $h_{t+1}$ to be used at time $t+1$. -->
<div id="overview_diagram_div" style="text-align: center;">
<img src="./World Models_files/world_model_schematic.svg" style="display: block; margin: auto; width: 65%;">
<figcaption>Flow diagram of our Agent model. The raw observation is first processed by V at each timestep <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.61508em;"></span><span class="strut bottom" style="height:0.61508em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span></span></span></span> to produce <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>. The inputs into C is this latent vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> concatenated with M’s hidden state <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> at each time step. C will then output an action vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">a_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> for motor control. M will then take the current <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> and action <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">a_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> as an input to update its own hidden state to produce <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">h_{t+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.902771em;vertical-align:-0.208331em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> to be used at time <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span>.</figcaption>
</div>
<p>Below is the pseudocode for how our agent model is used in the <dt-cite key="openai_gym"><span id="citation-27" data-hover-ref="dt-cite-hover-box-27">OpenAI Gym <span class="citation-number">[1]</span></span></dt-cite> environment. Running this function on a given <code>controller</code> C will return the cumulative reward during a rollout in the environment.</p>
<dt-code block="" language="python"><pre><code class="language-python"><span class="token keyword">def</span> <span class="token function">rollout</span><span class="token punctuation">(</span>controller<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">''' env, rnn, vae are '''</span>
  <span class="token triple-quoted-string string">''' global variables  '''</span>
  obs <span class="token operator">=</span> env<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span>
  h <span class="token operator">=</span> rnn<span class="token punctuation">.</span>initial_state<span class="token punctuation">(</span><span class="token punctuation">)</span>
  done <span class="token operator">=</span> <span class="token boolean">False</span>
  cumulative_reward <span class="token operator">=</span> <span class="token number">0</span>
  <span class="token keyword">while</span> <span class="token operator">not</span> done<span class="token punctuation">:</span>
    z <span class="token operator">=</span> vae<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>obs<span class="token punctuation">)</span>
    a <span class="token operator">=</span> controller<span class="token punctuation">.</span>action<span class="token punctuation">(</span><span class="token punctuation">[</span>z<span class="token punctuation">,</span> h<span class="token punctuation">]</span><span class="token punctuation">)</span>
    obs<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> done <span class="token operator">=</span> env<span class="token punctuation">.</span>step<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
    cumulative_reward <span class="token operator">+=</span> reward
    h <span class="token operator">=</span> rnn<span class="token punctuation">.</span>forward<span class="token punctuation">(</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span> z<span class="token punctuation">,</span> h<span class="token punctuation">]</span><span class="token punctuation">)</span>
  <span class="token keyword">return</span> cumulative_reward</code></pre></dt-code>
<p>This minimal design for C also offers important practical benefits. Advances in deep learning provided us with the tools to train large, sophisticated models efficiently, provided we can define a well-behaved, differentiable loss function. Our V and M models are designed to be trained efficiently with the backpropagation algorithm using modern GPU accelerators, so we would like most of the model’s complexity, and model parameters to reside in V and M. The number of parameters of C, a linear model, is minimal in comparison. This choice allows us to explore more unconventional ways to train C — for example, even using evolution strategies (ES)<dt-cite key="Rechenberg1973,Schwefel1977,visuales"><span id="citation-28" data-hover-ref="dt-cite-hover-box-28"><span class="citation-number">[43, 44, 45]</span></span></dt-cite> to tackle more challenging RL tasks where the credit assignment problem is difficult.</p>
<p>To optimize the parameters of C, we chose the Covariance-Matrix Adaptation Evolution Strategy (CMA-ES)<dt-cite key="cmaes,cmaes_original,visuales"><span id="citation-29" data-hover-ref="dt-cite-hover-box-29"><span class="citation-number">[46, 47, 45]</span></span></dt-cite> as our optimization algorithm since it is known to work well for solution spaces of up to a few thousand parameters. We evolved parameters of C on a single machine with multiple CPU cores running multiple rollouts of the environment in parallel.</p>
<p>For more specific information about the models, training procedures, and environments used in our experiments, please refer to the <a href="https://worldmodels.github.io/#appendix">Appendix</a>.</p>
<hr>
<h2>Car Racing Experiment: World Model for Feature Extraction</h2>
<p>A predictive world model can help us extract useful representations of space and time. By using these features as inputs of a controller, we can train a compact and minimal controller to perform a continuous control task, such as learning to drive from pixel inputs for a top-down car racing environment<dt-cite key="carracing_v0"><span id="citation-30" data-hover-ref="dt-cite-hover-box-30"><span class="citation-number">[48]</span></span></dt-cite>. In this section, we describe how we can train the Agent model described earlier to solve this car racing task. To our knowledge, our agent is the first known solution to achieve the required score to solve this task.<dt-fn><sup><span id="fn-4" data-hover-ref="dt-fn-hover-box-3" style="cursor:pointer">4</span></sup></dt-fn></p>
<div style="text-align: center;">
<video autoplay="" muted="" playsinline="" loop="" style="display: block; margin: auto; width: 100%;"><source src="https://storage.googleapis.com/quickdraw-models/sketchRNN/world_models/assets/mp4/carracing_mistake_short.mp4" type="video/mp4"></video>
<figcaption>Our agent learning to navigate a top-down racing environment.<dt-cite key="carracing_v0"><span id="citation-31" data-hover-ref="dt-cite-hover-box-31"><span class="citation-number">[48]</span></span></dt-cite></figcaption>
</div>
<p>In this environment, the tracks are randomly generated for each trial, and our agent is rewarded for visiting as many tiles as possible in the least amount of time. The agent controls three continuous actions: steering left/right, acceleration, and brake.</p>
<p>To train our V model, we first collect a dataset of 10,000 random rollouts in the environment. We have first an agent acting randomly to explore the environment multiple times, and record the random actions <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">a_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> taken and the resulting observations from the environment.<dt-fn><sup><span id="fn-5" data-hover-ref="dt-fn-hover-box-4" style="cursor:pointer">5</span></sup></dt-fn> We use this dataset to train V to learn a latent space of each frame observed. We train our VAE to encode each frame into low dimensional latent vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span> by minimizing the difference between a given frame and the reconstructed version of the frame produced by the decoder from <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span>. The following demo shows the results of our VAE after training:</p>
<div style="text-align: left;">
<div id="carvae_sketch" class="unselectable"><button style="position: absolute; left: 431px; top: 13371.4px;">Load Random Screenshot</button><button style="position: absolute; left: 751px; top: 13371.4px;">Randomize Z</button><canvas id="defaultCanvas5" width="760" height="420" style="width: 760px; height: 420px;"></canvas><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 13036.4px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 13057.8px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 13079.1px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 13100.4px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 13121.8px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 13143.1px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 13164.4px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 13185.8px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 13207.1px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 13228.4px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 13249.8px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 13271.1px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 13292.4px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 13313.8px;"><input type="range" min="-250" max="250" style="width: 100px; position: absolute; left: 748.5px; top: 13335.1px;"></div>
<figcaption style="color:#FF6C00;">Interactive Demo</figcaption>
<figcaption>Our VAE trained on observations from CarRacing-v0<dt-cite key="carracing_v0"><span id="citation-32" data-hover-ref="dt-cite-hover-box-32"><span class="citation-number">[48]</span></span></dt-cite>. Despite losing details during this lossy compression process, latent vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span> captures the essence of each 64x64px image frame.</figcaption>
</div>
<p>We can now use our trained V model to pre-process each frame at time <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.61508em;"></span><span class="strut bottom" style="height:0.61508em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span></span></span></span> into <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> to train our M model. Using this pre-processed data, along with the recorded random actions <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">a_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> taken, our MDN-RNN can now be trained to model <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mspace width="0.277778em"></mspace><mi mathvariant="normal">∣</mi><mspace width="0.277778em"></mspace><msub><mi>a</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>z</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>h</mi><mi>t</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">P(z_{t+1} \; | \; a_t, z_t, h_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mspace thickspace"></span><span class="mord mathrm">∣</span><span class="mord mspace thickspace"></span><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span> as a mixture of Gaussians.<dt-fn><sup><span id="fn-6" data-hover-ref="dt-fn-hover-box-5" style="cursor:pointer">6</span></sup></dt-fn></p>
<p>In this experiment, the world model (V and M) has no knowledge about the actual reward signals in the environment. Its task is simply to compress and predict the sequence of image frames observed. Only the Controller (C) Model has access to the reward information from the environment. Since the there are a mere 867 parameters inside the linear controller model, evolutionary algorithms such as CMA-ES are well suited for this optimization task.</p>
<p>The figure below compares actual the observation given to the agent and the observation captured by the world model. We can use the VAE to reconstruct each frame using <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> at each time step to visualize the quality of the information the agent actually sees during a rollout:</p>
<div>
<video autoplay="" muted="" playsinline="" loop="" style="display: block; margin: auto; width: 100%;"><source src="https://storage.googleapis.com/quickdraw-models/sketchRNN/world_models/assets/mp4/carracing_vae_compare.mp4" type="video/mp4"></video>
<table style="text-align:center;width:100%;border:none">
  <tbody><tr>
    <td style="width:50%;border:none"><figcaption>Actual observations from the environment.</figcaption></td>
    <td style="width:50%;border:none"><figcaption>What gets encoded into <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>.</figcaption></td>
  </tr>
</tbody></table>
</div>
<hr>
<h2>Procedure</h2>
<p>To summarize the Car Racing experiment, below are the steps taken:</p>
<ol>
<li>Collect 10,000 rollouts from a random policy.</li>
<li>Train VAE (V) to encode frames to latent vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi><mo>∈</mo><msup><mrow><mi mathvariant="script">R</mi></mrow><mrow><mn>3</mn><mn>2</mn></mrow></msup></mrow><annotation encoding="application/x-tex">z \in \mathcal{R}^{32}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:0.853208em;vertical-align:-0.0391em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mrel">∈</span><span class=""><span class="mord textstyle uncramped"><span class="mord mathcal">R</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">3</span><span class="mord mathrm">2</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>.</li>
<li>Train MDN-RNN (M) to model <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mspace width="0.277778em"></mspace><mi mathvariant="normal">∣</mi><mspace width="0.277778em"></mspace><msub><mi>a</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>z</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>h</mi><mi>t</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">P(z_{t+1} \; | \; a_t, z_t, h_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mspace thickspace"></span><span class="mord mathrm">∣</span><span class="mord mspace thickspace"></span><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span>.</li>
<li>Define Controller (C) as <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>=</mo><msub><mi>W</mi><mi>c</mi></msub><mspace width="0.277778em"></mspace><mo>[</mo><msub><mi>z</mi><mi>t</mi></msub><mspace width="0.277778em"></mspace><msub><mi>h</mi><mi>t</mi></msub><mo>]</mo><mspace width="0.277778em"></mspace><mo>+</mo><mspace width="0.277778em"></mspace><msub><mi>b</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">a_t = W_c \; [z_t \; h_t]\; + \; b_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">c</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mspace thickspace"></span><span class="mopen">[</span><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mspace thickspace"></span><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">]</span><span class="mord mspace thickspace"></span><span class="mbin">+</span><span class="mord mspace thickspace"></span><span class="mord"><span class="mord mathit">b</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">c</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>.</li>
<li>Use CMA-ES to solve for a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">W_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">c</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>b</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">b_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">b</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">c</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> that maximizes the expected cumulative reward.</li>
</ol>
<table style="text-align:left;width:320px;">
  <tbody><tr>
    <th>Model</th>
    <th>Parameter Count</th>
  </tr>
  <tr>
    <td>VAE</td>
    <td>4,348,547</td>
  </tr>
  <tr>
    <td>MDN-RNN</td>
    <td>422,368</td>
  </tr>
  <tr>
    <td>Controller</td>
    <td>867</td>
  </tr>
</tbody></table>
<hr>
<h2>Car Racing Experiment Results</h2>
<h3>V Model Only</h3>
<p>Training an agent to drive is not a difficult task if we have a good representation of the observation. Previous works<dt-cite key="browser_car,mar_io_kart,keras_car"><span id="citation-33" data-hover-ref="dt-cite-hover-box-33"><span class="citation-number">[49, 50, 51]</span></span></dt-cite> have shown that with a good set of hand-engineered information about the observation, such as LIDAR information, angles, positions and velocities, one can easily train a small feed-forward network to take this hand-engineered input and output a satisfactory navigation policy. For this reason, we first want to test our agent by handicapping C only to have access to V but not M, so we define our controller as <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>=</mo><msub><mi>W</mi><mi>c</mi></msub><mspace width="0.277778em"></mspace><msub><mi>z</mi><mi>t</mi></msub><mspace width="0.277778em"></mspace><mo>+</mo><mspace width="0.277778em"></mspace><msub><mi>b</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">a_t = W_c \; z_t \;+ \; b_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">c</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mspace thickspace"></span><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mspace thickspace"></span><span class="mbin">+</span><span class="mord mspace thickspace"></span><span class="mord"><span class="mord mathit">b</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">c</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>.</p>
<!--The agent only sees $z_t$ produced by the V model.-->
<div>
<video autoplay="" muted="" playsinline="" loop="" style="display: block; margin: auto; width: 100%;"><source src="https://storage.googleapis.com/quickdraw-models/sketchRNN/world_models/assets/mp4/carracing_z_only.mp4" type="video/mp4"></video>
<figcaption>Limiting our controller to see only <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>, but not <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> results in wobbly and unstable driving behaviours. </figcaption>
</div>
<p>Although the agent is still able to navigate the race track in this setting, we notice it wobbles around and misses the tracks on sharper corners. This handicapped agent achieved an average score of 632 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 251 over 100 random trials, in line with the performance of other agents on OpenAI Gym’s leaderboard<dt-cite key="carracing_v0"><span id="citation-34" data-hover-ref="dt-cite-hover-box-34"><span class="citation-number">[48]</span></span></dt-cite> and traditional Deep RL methods such as A3C<dt-cite key="carracing_cs221,carracing_cs234"><span id="citation-35" data-hover-ref="dt-cite-hover-box-35"><span class="citation-number">[52, 53]</span></span></dt-cite>. Adding a hidden layer to the C model’s policy network improves the results to 788 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 141, not enough to solve the environment.</p>
<h3>Full World Model (V and M)</h3>
<p>The representation <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> provided by our V model only captures a representation at a moment in time and doesn’t have much predictive power. In contrast, M is trained to do one thing, and to do it really well, which is to predict <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">z_{t+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>. Since M’s prediction of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">z_{t+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> are produced from the RNN’s hidden state <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> at time <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.61508em;"></span><span class="strut bottom" style="height:0.61508em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span></span></span></span>, this vector is a good candidate for the set of learned features we can give to our agent. Combining <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> with <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> gives our controller C a good representation of both the current observation, and what to expect in the future.</p>
<!--The agent only sees both $z_t$ and $h_t$.-->
<div>
<video autoplay="" muted="" playsinline="" loop="" style="display: block; margin: auto; width: 100%;"><source src="https://storage.googleapis.com/quickdraw-models/sketchRNN/world_models/assets/mp4/carracing_z_and_h.mp4" type="video/mp4"></video>
<figcaption>Driving is more stable if we give our controller access to both <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>.</figcaption>
</div>
<p>Indeed, we see that allowing the agent to access the both <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> greatly improves its driving capability. The driving is more stable, and the agent is able to seemingly attack the sharp corners effectively. Furthermore, we see that in making these fast reflexive driving decisions during a car race, the agent does not need to <em>plan ahead</em> and roll out hypothetical scenarios of the future. Since <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> contain information about the probability distribution of the future, the agent can just query the RNN instinctively to guide its action decisions. Like a seasoned Formula One driver or the baseball player discussed earlier, the agent can instinctively predict when and where to navigate in the heat of the moment.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mspace width="0.277778em"></mspace><mspace width="0.277778em"></mspace></mrow><annotation encoding="application/x-tex">\;\;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0em;"></span><span class="strut bottom" style="height:0em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mspace thickspace"></span><span class="mord mspace thickspace"></span></span></span></span> Average Score over 100 Random Tracks <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mspace width="0.277778em"></mspace><mspace width="0.277778em"></mspace></mrow><annotation encoding="application/x-tex">\;\;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0em;"></span><span class="strut bottom" style="height:0em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mspace thickspace"></span><span class="mord mspace thickspace"></span></span></span></span></th>
</tr>
</thead>
<tbody>
<tr>
<td>DQN<dt-cite key="dqn_racecar"><span id="citation-36" data-hover-ref="dt-cite-hover-box-36"><span class="citation-number">[54]</span></span></dt-cite></td>
<td><div style="text-align: center;">343 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 18</div></td>
</tr>
<tr>
<td>A3C (continuous)<dt-cite key="carracing_cs234"><span id="citation-37" data-hover-ref="dt-cite-hover-box-37"><span class="citation-number">[53]</span></span></dt-cite></td>
<td><div style="text-align: center;">591 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 45</div></td>
</tr>
<tr>
<td>A3C (discrete)<dt-cite key="carracing_cs221"><span id="citation-38" data-hover-ref="dt-cite-hover-box-38"><span class="citation-number">[52]</span></span></dt-cite></td>
<td><div style="text-align: center;">652 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 10</div></td>
</tr>
<tr>
<td>ceobillionaire’s algorithm (unpublished)<dt-cite key="carracing_v0"><span id="citation-39" data-hover-ref="dt-cite-hover-box-39"><span class="citation-number">[48]</span></span></dt-cite></td>
<td><div style="text-align: center;">838 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 11</div></td>
</tr>
<tr>
<td>V model only, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span> input</td>
<td><div style="text-align: center;">632 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 251</div></td>
</tr>
<tr>
<td>V model only, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span> input with a hidden layer</td>
<td><div style="text-align: center;">788 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 141</div></td>
</tr>
<tr>
<td><strong>Full World Model</strong>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">h</span></span></span></span></td>
<td><div style="text-align: center;"><strong>906 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 21</strong></div></td>
</tr>
</tbody>
</table>
<p>Our agent was able to achieve a score of 906 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 21 over 100 random trials, effectively solving the task and obtaining new state of the art results. Previous attempts<dt-cite key="carracing_cs221,carracing_cs234"><span id="citation-40" data-hover-ref="dt-cite-hover-box-40"><span class="citation-number">[52, 53]</span></span></dt-cite> using traditional Deep RL methods obtained average scores of 591—652 range, and the best reported solution on the <dt-cite key="carracing_v0"><span id="citation-41" data-hover-ref="dt-cite-hover-box-41">leaderboard <span class="citation-number">[48]</span></span></dt-cite> obtained an average score of 838 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 11 over 100 random consecutive trials. Traditional Deep RL methods often require pre-processing of each frame, such as employing edge-detection<dt-cite key="carracing_cs234"><span id="citation-42" data-hover-ref="dt-cite-hover-box-42"><span class="citation-number">[53]</span></span></dt-cite>, in addition to stacking a few recent frames<dt-cite key="carracing_cs221,carracing_cs234"><span id="citation-43" data-hover-ref="dt-cite-hover-box-43"><span class="citation-number">[52, 53]</span></span></dt-cite> into the input. In contrast, our world model takes in a stream of raw RGB pixel images and directly learns a spatial-temporal representation. To our knowledge, our method is the first reported solution to solve this task.</p>
<hr>
<h2>Car Racing Dreams</h2>
<p>Since our world model is able to model the future, we are able to have it hallucinate hypothetical car racing scenarios on its own. We can ask it to produce the probability distribution of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">z_{t+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> given the current states, <em>sample</em> a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">z_{t+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> and use this sample as the real observation. We can put our trained C back into this hallucinated environment generated by M. The following demo shows how our world model can be used to hallucinate the car racing environment:</p>
<div style="text-align: left;">
<div id="carrnn_sketch" class="unselectable"><canvas id="defaultCanvas1" width="648" height="648" style="width: 648px; height: 648px;"></canvas><input type="range" min="25" max="125" style="position: absolute; left: 907px; top: 19305.6px; width: 152px;"></div>
<figcaption style="color:#FF6C00;">Interactive Demo</figcaption>
<figcaption>Our agent driving inside its own dream. Here, we deploy our trained policy into a fake environment generated by the MDN-RNN, and rendered using the VAE’s decoder. You can override the agent’s actions by tapping on the left or right side of the screen, or by hitting arrow keys (left/right to steer, up/down to accelerate or brake). The uncertainty level of the environment can be adjusted by changing <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span></span></span> using the slider on the bottom right.</figcaption>
</div>
<p>We have just seen that a policy learned in the real environment appears to somewhat function inside of the dream environment. This begs the question — can we train our agent to learn inside of its own dream, and transfer this policy back to the actual environment?</p>
<hr>
<h2>VizDoom Experiment: Learning Inside of a Dream</h2>
<p>If our world model is sufficiently accurate for its purpose, and complete enough for the problem at hand, we should be able to substitute the actual environment with this world model. After all, our agent does not directly observe the reality, but only sees what the world model lets it see. In this experiment, we train an agent inside the hallucination generated by its world model trained to mimic a VizDoom<dt-cite key="vizdoom"><span id="citation-44" data-hover-ref="dt-cite-hover-box-44"><span class="citation-number">[34]</span></span></dt-cite> environment.</p>
<div style="text-align: center;">
<video autoplay="" muted="" playsinline="" loop="" style="display: block; margin: auto; width: 100%;"><source src="https://storage.googleapis.com/quickdraw-models/sketchRNN/world_models/assets/mp4/doom_lazy_small.mp4" type="video/mp4"></video>
<figcaption>Our final agent solving the <i>VizDoom: Take Cover</i> environment.<dt-cite key="vizdoom,takecover"><span id="citation-45" data-hover-ref="dt-cite-hover-box-45"><span class="citation-number">[34, 35]</span></span></dt-cite></figcaption>
</div>
<p>The agent must learn to avoid fireballs shot by monsters from the other side of the room with the sole intent of killing the agent. There are no explicit rewards in this environment, so to mimic natural selection, the cumulative reward can be defined to be the number of time steps the agent manages to stay alive in a rollout. Each rollout in the environment runs for a maximum of 2100 frames (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>∼</mo></mrow><annotation encoding="application/x-tex">\sim</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.36687em;"></span><span class="strut bottom" style="height:0.36687em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mrel">∼</span></span></span></span> 60 seconds), and the task is considered solved when the average survival time over 100 consecutive rollouts is greater than 750 frames (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>∼</mo></mrow><annotation encoding="application/x-tex">\sim</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.36687em;"></span><span class="strut bottom" style="height:0.36687em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mrel">∼</span></span></span></span> 20 seconds)<dt-cite key="takecover"><span id="citation-46" data-hover-ref="dt-cite-hover-box-46"><span class="citation-number">[35]</span></span></dt-cite>.</p>
<hr>
<h2>Procedure</h2>
<p>Our VizDoom experiment is largely the same as the Car Racing task, except for a few key differences. In the Car Racing task, M is only trained to model the next <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mrow><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">z_{t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>. Since we want to build a world model we can train our agent in, our M model here will also predict whether the agent dies in the next frame (as a binary event <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi><mi>o</mi><mi>n</mi><msub><mi>e</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">done_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord mathit">d</span><span class="mord mathit">o</span><span class="mord mathit">n</span><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>), in addition to the next frame <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>.</p>
<p>Since the M model can predict the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi><mi>o</mi><mi>n</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">done</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">d</span><span class="mord mathit">o</span><span class="mord mathit">n</span><span class="mord mathit">e</span></span></span></span> state in addition to the next observation, we now have all of the ingredients needed to make a full RL environment. We first build an OpenAI Gym environment interface by wrapping a <code>gym.Env</code><dt-cite key="openai_gym"><span id="citation-47" data-hover-ref="dt-cite-hover-box-47"><span class="citation-number">[1]</span></span></dt-cite> interface over our M if it were a real Gym environment, and then train our agent in this virtual environment instead of using the actual environment.</p>
<p>In this simulation, we don’t need the V model to encode any real pixel frames during the hallucination process, so our agent will therefore only train entirely in a latent space environment. This has many advantages that will be discussed later on.</p>
<p>This virtual environment has an identical interface to the real environment, so after the agent learns a satisfactory policy in the virtual environment, we can easily deploy this policy back into the actual environment to see how well the policy transfers over.</p>
<p>To summarize the <em>Take Cover</em> experiment, below are the steps taken:</p>
<ol>
<li>Collect 10,000 rollouts from a random policy.</li>
<li>Train VAE (V) to encode frames to latent vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi><mo>∈</mo><msup><mrow><mi mathvariant="script">R</mi></mrow><mrow><mn>6</mn><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">z \in \mathcal{R}^{64}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:0.853208em;vertical-align:-0.0391em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mrel">∈</span><span class=""><span class="mord textstyle uncramped"><span class="mord mathcal">R</span></span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">6</span><span class="mord mathrm">4</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>, and use V to convert the images collected from (1) into latent space representation.</li>
<li>Train MDN-RNN (M) to model <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><mi>d</mi><mi>o</mi><mi>n</mi><msub><mi>e</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mspace width="0.277778em"></mspace><mi mathvariant="normal">∣</mi><mspace width="0.277778em"></mspace><msub><mi>a</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>z</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>h</mi><mi>t</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">P(z_{t+1}, done_{t+1} \; | \; a_t, z_t, h_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathit">d</span><span class="mord mathit">o</span><span class="mord mathit">n</span><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mspace thickspace"></span><span class="mord mathrm">∣</span><span class="mord mspace thickspace"></span><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span>.</li>
<li>Define Controller (C) as <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>=</mo><msub><mi>W</mi><mi>c</mi></msub><mspace width="0.277778em"></mspace><mo>[</mo><msub><mi>z</mi><mi>t</mi></msub><mspace width="0.277778em"></mspace><msub><mi>h</mi><mi>t</mi></msub><mo>]</mo></mrow><annotation encoding="application/x-tex">a_t = W_c \; [z_t \; h_t]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">c</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mspace thickspace"></span><span class="mopen">[</span><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mspace thickspace"></span><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">]</span></span></span></span>.</li>
<li>Use CMA-ES to solve for a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">W_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">c</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> that maximizes the expected survival time inside the virtual environment.</li>
<li>Use learned policy from (5) on actual Gym environment.</li>
</ol>
<table style="text-align:left;width:320px;">
  <tbody><tr>
    <th>Model</th>
    <th>Parameter Count</th>
  </tr>
  <tr>
    <td>VAE</td>
    <td>4,446,915</td>
  </tr>
  <tr>
    <td>MDN-RNN</td>
    <td>1,678,785</td>
  </tr>
  <tr>
    <td>Controller</td>
    <td>1,088</td>
  </tr>
</tbody></table>
<hr>
<h2>Training Inside of the Dream</h2>
<p>After some training, our controller learns to navigate around the dream environment and escape from deadly fireballs shot by monsters generated by the M model.<!--a policy where it can avoid fireballs
by moving from one side of the room to the other side of the room. Since the monsters shoot at where the player is located, rather than where the player will be in the future, this is a reasonable policy, and also a strategy that human players often discover as well.<dt-fn>This is also the policy that the author discovered while playing the game himself.</dt-fn>--> Our agent achieved a <em>score</em> in this virtual environment of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>∼</mo></mrow><annotation encoding="application/x-tex">\sim</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.36687em;"></span><span class="strut bottom" style="height:0.36687em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mrel">∼</span></span></span></span> 900 frames.</p>
<p>The following demo shows how our agent navigates inside its own dream. The M model learns to generate monsters that shoot fireballs at the direction of the agent, while the C model discovers a policy to avoid these hallucinated fireballs. Here, the V model is only used to decode the latent vectors <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> produced by M into a sequence of pixel images we can see:</p>
<div style="text-align: left;">
<div id="doomrnn_sketch" class="unselectable"><canvas id="defaultCanvas2" width="648" height="648" style="width: 648px; height: 648px;"></canvas><input type="range" min="50" max="300" style="position: absolute; left: 907px; top: 23433.6px; width: 152px;"></div>
<figcaption style="color:#FF6C00;">Interactive Demo</figcaption>
<figcaption>Our agent discovers a policy to avoid hallucinated fireballs. In this demo, you can override the agent’s action by using the left/right keys on your keyboard, or by tapping on either side of the screen. You can also control the uncertainty level of the environment by adjusting the temperature parameter using slider on the bottom right.</figcaption>
</div>
<p>Here, our RNN-based world model is trained to mimic a complete game environment designed by human programmers. By learning only from raw image data collected from random episodes, it learns how to simulate the essential aspects of the game — such as the game logic, enemy behaviour, physics, and the 3D graphics rendering.</p>
<p>For instance, if the agent selects the left action, the M model learns to move the agent to the left and adjust its internal representation of the game states accordingly. It also learns to block the agent from moving beyond the walls on both sides of the level if the agent attempts to move too far in either direction. Occasionally, the M model needs to keep track of multiple fireballs being shot from several different monsters and coherently move them along in their intended directions. It must also detect whether the agent has been killed by one of these fireballs.</p>
<p>Unlike the actual game environment, however, we note that it is possible to add extra uncertainty into the virtual environment, thus making the game more challenging in the dream environment. We can do this by increasing the temperature <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span></span></span> parameter during the sampling process of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">z_{t+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>, as done in <dt-cite key="sketchrnn"><span id="citation-48" data-hover-ref="dt-cite-hover-box-48"><span class="citation-number">[36]</span></span></dt-cite>. By increasing the uncertainty, our dream environment becomes more difficult compared to the actual environment. The fireballs may move more randomly in a less predictable path compared to the actual game. Sometimes the agent may even die due to sheer misfortune, without explanation.</p>
<p>We find agents that perform well in higher temperature settings generally perform better in the normal setting. In fact, increasing <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span></span></span> helps prevent our controller from taking advantage of the imperfections of our world model — we will discuss this in more depth later on.</p>
<hr>
<h2>Transfer Policy to Actual Environment</h2>
<div>
<video autoplay="" muted="" playsinline="" loop="" style="display: block; margin: auto; width: 100%;"><source src="https://storage.googleapis.com/quickdraw-models/sketchRNN/world_models/assets/mp4/doom_real_deploy.mp4" type="video/mp4"></video>
<figcaption>Deploying our policy learned in the dream RNN environment back into the actual VizDoom environment.</figcaption>
</div>
<p>We took the agent trained in the virtual environment and tested its performance on the original VizDoom scenario. The score over 100 random consecutive trials is <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>∼</mo></mrow><annotation encoding="application/x-tex">\sim</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.36687em;"></span><span class="strut bottom" style="height:0.36687em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mrel">∼</span></span></span></span> 1100 frames, far beyond the required score of 750, and also much higher than the score obtained inside the more difficult virtual environment.<dt-fn><sup><span id="fn-7" data-hover-ref="dt-fn-hover-box-6" style="cursor:pointer">7</span></sup></dt-fn></p>
<div>
<video autoplay="" muted="" playsinline="" loop="" style="display: block; margin: auto; width: 100%;"><source src="https://storage.googleapis.com/quickdraw-models/sketchRNN/world_models/assets/mp4/doom_real_vae.mp4" type="video/mp4"></video>
<table style="text-align:center;width:100%;border:none">
  <tbody><tr>
    <td style="width:50%;border:none"><figcaption>Cropped 64x64px frame of environment.</figcaption></td>
    <td style="width:50%;border:none"><figcaption>Reconstruction from latent vector.</figcaption></td>
  </tr>
</tbody></table>
</div>
<p>We see that even though the V model is not able to capture all of the details of each frame correctly, for instance, getting the number of monsters correct, the agent is still able to use the learned policy to navigate in the real environment. The virtual environment also did not keep track of a clear number of monsters in the first place, and an agent that is able to survive the noisier and uncertain virtual nightmare environment will thrive in this clean, noiseless environment.</p>
<hr>
<h2>Cheating the World Model</h2>
<p>In our childhood, we may have encountered ways to exploit video games in ways that were not intended by the original game designer<dt-cite key="video_game_exploits"><span id="citation-49" data-hover-ref="dt-cite-hover-box-49"><span class="citation-number">[55]</span></span></dt-cite>. Players discover ways to collect unlimited lives or health, and by taking advantage of these exploits, they can easily complete an otherwise difficult game. However, in the process of doing so, they may have forfeited the opportunity to learn the skill required to master the game as intended by the game designer.</p>
<div>
<video autoplay="" muted="" playsinline="" loop="" style="display: block; margin: auto; width: 100%;"><source src="https://storage.googleapis.com/quickdraw-models/sketchRNN/world_models/assets/mp4/doom_adversarial.mp4" type="video/mp4"></video>
<figcaption>Agent discovers an adversarial policy that fools the monsters inside the world model into never launching any fireballs during some rollouts.</figcaption>
</div>
<p>For instance, in our initial experiments, we noticed that our agent discovered an <em>adversarial</em> policy to move around in such a way so that the monsters in this virtual environment governed by the M model never shoots a single fireball in some rollouts. Even when there are signs of a fireball forming, the agent will move in a way to extinguish the fireballs magically as if it has superpowers in the environment.</p>
<p>Because our world model is only an approximate probabilistic model of the environment, it will occasionally generate trajectories that do not follow the laws governing the actual environment. As we saw previously, even the number of monsters on the other side of the room in the actual environment is not exactly reproduced by the world model. Like a child who learns that objects in the air usually fall to the ground, the child might also imagine unrealistic superheroes who fly across the sky. For this reason, our world model will be exploitable by the controller, even if in the actual environment such exploits do not exist.</p>
<p>And since we are using the M model to generate a virtual dream environment for our agent, we are also giving the controller access to all of the hidden states of M. This is essentially granting our agent access to all of the internal states and memory of the game engine of the game it is playing. Therefore our agent can efficiently explore ways to directly manipulate the hidden states of the game engine in its quest to maximize its expected cumulative reward. The weakness of this approach of learning a policy inside a learned dynamics model is that our agent can easily find an adversarial policy that can fool our dynamics model — it’ll find a policy that looks good under our dynamics model, but will fail in the actual environment, usually because it visits states where the model is wrong because they are away from the training distribution.</p>
<p>This weakness could be the reason that many previous works that learn dynamics models of RL environments but don’t actually use those models to fully replace the actual environments <dt-cite key="action_conditional_video_prediction,recurrent_env_sim"><span id="citation-50" data-hover-ref="dt-cite-hover-box-50"><span class="citation-number">[56, 57]</span></span></dt-cite>. Like in the M model proposed in 1990 <dt-cite key="s05_making_the_world_differentiable,s05a_cm,s05b_rl"><span id="citation-51" data-hover-ref="dt-cite-hover-box-51"><span class="citation-number">[22, 23, 24]</span></span></dt-cite>, the dynamics model is a deterministic differentiable model, making the model easy for the agent to exploit if it is not perfect. Using Bayesian models (as in PILCO<dt-cite key="pilco"><span id="citation-52" data-hover-ref="dt-cite-hover-box-52"><span class="citation-number">[58]</span></span></dt-cite>) helps to address this issue with the uncertainty estimates to some extent, however, they don’t fully solve the problem. Recent work<dt-cite key="Nagabandi2017"><span id="citation-53" data-hover-ref="dt-cite-hover-box-53"><span class="citation-number">[59]</span></span></dt-cite> combines the model-based approach with traditional model-free RL training by first initializing the policy network with the learned policy, but must subsequently rely on a model-free method to fine-tune this policy in the actual environment.<dt-fn><sup><span id="fn-8" data-hover-ref="dt-fn-hover-box-7" style="cursor:pointer">8</span></sup></dt-fn></p>
<!--In *Learning to Think*<dt-cite key="learning_to_think"></dt-cite>, it is acceptable that the RNN M isn't always a reliable predictor. A (potentially evolution-based) RNN C can in principle learn to ignore a flawed M, or exploit certain useful parts of M for arbitrary computational purposes including hierarchical planning etc. This is not what we do here though - - our present approach is still closer to some of the old systems<dt-cite key="s05_making_the_world_differentiable,s05a_cm,s05b_rl"></dt-cite>, where a RNN M is used to predict and plan ahead step by step. Unlike this early work, however, we use evolution for C (like in *Learning to Think*<dt-cite key="learning_to_think"></dt-cite>) rather than traditional RL combined with RNNs, which has the advantage of both simplicity and generality.-->
<p>To make it more difficult for our C model to exploit deficiencies in the M model, we chose to use the MDN-RNN as the dynamics model, which models the <em>distribution</em> of possible outcomes in the actual environment, rather than merely predicting a deterministic future. Even if the actual environment is deterministic, the MDN-RNN would in effect approximate it as a stochastic environment. This has the advantage of allowing us to train our C model inside a more stochastic version of any environment — we can simply adjust the temperature <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span></span></span> parameter to control the amount of randomness in the M model, hence controlling the tradeoff between realism and exploitability.</p>
<p>Using a mixture of Gaussian model may seem like overkill given that the latent space encoded with the VAE model is just a diagonal Gaussian. However, the discrete modes in a mixture density model is useful for environments with random discrete events, such as whether a monster decides to shoot a fireball or stay put. While a Gaussian might be sufficient to encode individual frames, a RNN with a mixture density output layer makes it easier to model the logic behind a more complicated environment with discrete random states.</p>
<p>For instance, if we set the temperature parameter to a very low value of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>τ</mi><mo>=</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">\tau=0.1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="mrel">=</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">1</span></span></span></span>, effectively training our C model with an M model that is almost identical to a deterministic LSTM, the monsters inside this dream environment fail to shoot fireballs, no matter what the agent does, due to mode collapse. The M model is not able to <em>jump</em> to another mode in the mixture of Gaussian model where fireballs are formed and shot. Whatever policy trained in this dream will get a perfect score of 2100 most of the time, but will obviously fail when unleashed into the harsh reality of the actual world, underperforming even a random policy.</p>
<p>In the following demo, we show that even low values of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>τ</mi><mo>∼</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>5</mn></mrow><annotation encoding="application/x-tex">\tau \sim 0.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="mrel">∼</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">5</span></span></span></span> make it difficult for the MDN-RNN to generate fireballs:</p>
<div style="text-align: center;">
<div id="doomrnn_cheating_sketch" class="unselectable"><canvas id="defaultCanvas3" width="648" height="648" style="width: 648px; height: 648px;"></canvas><input type="range" min="50" max="100" style="position: absolute; left: 907px; top: 29996.6px; width: 152px;"></div>
<figcaption style="color:#FF6C00;">Interactive Demo</figcaption>
<figcaption>For low <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span></span></span> settings, monsters in the M model rarely shoot fireballs. Even when you try to increase <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span></span></span> to 1.0 using the slider bar, the agent will occasionally extinguish fireballs still being formed, by fooling M.</figcaption>
</div>
<p>Note again, however, that the simpler and more robust approach in <dt-cite key="learning_to_think"><span id="citation-54" data-hover-ref="dt-cite-hover-box-54">Learning to Think <span class="citation-number">[31]</span></span></dt-cite> does not insist on using M for step by step planning. Instead, C can learn to use M’s <em>subroutines</em> (parts of M’s weight matrix) for arbitrary computational purposes but can also learn to ignore M when M is useless and when ignoring M yields better performance. Nevertheless, at least in our present C—M variant, M’s predictions are essential for teaching C, more like in some of the early C—M systems<dt-cite key="s05_making_the_world_differentiable,s05a_cm,s05b_rl"><span id="citation-55" data-hover-ref="dt-cite-hover-box-55"><span class="citation-number">[22, 23, 24]</span></span></dt-cite>, but combined with evolution or black box optimization.</p>
<p>By making the temperature <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span></span></span> an adjustable parameter of the M model, we can see the effect of training the C model on hallucinated virtual environments with different levels of uncertainty, and see how well they transfer over to the actual environment. We experimented with varying the temperature in the virtual environment and observing the resulting average score over 100 random rollouts in the actual environment after training the agent inside the virtual environment with a given temperature:</p>
<table>
<thead>
<tr>
<th><div style="text-align: center;"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mspace width="0.277778em"></mspace><mspace width="0.277778em"></mspace></mrow><annotation encoding="application/x-tex">\;\;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0em;"></span><span class="strut bottom" style="height:0em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mspace thickspace"></span><span class="mord mspace thickspace"></span></span></span></span>Temperature<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mspace width="0.277778em"></mspace><mspace width="0.277778em"></mspace></mrow><annotation encoding="application/x-tex">\;\;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0em;"></span><span class="strut bottom" style="height:0em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mspace thickspace"></span><span class="mord mspace thickspace"></span></span></span></span></div></th>
<th><div style="text-align: center;"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mspace width="0.277778em"></mspace><mspace width="0.277778em"></mspace></mrow><annotation encoding="application/x-tex">\;\;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0em;"></span><span class="strut bottom" style="height:0em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mspace thickspace"></span><span class="mord mspace thickspace"></span></span></span></span> Score in Virtual Environment</div></th>
<th><div style="text-align: center;"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mspace width="0.277778em"></mspace><mspace width="0.277778em"></mspace></mrow><annotation encoding="application/x-tex">\;\;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0em;"></span><span class="strut bottom" style="height:0em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mspace thickspace"></span><span class="mord mspace thickspace"></span></span></span></span>Score in Actual Environment<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mspace width="0.277778em"></mspace><mspace width="0.277778em"></mspace></mrow><annotation encoding="application/x-tex">\;\;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0em;"></span><span class="strut bottom" style="height:0em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mspace thickspace"></span><span class="mord mspace thickspace"></span></span></span></span></div></th>
</tr>
</thead>
<tbody>
<tr>
<td><div style="text-align: center;">0.10</div></td>
<td><div style="text-align: center;">2086 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 140</div></td>
<td><div style="text-align: center;">193 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 58</div></td>
</tr>
<tr>
<td><div style="text-align: center;">0.50</div></td>
<td><div style="text-align: center;">2060 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 277</div></td>
<td><div style="text-align: center;">196 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 50</div></td>
</tr>
<tr>
<td><div style="text-align: center;">1.00</div></td>
<td><div style="text-align: center;">1145 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 690</div></td>
<td><div style="text-align: center;">868 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 511</div></td>
</tr>
<tr>
<td><div style="text-align: center;">1.15</div></td>
<td><div style="text-align: center;">918 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 546</div></td>
<td><div style="text-align: center;">1092 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 556</div></td>
</tr>
<tr>
<td><div style="text-align: center;">1.30</div></td>
<td><div style="text-align: center;">732 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 269</div></td>
<td><div style="text-align: center;">753 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 139</div></td>
</tr>
<tr>
<td><div style="text-align: center;">Random Policy Baseline</div></td>
<td><div style="text-align: center;">N/A</div></td>
<td><div style="text-align: center;">210 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 108</div></td>
</tr>
<tr>
<td><div style="text-align: center;">Gym Leaderboard<dt-cite key="takecover"><span id="citation-56" data-hover-ref="dt-cite-hover-box-56"><span class="citation-number">[35]</span></span></dt-cite></div></td>
<td><div style="text-align: center;">N/A</div></td>
<td><div style="text-align: center;">820 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 58</div></td>
</tr>
</tbody>
</table>
<p>We see that while increasing the temperature of the M model makes it more difficult for the C model to find adversarial policies, increasing it too much will make the virtual environment too difficult for the agent to learn anything, hence in practice it is a hyperparameter we can tune. The temperature also affects the types of strategies the agent discovers. For example, although the best score obtained is 1092 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 556 over 100 random trials using a temperature of 1.15, increasing <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span></span></span> a notch to 1.30 results in a lower score but at the same time a less risky strategy with a lower variance of returns. For comparison, the best score on the OpenAI Gym leaderboard<dt-cite key="takecover"><span id="citation-57" data-hover-ref="dt-cite-hover-box-57"><span class="citation-number">[35]</span></span></dt-cite> is 820 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 58.</p>
<hr>
<h2>Iterative Training Procedure</h2>
<p>In our experiments, the tasks are relatively simple, so a reasonable world model can be trained using a dataset collected from a random policy. But what if our environments become more sophisticated? In any difficult environment, only parts of the world are made available to the agent only after it learns how to strategically navigate through its world.</p>
<p>For more complicated tasks, an iterative training procedure is required. We need our agent to be able to explore its world, and constantly collect new observations so that its world model can be improved and refined over time. An iterative training procedure, adapted from <em>Learning To Think</em><dt-cite key="learning_to_think"><span id="citation-58" data-hover-ref="dt-cite-hover-box-58"><span class="citation-number">[31]</span></span></dt-cite> is as follows:</p>
<ol>
<li>Initialize M, C with random model parameters.</li>
<li>Rollout to actual environment <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">N</span></span></span></span> times. Agent may learn during rollouts. Save all actions <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">a_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> and observations <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> during rollouts to storage device.</li>
<li>Train M to model <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>r</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>a</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><mi>d</mi><mi>o</mi><mi>n</mi><msub><mi>e</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mspace width="0.277778em"></mspace><mi mathvariant="normal">∣</mi><mspace width="0.277778em"></mspace><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>a</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>h</mi><mi>t</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">P(x_{t+1}, r_{t+1}, a_{t+1}, done_{t+1} \; | \; x_t, a_t, h_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathit">d</span><span class="mord mathit">o</span><span class="mord mathit">n</span><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mspace thickspace"></span><span class="mord mathrm">∣</span><span class="mord mspace thickspace"></span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span>.</li>
<li>Go back to (2) if task has not been completed.</li>
</ol>
<p>We have shown that one iteration of this training loop was enough to solve simple tasks. For more difficult tasks, we need our controller in Step 2 to actively explore parts of the environment that is beneficial to improve its world model. An exciting research direction is to look at ways to incorporate artificial curiosity and intrinsic motivation<dt-cite key="schmidhuber_creativity,s07_intrinsic,s08_curiousity,pathak2017,intrinsic_motivation"><span id="citation-59" data-hover-ref="dt-cite-hover-box-59"><span class="citation-number">[60, 61, 62, 63, 64]</span></span></dt-cite> and information seeking<dt-cite key="SchmidhuberStorck:94,Gottlieb2013"><span id="citation-60" data-hover-ref="dt-cite-hover-box-60"><span class="citation-number">[65, 66]</span></span></dt-cite> abilities in an agent to encourage novel exploration<dt-cite key="Lehman2011"><span id="citation-61" data-hover-ref="dt-cite-hover-box-61"><span class="citation-number">[67]</span></span></dt-cite>. In particular, we can augment the reward function based on improvement in compression quality<dt-cite key="schmidhuber_creativity,s07_intrinsic,s08_curiousity,learning_to_think"><span id="citation-62" data-hover-ref="dt-cite-hover-box-62"><span class="citation-number">[60, 61, 62, 31]</span></span></dt-cite>.</p>
<p>In the present approach, since M is a MDN-RNN that models a probability distribution for the next frame, if it does a poor job, then it means the agent has encountered parts of the world that it is not familiar with. Therefore we can adapt and reuse M’s training loss function to encourage curiosity. By flipping the sign of M’s loss function in the actual environment, the agent will be encouraged to explore parts of the world that it is not familiar with. The new data it collects may improve the world model.</p>
<p>The iterative training procedure requires the M model to not only predict the next observation <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi><mi>o</mi><mi>n</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">done</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">d</span><span class="mord mathit">o</span><span class="mord mathit">n</span><span class="mord mathit">e</span></span></span></span>, but also predict the action and reward for the next time step. This may be required for more difficult tasks. For instance, if our agent needs to learn complex motor skills to walk around its environment, the world model will learn to imitate its own C model that has already learned to walk. After difficult motor skills, such as walking, is absorbed into a large world model with lots of capacity, the smaller C model can rely on the motor skills already absorbed by the world model and focus on learning more higher level skills to navigate itself using the motor skills it had already learned.<dt-fn><sup><span id="fn-9" data-hover-ref="dt-fn-hover-box-8" style="cursor:pointer">9</span></sup></dt-fn></p>
<div style="text-align: center;">
<img src="./World Models_files/memory_consolidation.svg" style="display: block; margin: auto; width: 86%;">
<br>
<figcaption style="text-align: center;">How information becomes memory.<dt-cite key="memory_consolidation"><span id="citation-63" data-hover-ref="dt-cite-hover-box-63"><span class="citation-number">[68]</span></span></dt-cite></figcaption>
</div>
<p>An interesting connection to the neuroscience literature is the work on hippocampal replay that examines how the brain replays recent experiences when an animal rests or sleeps. Replaying recent experiences plays an important role in memory consolidation<dt-cite key="Foster2017"><span id="citation-64" data-hover-ref="dt-cite-hover-box-64"><span class="citation-number">[69]</span></span></dt-cite> — where hippocampus-dependent memories become independent of the hippocampus over a period of time<dt-cite key="memory_consolidation"><span id="citation-65" data-hover-ref="dt-cite-hover-box-65"><span class="citation-number">[68]</span></span></dt-cite>. As Foster<dt-cite key="Foster2017"><span id="citation-66" data-hover-ref="dt-cite-hover-box-66"><span class="citation-number">[69]</span></span></dt-cite> puts it, replay is “less like dreaming and more like thought”. We invite readers to read <em>Replay Comes of Age</em><dt-cite key="Foster2017"><span id="citation-67" data-hover-ref="dt-cite-hover-box-67"><span class="citation-number">[69]</span></span></dt-cite> for a detailed overview of replay from a neuroscience perspective with connections to theoretical reinforcement learning.</p>
<p>Iterative training could allow the C—M model to develop a natural hierarchical way to learn. Recent works about self-play in RL<dt-cite key="asymmetric_self_play,competitive_self_play,continuous_adaptation_via_meta_learning"><span id="citation-68" data-hover-ref="dt-cite-hover-box-68"><span class="citation-number">[70, 71, 72]</span></span></dt-cite> and PowerPlay<dt-cite key="s10_powerplay,s11_powerplay"><span id="citation-69" data-hover-ref="dt-cite-hover-box-69"><span class="citation-number">[73, 74]</span></span></dt-cite> also explores methods that lead to a natural curriculum learning<dt-cite key="s09_optimal_order"><span id="citation-70" data-hover-ref="dt-cite-hover-box-70"><span class="citation-number">[75]</span></span></dt-cite>, and we feel this is one of the more exciting research areas of reinforcement learning.</p>
<hr>
<h2>Related Work</h2>
<p>There is extensive literature on learning a dynamics model, and using this model to train a policy. Many concepts first explored in the 1980s for feed-forward neural networks (FNNs)<dt-cite key="Werbos87specifications,Munro87,RobinsonFallside89,Werbos89identification,NguyenWidrow89"><span id="citation-71" data-hover-ref="dt-cite-hover-box-71"><span class="citation-number">[20, 76, 77, 78, 79]</span></span></dt-cite> and in the 1990s for RNNs<dt-cite key="s05_making_the_world_differentiable,s05a_cm,s05b_rl,s05c_boredom"><span id="citation-72" data-hover-ref="dt-cite-hover-box-72"><span class="citation-number">[22, 23, 24, 30]</span></span></dt-cite> laid some of the groundwork for <em>Learning to Think</em><dt-cite key="learning_to_think"><span id="citation-73" data-hover-ref="dt-cite-hover-box-73"><span class="citation-number">[31]</span></span></dt-cite>. The more recent PILCO<dt-cite key="pilco,pilco_tutorial,McAllister2017"><span id="citation-74" data-hover-ref="dt-cite-hover-box-74"><span class="citation-number">[58, 80, 81]</span></span></dt-cite> is a probabilistic model-based search policy method designed to solve difficult control problems. Using data collected from the environment, PILCO uses a Gaussian process (GP) model to learn the system dynamics, and then uses this model to sample many trajectories in order to train a controller to perform a desired task, such as swinging up a pendulum, or riding a unicycle.</p>
<p>While Gaussian processes work well with a small set of low dimensional data, their computational complexity makes them difficult to scale up to model a large history of high dimensional observations. Other recent works<dt-cite key="deep_pilco,Depeweg2017"><span id="citation-75" data-hover-ref="dt-cite-hover-box-75"><span class="citation-number">[82, 83]</span></span></dt-cite> use Bayesian neural networks instead of GPs to learn a dynamics model. These methods have demonstrated promising results on challenging control tasks<dt-cite key="Hein2017"><span id="citation-76" data-hover-ref="dt-cite-hover-box-76"><span class="citation-number">[84]</span></span></dt-cite>, where the states are known and well defined, and the observation is relatively low dimensional. Here we are interested in modelling dynamics observed from high dimensional visual data where our input is a sequence of raw pixel frames.</p>
<p>In robotic control applications, the ability to learn the dynamics of a system from observing only camera-based video inputs is a challenging but important problem. Early work on RL for active vision trained an FNN to take the current image frame of a video sequence to predict the next frame<dt-cite key="s04_trajectories"><span id="citation-77" data-hover-ref="dt-cite-hover-box-77"><span class="citation-number">[85]</span></span></dt-cite>, and use this predictive model to train a fovea-shifting control network trying to find targets in a visual scene. To get around the difficulty of training a dynamical model to learn directly from high-dimensional pixel images, researchers explored using neural networks to first learn a compressed representation of the video frames.  Recent work along these lines<dt-cite key="learning_deep_dynamical_models_from_image_pixels,from_pixels_to_torques"><span id="citation-78" data-hover-ref="dt-cite-hover-box-78"><span class="citation-number">[86, 87]</span></span></dt-cite> was able to train controllers using the bottleneck hidden layer of an autoencoder as low-dimensional feature vectors to control a pendulum from pixel inputs. Learning a model of the dynamics from a compressed latent space enable RL algorithms to be much more data-efficient<dt-cite key="deep_spacial_autoencoders,embed_to_control,finn_lecture"><span id="citation-79" data-hover-ref="dt-cite-hover-box-79"><span class="citation-number">[88, 89, 90]</span></span></dt-cite>. We invite readers to watch Finn’s lecture on Model-Based RL<dt-cite key="finn_lecture"><span id="citation-80" data-hover-ref="dt-cite-hover-box-80"><span class="citation-number">[90]</span></span></dt-cite> to learn more.</p>
<p>Video game environments are also popular in model-based RL research as a testbed for new ideas. Guzdial et al.<dt-cite key="game_engine_learning"><span id="citation-81" data-hover-ref="dt-cite-hover-box-81"><span class="citation-number">[91]</span></span></dt-cite> used a feed-forward convolutional neural network (CNN) to learn a forward simulation model of a video game. Learning to predict how different actions affect future states in the environment is useful for game-play agents, since if our agent can predict what happens in the future given its current state and action, it can simply select the best action that suits its goal. This has been demonstrated not only in early work<dt-cite key="NguyenWidrow89,s04_trajectories"><span id="citation-82" data-hover-ref="dt-cite-hover-box-82"><span class="citation-number">[79, 85]</span></span></dt-cite> (when compute was a million times more expensive than today) but also in recent studies<dt-cite key="learn_to_act_by_predicting_future"><span id="citation-83" data-hover-ref="dt-cite-hover-box-83"><span class="citation-number">[92]</span></span></dt-cite> on several competitive VizDoom<dt-cite key="vizdoom"><span id="citation-84" data-hover-ref="dt-cite-hover-box-84"><span class="citation-number">[34]</span></span></dt-cite> environments.</p>
<p>The works mentioned above use FNNs to predict the next video frame. We may want to use models that can capture longer term time dependencies. RNNs are powerful models suitable for sequence modelling<dt-cite key="graves_rnn"><span id="citation-85" data-hover-ref="dt-cite-hover-box-85"><span class="citation-number">[40]</span></span></dt-cite>. In a lecture called <em>Hallucination with RNNs</em><dt-cite key="graves_lecture"><span id="citation-86" data-hover-ref="dt-cite-hover-box-86"><span class="citation-number">[93]</span></span></dt-cite>, Graves demonstrated the ability of RNNs to learn a probabilistic model of Atari game environments. He trained RNNs to learn the structure of such a game and then showed that they can hallucinate similar game levels on its own.</p>
<div style="text-align: center;">
<img src="./World Models_files/world_models_1990.jpeg" style="display: block; margin: auto; width: 80%;">
<figcaption style="text-align: center;">A controller with internal RNN model of the world.<dt-cite key="s05_making_the_world_differentiable"><span id="citation-87" data-hover-ref="dt-cite-hover-box-87"><span class="citation-number">[22]</span></span></dt-cite></figcaption>
</div>
<p>Using RNNs to develop internal models to reason about the future has been explored as early as 1990 in a paper called <em>Making the World Differentiable</em><dt-cite key="s05_making_the_world_differentiable"><span id="citation-88" data-hover-ref="dt-cite-hover-box-88"><span class="citation-number">[22]</span></span></dt-cite>, and then further explored in <dt-cite key="s05a_cm,s05b_rl,s05c_boredom"><span id="citation-89" data-hover-ref="dt-cite-hover-box-89"><span class="citation-number">[23, 24, 30]</span></span></dt-cite>. A more recent paper called <em>Learning to Think</em><dt-cite key="learning_to_think"><span id="citation-90" data-hover-ref="dt-cite-hover-box-90"><span class="citation-number">[31]</span></span></dt-cite> presented a unifying framework for building a RNN-based general problem solver that can learn a world model of its environment and also learn to reason about the future using this model. Subsequent works have used RNN-based models to generate many frames into the future<dt-cite key="recurrent_env_sim,action_conditional_video_prediction,Denton2017"><span id="citation-91" data-hover-ref="dt-cite-hover-box-91"><span class="citation-number">[57, 56, 94]</span></span></dt-cite>, and also as an internal model to reason about the future<dt-cite key="Silver2016,imagination_agent,Watters2017"><span id="citation-92" data-hover-ref="dt-cite-hover-box-92"><span class="citation-number">[95, 96, 97]</span></span></dt-cite>.</p>
<p>In this work, we used evolution strategies (ES) to train our controller, as it offers many benefits. For instance, we only need to provide the optimizer with the final cumulative reward, rather than the entire history. ES is also easy to parallelize — we can launch many instances of <code>rollout</code> with different solutions to many workers and quickly compute a set of cumulative rewards in parallel. Recent works <dt-cite key="pathnet,openai,stablees,stanley2017"><span id="citation-93" data-hover-ref="dt-cite-hover-box-93"><span class="citation-number">[98, 99, 100, 101]</span></span></dt-cite> have confirmed that ES is a viable alternative to traditional Deep RL methods on many strong baseline tasks.</p>
<p>Before the popularity of Deep RL methods<dt-cite key="dqn"><span id="citation-94" data-hover-ref="dt-cite-hover-box-94"><span class="citation-number">[102]</span></span></dt-cite>, evolution-based algorithms have been shown to be effective at finding solutions for RL tasks<dt-cite key="neat,gom5_ne_accelerated,gom2_coevolve,hyperneat,pepg,evolving_neural_networks"><span id="citation-95" data-hover-ref="dt-cite-hover-box-95"><span class="citation-number">[103, 104, 105, 106, 107, 108]</span></span></dt-cite>. Evolution-based algorithms have even been able to solve difficult RL tasks from high dimensional pixel inputs<dt-cite key="kou1_torcs,hausknecht,parker2012"><span id="citation-96" data-hover-ref="dt-cite-hover-box-96"><span class="citation-number">[109, 110, 111]</span></span></dt-cite>. More recent works<dt-cite key="vae_evolution"><span id="citation-97" data-hover-ref="dt-cite-hover-box-97"><span class="citation-number">[112]</span></span></dt-cite> also combine VAE and ES, which is similar to our approach.</p>
<hr>
<h2>Discussion</h2>
<p>We have demonstrated the possibility of training an agent to perform tasks entirely inside of its simulated latent space dream world. This approach offers many practical benefits. For instance, running computationally intensive game engines require using heavy compute resources for rendering the game states into image frames, or calculating physics not immediately relevant to the game. We may not want to waste cycles training an agent in the actual environment, but instead train the agent as many times as we want inside its simulated environment. Training agents in the real world is even more expensive, so world models that are trained incrementally to simulate reality will make it easier to experiment with different approaches for training our agents.</p>
<p>Furthermore, we can take advantage of deep learning frameworks to accelerate our world model simulations using GPUs in a distributed environment. The benefit of implementing the world model as a fully differentiable recurrent computation graph also means that we may be able to train our agents in the dream directly using the backpropagation algorithm to fine-tune its policy to maximize an objective function <dt-cite key="s05_making_the_world_differentiable,s05a_cm,s05b_rl"><span id="citation-98" data-hover-ref="dt-cite-hover-box-98"><span class="citation-number">[22, 23, 24]</span></span></dt-cite>.</p>
<p>The choice of using a VAE for the V model and training it as a standalone model also has its limitations, since it may encode parts of the observations that are not relevant to a task. After all, unsupervised learning cannot, by definition, know what will be useful for the task at hand. For instance, it reproduced unimportant detailed brick tile patterns on the side walls in the Doom environment, but failed to reproduce task-relevant tiles on the road in the Car Racing environment. By training together with an M model that predicts rewards, the VAE may learn to focus on task-relevant areas of the image, but the tradeoff here is that we may not be able to reuse the VAE effectively for new tasks without retraining.</p>
<p>Learning task-relevant features has connections to neuroscience as well. Primary sensory neurons are released from inhibition when rewards are received, which suggests that they generally learn task-relevant features, rather than just any features, at least in adulthood<dt-cite key="Pi2013"><span id="citation-99" data-hover-ref="dt-cite-hover-box-99"><span class="citation-number">[113]</span></span></dt-cite>.</p>
<p>Future work might explore the use of an unsupervised segmentation layer like in <dt-cite key="Byravan2017"><span id="citation-100" data-hover-ref="dt-cite-hover-box-100"><span class="citation-number">[114]</span></span></dt-cite> to extract better feature representations that might be more useful and interpretable compared to the representations learned using a VAE.</p>
<p>Another concern is the limited capacity of our world model. While modern storage devices can store large amounts of historical data generated using the iterative training procedure, our LSTM<dt-cite key="lstm,s12_lstm_forget"><span id="citation-101" data-hover-ref="dt-cite-hover-box-101"><span class="citation-number">[115, 116]</span></span></dt-cite>-based world model may not be able to store all of the recorded information inside its weight connections. While the human brain can hold decades and even centuries of memories to some resolution<dt-cite key="brain_capacity"><span id="citation-102" data-hover-ref="dt-cite-hover-box-102"><span class="citation-number">[117]</span></span></dt-cite>, our neural networks trained with backpropagation have more limited capacity and suffer from issues such as catastrophic forgetting<dt-cite key="Ratcliff1990,French1994,Kirkpatrick2016"><span id="citation-103" data-hover-ref="dt-cite-hover-box-103"><span class="citation-number">[118, 119, 120]</span></span></dt-cite>. Future work may explore replacing the small MDN-RNN network with higher capacity models <dt-cite key="outrageously_large_neural_nets,hypernetworks,suarez2017,wavenet,attention"><span id="citation-104" data-hover-ref="dt-cite-hover-box-104"><span class="citation-number">[121, 122, 123, 124, 125]</span></span></dt-cite>, or incorporating an external memory module<dt-cite key="Gemici2017"><span id="citation-105" data-hover-ref="dt-cite-hover-box-105"><span class="citation-number">[126]</span></span></dt-cite>, if we want our agent to learn to explore more complicated worlds.</p>
<div style="text-align: center;">
<img src="./World Models_files/world_models_1990_feedback.jpeg" style="display: block; margin: auto; width: 65%;">
<figcaption style="text-align: center;">Ancient drawing (1990) of a RNN-based controller interacting with an environment.<dt-cite key="s05_making_the_world_differentiable"><span id="citation-106" data-hover-ref="dt-cite-hover-box-106"><span class="citation-number">[22]</span></span></dt-cite></figcaption>
</div>
<p>Like early RNN-based C—M systems <dt-cite key="s05_making_the_world_differentiable,s05a_cm,s05b_rl,s05c_boredom"><span id="citation-107" data-hover-ref="dt-cite-hover-box-107"><span class="citation-number">[22, 23, 24, 30]</span></span></dt-cite>, ours simulates possible futures time step by time step, without profiting from human-like hierarchical planning or abstract reasoning, which often ignores irrelevant spatial-temporal details. However, the more general <em>Learning To Think</em><dt-cite key="learning_to_think"><span id="citation-108" data-hover-ref="dt-cite-hover-box-108"><span class="citation-number">[31]</span></span></dt-cite> approach is not limited to this rather naive approach. Instead it allows a recurrent C to learn to address “subroutines” of the recurrent M, and reuse them for problem solving in arbitrary computable ways, e.g., through hierarchical planning or other kinds of exploiting parts of M’s program-like weight matrix. A recent <em>One Big Net</em><dt-cite key="onebignet2018"><span id="citation-109" data-hover-ref="dt-cite-hover-box-109"><span class="citation-number">[127]</span></span></dt-cite> extension of the C—M approach
collapses C and M into a single network, and uses PowerPlay-like<dt-cite key="s10_powerplay,s11_powerplay"><span id="citation-110" data-hover-ref="dt-cite-hover-box-110"><span class="citation-number">[73, 74]</span></span></dt-cite> behavioural replay (where the behaviour of a teacher net is compressed into a student net<dt-cite key="chunker91and92"><span id="citation-111" data-hover-ref="dt-cite-hover-box-111"><span class="citation-number">[128]</span></span></dt-cite>) to avoid forgetting old prediction and control skills when learning new ones. Experiments with those more general approaches are left for future work.</p>
<p><em>This work is meant to be a live research project and will be revised and expanded over time. This article will be the first of a series of articles exploring World Models. If you would like to discuss any issues, give feedback, or even contribute to future work, please visit the <a href="https://github.com/worldmodels">GitHub</a> repository of this page for more information.</em></p>
</dt-article>
<dt-appendix>
<style>
  dt-appendix {
    display: block;
    font-size: 14px;
    line-height: 24px;
    margin-bottom: 0;
    border-top: 1px solid rgba(0,0,0,0.1);
    color: rgba(0,0,0,0.5);
    background: rgb(250, 250, 250);
    padding-top: 36px;
    padding-bottom: 60px;
  }
  dt-appendix h3 {
    font-size: 16px;
    font-weight: 500;
    margin-top: 18px;
    margin-bottom: 18px;
    color: rgba(0,0,0,0.65);
  }
  dt-appendix .citation {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
  }
  dt-appendix .references {
    font-size: 12px;
    line-height: 20px;
  }
  dt-appendix a {
    color: rgba(0, 0, 0, 0.6);
  }
  dt-appendix ol,
  dt-appendix ul {
    padding-left: 24px;
  }
</style>

<div class="l-body">
<h2>Acknowledgments</h2>
<p>We would like to thank <a href="http://linclab.org/blake-richards/">Blake Richards</a>, <a href="http://korymathewson.com/">Kory Mathewson</a>, <a href="http://www.kylemcdonald.net/">Kyle McDonald</a>, <a href="http://kaixhin.com/">Kai Arulkumaran</a>, <a href="https://ankurhanda.github.io/">Ankur Handa</a>, <a href="http://www.wildml.com/">Denny Britz</a>, <a href="http://elwinha.com/">Elwin Ha</a> and <a href="https://www.media.mit.edu/people/jaquesn/overview/">Natasha Jaques</a> for their thoughtful feedback on this article, and for offering their valuable perspectives and insights from their areas of expertise.</p>
<p>The interative demos in this article were all built using <a href="https://p5js.org/">p5.js</a>. Deploying all of these machine learning models in a web browser was made possible with <a href="https://deeplearnjs.org/">deeplearn.js</a>, a hardware-accelerated machine learning framework for the browser, developed by the <a href="https://ai.google/pair">People+AI Research Initiative</a> (PAIR) team at Google. A special thanks goes to Nikhil Thorat and Daniel Smilkov for their support.</p>
<p>We would like to thank Chris Olah and the rest of the Distill editorial team for their valuable feedback and generous editorial support, in addition to supporting the use of their <a href="https://distill.pub/">distill.pub</a> technology.</p>
<p>We would to extend our thanks to Alex Graves, Douglas Eck, Mike Schuster, Rajat Monga, Vincent Vanhoucke, Jeff Dean and the Google Brain team for helpful feedback and for encouraging us to explore this area of research.</p>
<p>Any errors here are our own and do not reflect opinions of our proofreaders and colleagues. If you see mistakes or want to suggest changes, feel free to contribute feedback by participating in the discussion <a href="https://github.com/worldmodels/worldmodels.github.io/issues">forum</a> for this article.</p>
<p>The experiments in this article were performed on both a P100 GPU and a 64-core CPU Ubuntu Linux virtual machine provided by <a href="https://cloud.google.com/">Google Cloud Platform</a>, using <a href="https://www.tensorflow.org/">TensorFlow</a> and <a href="https://github.com/openai/gym">OpenAI Gym</a>.</p>
<h3 id="citation">Citation</h3>
<p>For attribution in academic contexts, please cite this work as</p>
<pre class="citation short">Ha and Schmidhuber, "World Models", 2018. https://doi.org/10.5281/zenodo.1207631</pre>
<p>BibTeX citation</p>
<pre class="citation long">@article{Ha2018WorldModels,
  author = {Ha, D. and Schmidhuber, J.},
  title  = {World Models},
  eprint = {arXiv:1803.10122},
  doi    = {10.5281/zenodo.1207631},
  url    = {https://worldmodels.github.io},
  year   = {2018}
}</pre>
<h3>Open Source Code</h3>
<p>The code to reproduce experiments in this work, as well as IPython notebooks for training and visualizing VAE and MDN-RNN models will be made available at a later date.</p>
<h3>Reuse</h3>
<p>Diagrams and text are licensed under Creative Commons Attribution <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a> with the <a href="https://github.com/worldmodels/worldmodels.github.io">source available on GitHub</a>, unless noted otherwise. The figures that have been reused from other sources don’t fall under this license and can be recognized by the citations in their caption.</p>
<h2 id="appendix">Appendix</h2>
<p>In this section we will describe in more details the models and training methods used in this work.</p>
<h3>Variational Autoencoder</h3>
<p>We trained a Convolutional Variational Autoencoder (ConvVAE) model as the V Model of our agent. Unlike vanilla autoencoders, enforcing a Gaussian prior over the latent vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span> also limits the amount its information capacity for compressing each frame, but this Gaussian prior also makes the world model more robust to unrealistic <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span> vectors generated by the M Model. As the environment may give us observations as high dimensional pixel images, we first resize each image to 64x64 pixels before as use this resized image as the V Model’s observation. Each pixel is stored as three floating point values between 0 and 1 to represent each of the RGB channels. The ConvVAE takes in this 64x64x3 input tensor and passes this data through 4 convolutional layers to <em>encode</em> it into low dimension vectors <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">μ</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">σ</span></span></span></span>, each of size <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>N</mi><mi>z</mi></msub></mrow><annotation encoding="application/x-tex">N_z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.10903em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>. The latent vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span> is sampled from the Gaussian prior <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi><mo>(</mo><mi>μ</mi><mo separator="true">,</mo><mi>σ</mi><mi>I</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">N(\mu, \sigma I)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord mathit">μ</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="mclose">)</span></span></span></span>. In the <dt-cite key="carracing_v0"><span id="citation-112" data-hover-ref="dt-cite-hover-box-112">Car Racing task <span class="citation-number">[48]</span></span></dt-cite>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>N</mi><mi>z</mi></msub></mrow><annotation encoding="application/x-tex">N_z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.10903em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> is 32 while for the Doom task <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>N</mi><mi>z</mi></msub></mrow><annotation encoding="application/x-tex">N_z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.10903em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> is 64. The latent vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span> is passed through 4 of <em>deconvolution</em> layers used to <em>decode</em> and reconstruct the image.</p>
<p>In the following diagram, we describe the shape of our tensor at each layer of the ConvVAE and also describe the details of each layer:</p>
<div style="text-align: center;">
<img src="./World Models_files/conv_vae_label.svg" style="display: block; margin: auto; width: 50%;">
<figcaption>Convolutional Variational Autoencoder</figcaption>
</div>
<p>Each convolution and deconvolution layer uses a stride of 2. The layers are indicated in the diagram in <em>Italics</em> as <em>Activation-type Output Channels x Filter Size</em>. All convolutional and deconvolutional layers use relu activations except for the output layer as we need the output to be between 0 and 1. We trained the model for 1 epoch over the data collected from a random policy, using <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>L</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">L^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">L</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> distance between the input image and the reconstruction to quantify the reconstruction loss we optimize for, in addition to the KL loss.</p>
<h3>Recurrent Neural Network</h3>
<p>For the M Model, we use an <dt-cite key="lstm"><span id="citation-113" data-hover-ref="dt-cite-hover-box-113">LSTM <span class="citation-number">[115]</span></span></dt-cite> recurrent neural network combined with a Mixture Density Network<dt-cite key="bishop"><span id="citation-114" data-hover-ref="dt-cite-hover-box-114"><span class="citation-number">[38]</span></span></dt-cite><dt-cite key="mdntf"><span id="citation-115" data-hover-ref="dt-cite-hover-box-115"><span class="citation-number">[39]</span></span></dt-cite> as the output layer. We use this network to model the probability distribution of the next <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span> in the next time step as a Mixture of Gaussian distribution. This approach is very similar to Graves’ <dt-cite key="graves_rnn"><span id="citation-116" data-hover-ref="dt-cite-hover-box-116">Generating Sequences with RNNs <span class="citation-number">[40]</span></span></dt-cite> in the Unconditional Handwriting Generation section and also the decoder-only section of <dt-cite key="sketchrnn"><span id="citation-117" data-hover-ref="dt-cite-hover-box-117">Sketch-RNN <span class="citation-number">[36]</span></span></dt-cite>. The only difference in the approach used is that we did not model the correlation parameter between each element of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span>, and instead had the MDN-RNN output a diagonal covariance matrix of a factored Gaussian distribution.</p>
<div style="text-align: center;">
<img src="./World Models_files/mdn_rnn.svg" style="display: block; margin: auto; width: 100%;">
<figcaption>MDN-RNN<dt-cite key="sketchrnn"><span id="citation-118" data-hover-ref="dt-cite-hover-box-118"><span class="citation-number">[36]</span></span></dt-cite></figcaption>
</div>
<p>Unlike the handwriting and sketch generation works, rather than using the MDN-RNN to model the pdf of the next pen stroke, we model instead the pdf of the next latent vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span>. We would sample from this pdf at each timestep to generate the hallucinated environments. In the Doom task, we also also use the MDN-RNN to predict the probability of whether the agent has died in this frame. If that probability is above 50%, then we set <code>done</code> to be <code>True</code> in the virtual dream environment. Given that death is a low probability event at each timestep, we find the cutoff approach to more stable compared to sampling from the Bernoulli distribution.</p>
<p>The MDN-RNNs were trained for 20 epochs on the data collected from a random policy agent. In the Car Racing task, the LSTM used 256 hidden units, while the Doom task used 512 hidden units. In both tasks, we used 5 Gaussian mixtures and did not model the correlation <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ρ</mi></mrow><annotation encoding="application/x-tex">\rho</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">ρ</span></span></span></span> parameter, hence <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span> is sampled from a factored mixture of Gaussian distribution.</p>
<p>When training the MDN-RNN using teacher forcing from the recorded data, we store a pre-computed set of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">μ</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">σ</span></span></span></span> for each of the frames, and sample an input <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi><mo>∼</mo><mi>N</mi><mo>(</mo><mi>μ</mi><mo separator="true">,</mo><mi>σ</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">z \sim N(\mu, \sigma)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mrel">∼</span><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord mathit">μ</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="mclose">)</span></span></span></span> each time we construct a training batch, to prevent overfitting our MDN-RNN to a specific sampled <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span>.</p>
<h3>Controller</h3>
<p>For both environments, we applied <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>tanh</mi></mrow><annotation encoding="application/x-tex">\tanh</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mop">tanh</span></span></span></span> nonlinearities to clip and bound the action space to the appropriate ranges. For instance, in the Car Racing task, the steering wheel has a range from -1 to 1, the acceleration pedal from 0 to 1, and the brakes from 0 to 1. In the Doom environment, we converted the discrete actions into a continuous action space between -1 to 1, and divided this range into thirds to indicate whether the agent is moving left, staying where it is, or moving to the right. We would give the C Model a feature vector as its input, consisting of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span> and the hidden state of the MDN-RNN. In the Car Racing task, this hidden state is the output vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">h</span></span></span></span> of the LSTM, while for the Doom task it is both the cell vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">c</span></span></span></span> and the output vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">h</span></span></span></span> of the LSTM.</p>
<h3>Evolution Strategies</h3>
<p>We used <dt-cite key="cmaes"><span id="citation-119" data-hover-ref="dt-cite-hover-box-119">Covariance-Matrix Adaptation Evolution Strategy (CMA-ES) <span class="citation-number">[46]</span></span></dt-cite>, an <dt-cite key="visuales"><span id="citation-120" data-hover-ref="dt-cite-hover-box-120">Evolution Strategy <span class="citation-number">[45]</span></span></dt-cite> to evolve the weights for our C Model. Following the approach described in <dt-cite key="stablees"><span id="citation-121" data-hover-ref="dt-cite-hover-box-121">Evolving Stable Strategies <span class="citation-number">[100]</span></span></dt-cite>, we used a population size of 64, and had each agent perform the task 16 times with different initial random seeds. The fitness value for the agent is the <em>average cumulative reward</em> of the 16 random rollouts. The diagram below charts the best performer, worst performer, and mean fitness of the population of 64 agents at each generation:</p>
<div style="text-align: center;">
<img src="./World Models_files/carracing.svg" style="display: block; margin: auto; width: 100%;">
<figcaption>Training of <dt-cite key="carracing_v0"><span id="citation-122" data-hover-ref="dt-cite-hover-box-122">CarRacing-v0 <span class="citation-number">[48]</span></span></dt-cite></figcaption>
</div>
<p>Since the requirement of this environment is to have an agent achieve an average score above 900 over 100 random rollouts, we took the best performing agent at the end of every 25 generations, and tested that agent over 1024 random rollout scenarios to record this average on the red line. After 1800 generations, an agent was able to achieve an average score of 900.46 over 1024 random rollouts. We used 1024 random rollouts rather than 100 because each process of the 64 core machine had been configured to run 16 times already, effectively using a full generation of compute after every 25 generations to evaluate the best agent 1024 times. Below, we plot the results of same agent evaluated over 100 rollouts:</p>
<div style="text-align: center;">
<img src="./World Models_files/carracing_histogram.svg" style="display: block; margin: auto; width: 100%;">
<figcaption>Histogram of cumulative rewards. Average score is 906 ± 21.</figcaption>
</div>
<p>We also experimented with an agent that has access to only the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span> vector from the VAE, and not letting it see the RNN’s hidden states. We tried 2 variations, where in the first variation, the C Model mapped <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span> directly to the action space <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">a</span></span></span></span>. In second variation, we attempted to add a hidden layer with 40 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">tanh</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span><span class="mord mathit">a</span><span class="mord mathit">n</span><span class="mord mathit">h</span></span></span></span> activations between <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">a</span></span></span></span>, increasing the number of model parameters of the C Model to 1443, making it more comparable with the original setup.</p>
<div style="text-align: center;">
<img src="./World Models_files/carracing_histogram_z.svg" style="display: block; margin: auto; width: 100%;">
<figcaption>When agent sees only <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>, average score is 632 ± 251.</figcaption>
</div>
<div style="text-align: center;">
<img src="./World Models_files/carracing_histogram_z_hidden.svg" style="display: block; margin: auto; width: 100%;">
<figcaption>When agent sees only <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>, with a hidden layer, average score is 788 ± 141.</figcaption>
</div>
<h3>DoomRNN</h3>
<p>We conducted a similar experiment on the hallucinated Doom environment we called <em>DoomRNN</em>. Please note that we have not actually attempted to train our agent on the actual <dt-cite key="vizdoom"><span id="citation-123" data-hover-ref="dt-cite-hover-box-123">VizDoom <span class="citation-number">[34]</span></span></dt-cite> environment, and had only used VizDoom for the purpose of collecting training data using a random policy. <em>DoomRNN</em> is more computationally efficient compared to VizDoom as it only operates in latent space without the need to render a screenshot at each timestep, and does not require running the actual Doom game engine.</p>
<div style="text-align: center;">
<img src="./World Models_files/doomrnn.svg" style="display: block; margin: auto; width: 100%;">
<figcaption>Training of DoomRNN</figcaption>
</div>
<p>In the virtual DoomRNN environment we constructed, we increased the temperature slightly and used <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>τ</mi><mo>=</mo><mn>1</mn><mi mathvariant="normal">.</mi><mn>1</mn><mn>5</mn></mrow><annotation encoding="application/x-tex">\tau=1.15</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="mrel">=</span><span class="mord mathrm">1</span><span class="mord mathrm">.</span><span class="mord mathrm">1</span><span class="mord mathrm">5</span></span></span></span> to make the agent learn in a more challenging environment. The best agent managed to obtain an average score of 959 over 1024 random rollouts (the highest score of the red line in the diagram). This same agent achieved an average score of 1092 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 556 over 100 random rollouts when deployed to the actual environment <dt-cite key="takecover"><span id="citation-124" data-hover-ref="dt-cite-hover-box-124">DoomTakeCover-v0 <span class="citation-number">[35]</span></span></dt-cite>.</p>
<div style="text-align: center;">
<img src="./World Models_files/doomtakecover_histogram.svg" style="display: block; margin: auto; width: 100%;">
<figcaption>Histogram of timesteps survived in the actual environment over 100 consecutive trials.</figcaption>
</div><h3>Footnotes</h3><dt-fn-list><ol><li>In many RL problems, the feedback (positive or negative reward) is given at end of a sequence of steps. The credit assignment problem tackles the problem of figuring out which steps caused the resulting feedback—which steps should receive credit or blame for a final result?</li><li>Typical model-free RL models have in the order of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><msup><mn>0</mn><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">10^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span><span class="mord"><span class="mord mathrm">0</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">3</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><msup><mn>0</mn><mn>6</mn></msup></mrow><annotation encoding="application/x-tex">10^6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span><span class="mord"><span class="mord mathrm">0</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">6</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> model parameters. We look at training models in the order of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><msup><mn>0</mn><mn>7</mn></msup></mrow><annotation encoding="application/x-tex">10^7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span><span class="mord"><span class="mord mathrm">0</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">7</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> parameters, which is still rather small compared to state-of-the-art deep learning models with <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><msup><mn>0</mn><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">10^8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span><span class="mord"><span class="mord mathrm">0</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">8</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> to even <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><msup><mn>0</mn><mrow><mn>9</mn></mrow></msup></mrow><annotation encoding="application/x-tex">10^{9}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span><span class="mord"><span class="mord mathrm">0</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">9</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> parameters. In principle, the procedure described in this article can take advantage of these larger networks if we wanted to use them.</li><li>To be clear, the prediction of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">z_{t+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> is not fed into the controller C directly — just the hidden state <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>. This is because <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> has all the information needed to generate the parameters of a mixture of Gaussian distribution, if we want to sample <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">z_{t+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> to make a prediction.</li><li>We find this task interesting because although it is not difficult to train an agent to wobble around randomly generated tracks and obtain a mediocre score, CarRacing-v0 defines “solving” as getting average reward of 900 over 100 consecutive trials, which means the agent can only afford very few driving mistakes.</li><li>We will discuss an iterative training procedure later on for more complicated environments where a random policy is not sufficient.</li><li>In principle, we can train both models together in an end-to-end manner, although we found that training each separately is more practical, and also achieves satisfactory results. Training each model only required less than an hour of computation time using a single NVIDIA P100 GPU. We can also train individual VAE and MDN-RNN models without having to exhaustively tune hyperparameters.</li><li>We will discuss how this score compares to other models later on.</li><li>In <em>Learning to Think</em>, it is acceptable that the RNN M isn’t always a reliable predictor. A (potentially evolution-based) RNN C can in principle learn to ignore a flawed M, or exploit certain useful parts of M for arbitrary computational purposes including hierarchical planning etc. This is not what we do here though — our present approach is still closer to some of the old systems, where a RNN M is used to predict and plan ahead step by step. Unlike this early work, however, we use evolution for C (like in <em>Learning to Think</em>) rather than traditional RL combined with RNNs, which has the advantage of both simplicity and generality.</li><li>Another related connection is to muscle memory. For instance, as you learn to do something like play the piano, you no longer have to spend working memory capacity on translating individual notes to finger motions — this all becomes encoded at a subconscious level.</li></ol></dt-fn-list><h3>References</h3><dt-bibliography><ol><li><b>OpenAI Gym</b>   <a href="http://arxiv.org/pdf/1606.01540.pdf">[PDF]</a><br>Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J. and Zaremba, W., 2016. ArXiv preprint. </li><li><b>Understanding Comics: The Invisible Art</b>   <a href="https://en.wikipedia.org/wiki/Understanding_Comics">[link]</a><br>McCloud, S., 1993. Tundra Publishing.</li><li><b>More thoughts from Understanding Comics by Scott McCloud</b>   <a href="http://memeengine.tumblr.com/post/28333277260/more-thoughts-from-understanding-comics-by-scott">[link]</a><br>E, M., 2012. Tumblr.</li><li><b>Counterintuitive behavior of social systems</b>   <a href="https://en.wikipedia.org/wiki/Mental_model">[link]</a><br>Forrester, J.W., 1971. Technology Review.</li><li><b>The Code for Facial Identity in the Primate Brain</b>   <a href="http://www.cell.com/cell/fulltext/S0092-8674%2817%2930538-X">[link]</a><br>Cheang, L. and Tsao, D., 2017. Cell.  <a href="https://doi.org/10.1016/j.cell.2017.05.011" style="text-decoration:inherit;">DOI: 10.1016/j.cell.2017.05.011</a></li><li><b>Invariant visual representation by single neurons in the human brain</b>   <a href="http://www.nature.com/nature/journal/v435/n7045/abs/nature03687.html">[HTML]</a><br>Quiroga, R., Reddy, L., Kreiman, G., Koch, C. and Fried, I., 2005. Nature.  <a href="https://doi.org/10.1038/nature03687" style="text-decoration:inherit;">DOI: 10.1038/nature03687</a></li><li><b>Primary Visual Cortex Represents the Difference Between Past and Present</b>   <a href="http://dx.doi.org/10.1093/cercor/bht318">[link]</a><br>Nortmann, N., Rekauzke, S., Onat, S., König, P. and Jancke, D., 2015. Cerebral Cortex, Vol 25(6), pp. 1427-1440.  <a href="https://doi.org/10.1093/cercor/bht318" style="text-decoration:inherit;">DOI: 10.1093/cercor/bht318</a></li><li><b>Motion-Dependent Representation of Space in Area MT+</b>   <a href="http://dx.doi.org/10.1016/j.neuron.2013.03.010">[link]</a><br>Gerrit, M., Fischer, J. and Whitney, D., 2013. Neuron.  <a href="https://doi.org/10.1016/j.neuron.2013.03.010" style="text-decoration:inherit;">DOI: 10.1016/j.neuron.2013.03.010</a></li><li><b>Akiyoshi’s Illusion Pages</b>   <a href="http://www.ritsumei.ac.jp/~akitaoka/index-e.html">[HTML]</a><br>Kitaoka, A., 2002. Kanzen.</li><li><b>Peripheral drift illusion</b>   <a href="https://en.wikipedia.org/wiki/Peripheral_drift_illusion">[link]</a><br>Authors, W., 2017. Wikipedia.</li><li><b>Illusory Motion Reproduced by Deep Neural Networks Trained for Prediction</b>   <a href="https://www.frontiersin.org/article/10.3389/fpsyg.2018.00345">[link]</a><br>Watanabe, E., Kitaoka, A., Sakamoto, K., Yasugi, M. and Tanaka, K., 2018. Frontiers in Psychology, Vol 9, pp. 345.  <a href="https://doi.org/10.3389/fpsyg.2018.00345" style="text-decoration:inherit;">DOI: 10.3389/fpsyg.2018.00345</a></li><li><b>Sensorimotor Mismatch Signals in Primary Visual Cortex of the Behaving Mouse</b>   <a href="http://www.sciencedirect.com/science/article/pii/S0896627312003844">[link]</a><br>Keller, G., Bonhoeffer, T. and Hübener, M., 2012. Neuron, Vol 74(5), pp. 809 - 815.  <a href="https://doi.org/https://doi.org/10.1016/j.neuron.2012.03.040" style="text-decoration:inherit;">DOI: https://doi.org/10.1016/j.neuron.2012.03.040</a></li><li><b>A Sensorimotor Circuit in Mouse Cortex for Visual Flow Predictions</b>   <a href="http://www.sciencedirect.com/science/article/pii/S0896627317307791">[link]</a><br>Leinweber, M., Ward, D.R., Sobczak, J.M., Attinger, A. and Keller, G.B., 2017. Neuron, Vol 95(6), pp. 1420 - 1432.e5.  <a href="https://doi.org/https://doi.org/10.1016/j.neuron.2017.08.036" style="text-decoration:inherit;">DOI: https://doi.org/10.1016/j.neuron.2017.08.036</a></li><li><b>The ecology of human fear: survival optimization and the nervous system.</b>   <a href="https://www.frontiersin.org/article/10.3389/fnins.2015.00055">[link]</a><br>Mobbs, D., Hagan, C.C., Dalgleish, T., Silston, B. and Prévost, C., 2015. Frontiers in Neuroscience.  <a href="https://doi.org/10.3389/fnins.2015.00055" style="text-decoration:inherit;">DOI: 10.3389/fnins.2015.00055</a></li><li><b>Baseball Icon Design (CC 3.0)</b>   <a href="https://thenounproject.com/gilad.sotil4231c9c47bce4f03/collection/ball-games/">[link]</a><br>Sotil, G., 2018. The Noun Project.</li><li><b>Tracking Fastballs</b>   <a href="http://sciencenetlinks.com/science-news/science-updates/tracking-fastballs/">[link]</a><br>Hirshon, B., 2013. Science Update Interview.</li><li><b>Reinforcement learning: a survey</b> <br>Kaelbling, L.P., Littman, M.L. and Moore, A.W., 1996. Journal of AI research, Vol 4, pp. 237—285. </li><li><b>Introduction to Reinforcement Learning</b>   <a href="http://ufal.mff.cuni.cz/~straka/courses/npfl114/2016/sutton-bookdraft2016sep.pdf">[PDF]</a><br>Sutton, R.S. and Barto, A.G., 1998. MIT Press.</li><li><b>Reinforcement Learning</b> <br>Wiering, M. and van Otterlo, M., 2012. Springer.</li><li><b>Learning How the World Works: Specifications for Predictive Networks in Robots and Brains</b> <br>Werbos, P.J., 1987. Proceedings of IEEE International Conference on Systems, Man and Cybernetics, N.Y.. </li><li><b>David Silver’s Lecture on Integrating Learning and Planning</b>   <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/dyna.pdf">[PDF]</a><br>Silver, D., 2017. </li><li><b>Making the World Differentiable: On Using Self-Supervised Fully Recurrent Neural Networks for Dynamic Reinforcement Learning and Planning in Non-Stationary Environments</b>   <a href="http://people.idsia.ch/~juergen/FKI-126-90_(revised)bw_ocr.pdf">[PDF]</a><br>Schmidhuber, J., 1990. </li><li><b>An on-line algorithm for dynamic reinforcement learning and planning in reactive environments</b>   <a href="ftp://ftp.idsia.ch/pub/juergen/ijcnn90.ps.gz">[link]</a><br>Schmidhuber, J., 1990. 1990 IJCNN International Joint Conference on Neural Networks, pp. 253-258 vol.2.  <a href="https://doi.org/10.1109/IJCNN.1990.137723" style="text-decoration:inherit;">DOI: 10.1109/IJCNN.1990.137723</a></li><li><b>Reinforcement Learning in Markovian and Non-Markovian Environments</b>   <a href="http://papers.nips.cc/paper/393-reinforcement-learning-in-markovian-and-non-markovian-environments.pdf">[PDF]</a><br>Schmidhuber, J., 1991. Advances in Neural Information Processing Systems 3, pp. 500—506. Morgan-Kaufmann.</li><li><b>The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors</b> <br>Linnainmaa, S., 1970. </li><li><b>Gradient Theory of Optimal Flight Paths</b> <br>Kelley, H.J., 1960. ARS Journal, Vol 30(10), pp. 947-954. </li><li><b>Applications of advances in nonlinear sensitivity analysis</b> <br>Werbos, P.J., 1982. System modeling and optimization, pp. 762—770. Springer.</li><li><b>Deep Reinforcement Learning: A Brief Survey</b>   <a href="http://arxiv.org/pdf/1708.05866.pdf">[PDF]</a><br>Arulkumaran, K., Deisenroth, M.P., Brundage, M. and Bharath, A.A., 2017. IEEE Signal Processing Magazine, Vol 34(6), pp. 26-38.  <a href="https://doi.org/10.1109/MSP.2017.2743240" style="text-decoration:inherit;">DOI: 10.1109/MSP.2017.2743240</a></li><li><b>Deep Learning in Neural Networks: An Overview</b> <br>Schmidhuber, J., 2015. Neural Networks, Vol 61, pp. 85-117.  <a href="https://doi.org/10.1016/j.neunet.2014.09.003" style="text-decoration:inherit;">DOI: 10.1016/j.neunet.2014.09.003</a></li><li><b>A Possibility for Implementing Curiosity and Boredom in Model-building Neural Controllers</b>   <a href="ftp://ftp.idsia.ch/pub/juergen/curiositysab.pdf">[PDF]</a><br>Schmidhuber, J., 1990. Proceedings of the First International Conference on Simulation of Adaptive Behavior on From Animals to Animats, pp. 222—227. MIT Press.</li><li><b>On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models</b>   <a href="http://arxiv.org/pdf/1511.09249.pdf">[PDF]</a><br>Schmidhuber, J., 2015. ArXiv preprint. </li><li><b>Auto-Encoding Variational Bayes</b>   <a href="http://arxiv.org/pdf/1312.6114.pdf">[PDF]</a><br>Kingma, D. and Welling, M., 2013. ArXiv preprint. </li><li><b>Stochastic Backpropagation and Approximate Inference in Deep Generative Models</b>   <a href="http://arxiv.org/pdf/1401.4082.pdf">[PDF]</a><br>Jimenez Rezende, D., Mohamed, S. and Wierstra, D., 2014. ArXiv preprint. </li><li><b>ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning</b>   <a href="http://arxiv.org/pdf/1605.02097.pdf">[PDF]</a><br>Kempka, M., Wydmuch, M., Runc, G., Toczek, J. and Jaskowski, W., 2016. IEEE Conference on Computational Intelligence and Games, pp. 341—348. IEEE.</li><li><b>DoomTakeCover-v0</b>   <a href="https://gym.openai.com/envs/DoomTakeCover-v0/">[link]</a><br>Paquette, P., 2016. </li><li><b>A Neural Representation of Sketch Drawings</b>   <a href="https://magenta.tensorflow.org/sketch-rnn-demo">[link]</a><br>Ha, D. and Eck, D., 2017. ArXiv preprint. </li><li><b>Draw Together with a Neural Network</b>   <a href="https://magenta.tensorflow.org/sketch-rnn-demo">[link]</a><br>Ha, D., Jongejan, J. and Johnson, I., 2017. Google AI Experiments. </li><li><b>Mixture density networks</b>   <a href="http://publications.aston.ac.uk/373/">[link]</a><br>Bishop, C.M., 1994. Technical Report. Aston University.</li><li><b>Mixture Density Networks with TensorFlow</b>   <a href="http://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/">[link]</a><br>Ha, D., 2015. blog.otoro.net. </li><li><b>Generating sequences with recurrent neural networks</b>   <a href="http://arxiv.org/pdf/1308.0850.pdf">[PDF]</a><br>Graves, A., 2013. ArXiv preprint. </li><li><b>Recurrent Neural Network Tutorial for Artists</b>   <a href="http://blog.otoro.net/2017/01/01/recurrent-neural-network-artist/">[link]</a><br>Ha, D., 2017. blog.otoro.net. </li><li><b>Experiments in Handwriting with a Neural Network</b>   <a href="http://distill.pub/2016/handwriting">[link]</a><br>Carter, S., Ha, D., Johnson, I. and Olah, C., 2016. Distill.  <a href="https://doi.org/10.23915/distill.00004" style="text-decoration:inherit;">DOI: 10.23915/distill.00004</a></li><li><b>Evolutionsstrategie: optimierung technischer systeme nach prinzipien der biologischen evolution</b>   <a href="https://en.wikipedia.org/wiki/Ingo_Rechenberg">[link]</a><br>Rechenberg, I., 1973. Frommann-Holzboog.</li><li><b>Numerical Optimization of Computer Models</b>   <a href="https://en.wikipedia.org/wiki/Hans-Paul_Schwefel">[link]</a><br>Schwefel, H., 1977. John Wiley and Sons, Inc.</li><li><b>A Visual Guide to Evolution Strategies</b>   <a href="http://blog.otoro.net/2017/10/29/visual-evolution-strategies/">[link]</a><br>Ha, D., 2017. blog.otoro.net. </li><li><b>The CMA Evolution Strategy: A Tutorial</b>   <a href="http://arxiv.org/pdf/1604.00772.pdf">[PDF]</a><br>Hansen, N., 2016. ArXiv preprint. </li><li><b>Completely Derandomized Self-Adaptation in Evolution Strategies</b>   <a href="http://www.cmap.polytechnique.fr/~nikolaus.hansen/cmaartic.pdf">[PDF]</a><br>Hansen, N. and Ostermeier, A., 2001. Evolutionary Computation, Vol 9(2), pp. 159—195. MIT Press. <a href="https://doi.org/10.1162/106365601750190398" style="text-decoration:inherit;">DOI: 10.1162/106365601750190398</a></li><li><b>CarRacing-v0</b>   <a href="https://gym.openai.com/envs/CarRacing-v0/">[link]</a><br>Klimov, O., 2016. </li><li><b>Self-driving cars in the browser</b>   <a href="http://janhuenermann.com/projects/learning-to-drive">[link]</a><br>Hünermann, J., 2017. </li><li><b>Mar I/O Kart</b>   <a href="https://youtu.be/S9Y_I9vY8Qw">[link]</a><br>Bling, S., 2015. </li><li><b>Using Keras and Deep Deterministic Policy Gradient to play TORCS</b>   <a href="https://yanpanlau.github.io/2016/10/11/Torcs-Keras.html">[HTML]</a><br>Lau, B., 2016. </li><li><b>Car Racing using Reinforcement Learning</b>   <a href="https://web.stanford.edu/class/cs221/2017/restricted/p-final/elibol/final.pdf">[PDF]</a><br>Khan, M. and Elibol, O., 2016. </li><li><b>Reinforcement Car Racing with A3C</b>   <a href="https://www.scribd.com/document/358019044/Reinforcement-Car-Racing-with-A3C">[link]</a><br>Jang, S., Min, J. and Lee, C., 2017. </li><li><b>Deep-Q Learning for Box2D Racecar RL problem.</b>   <a href="https://gist.github.com/lmclupr/b35c89b2f8f81b443166e88b787b03ab#file-race-car-cv2-nn-network-td0-15-possible-actions-ipynb">[link]</a><br>Prieur, L., 2017. “GitHub”.</li><li><b>Video Game Exploits</b>   <a href="https://en.wikipedia.org/wiki/Video_game_exploits">[link]</a><br>Wikipedia, A., 2017. Wikipedia.</li><li><b>Action-Conditional Video Prediction using Deep Networks in Atari Games</b>   <a href="http://arxiv.org/pdf/1507.08750.pdf">[PDF]</a><br>Oh, J., Guo, X., Lee, H., Lewis, R. and Singh, S., 2015. ArXiv preprint. </li><li><b>Recurrent Environment Simulators</b>   <a href="http://arxiv.org/pdf/1704.02254.pdf">[PDF]</a><br>Chiappa, S., Racaniere, S., Wierstra, D. and Mohamed, S., 2017. ArXiv preprint. </li><li><b>PILCO: A Model-Based and Data-Efficient Approach to Policy Search</b>   <a href="http://mlg.eng.cam.ac.uk/pub/pdf/DeiRas11.pdf">[PDF]</a><br>Deisenroth, M. and Rasmussen, C., 2011. In Proceedings of the International Conference on Machine Learning. </li><li><b>Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning</b>   <a href="http://arxiv.org/pdf/1708.02596.pdf">[PDF]</a><br>Nagabandi, A., Kahn, G., Fearing, R. and Levine, S., 2017. ArXiv preprint. </li><li><b>Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990-2010).</b>   <a href="http://people.idsia.ch/~juergen/creativity.html">[HTML]</a><br>Schmidhuber, J., 2010. IEEE Trans. Autonomous Mental Development. </li><li><b>Developmental Robotics, Optimal Artificial Curiosity, Creativity, Music, and the Fine Arts</b> <br>Schmidhuber, J., 2006. Connection Science, Vol 18(2), pp. 173—187. </li><li><b>Curious Model-Building Control Systems</b> <br>Schmidhuber, J., 1991. In Proc. International Joint Conference on Neural Networks, Singapore, pp. 1458—1463. IEEE.</li><li><b>Curiosity-driven Exploration by Self-supervised Prediction</b>   <a href="https://pathak22.github.io/noreward-rl/">[link]</a><br>Pathak, D., Agrawal, P., A., E. and Darrell, T., 2017. ArXiv preprint. </li><li><b>Intrinsic Motivation Systems for Autonomous Mental Development</b>   <a href="http://www.pyoudeyer.com/ims.pdf">[PDF]</a><br>Oudeyer, P., Kaplan, F. and Hafner, V., 2007. Trans. Evol. Comp. IEEE Press. <a href="https://doi.org/10.1109/TEVC.2006.890271" style="text-decoration:inherit;">DOI: 10.1109/TEVC.2006.890271</a></li><li><b>Reinforcement driven information acquisition in nondeterministic environments</b> <br>Schmidhuber, J., Storck, J. and Hochreiter, S., 1994. </li><li><b>Information-seeking, curiosity, and attention: computational and neural mechanisms</b>   <a href="http://www.pyoudeyer.com/TICSCuriosity2013.pdf">[PDF]</a><br>Gottlieb, J., Oudeyer, P., Lopes, M. and Baranes, A., 2013. Cell.  <a href="https://doi.org/10.1016/j.tics.2013.09.001" style="text-decoration:inherit;">DOI: 10.1016/j.tics.2013.09.001</a></li><li><b>Abandoning objectives: Evolution through the search for novelty alone</b>   <a href="http://eplex.cs.ucf.edu/noveltysearch/userspage/">[link]</a><br>Lehman, J. and Stanley, K., 2011. Evolutionary Computation, Vol 19(2), pp. 189—223. M I T Press.</li><li><b>Memory Consolidation</b>   <a href="https://en.wikipedia.org/wiki/Memory_consolidation">[link]</a><br>Authors, W., 2017. Wikipedia.</li><li><b>Replay Comes of Age</b>   <a href="https://doi.org/10.1146/annurev-neuro-072116-031538">[link]</a><br>Foster, D.J., 2017. Annual Review of Neuroscience, Vol 40(1), pp. 581-602.  <a href="https://doi.org/10.1146/annurev-neuro-072116-031538" style="text-decoration:inherit;">DOI: 10.1146/annurev-neuro-072116-031538</a></li><li><b>Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play</b>   <a href="http://arxiv.org/pdf/1703.05407.pdf">[PDF]</a><br>Sukhbaatar, S., Lin, Z., Kostrikov, I., Synnaeve, G., Szlam, A. and Fergus, R., 2017. ArXiv preprint. </li><li><b>Emergent Complexity via Multi-Agent Competition</b>   <a href="http://arxiv.org/pdf/1710.03748.pdf">[PDF]</a><br>Bansal, T., Pachocki, J., Sidor, S., Sutskever, I. and Mordatch, I., 2017. ArXiv preprint. </li><li><b>Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments</b>   <a href="http://arxiv.org/pdf/1710.03641.pdf">[PDF]</a><br>Al-Shedivat, M., Bansal, T., Burda, Y., Sutskever, I., Mordatch, I. and Abbeel, P., 2017. ArXiv preprint. </li><li><b>PowerPlay: Training an Increasingly General Problem Solver by Continually Searching for the Simplest Still Unsolvable Problem</b>   <a href="https://www.frontiersin.org/article/10.3389/fpsyg.2013.00313">[link]</a><br>Schmidhuber, J., 2013. Frontiers in Psychology, Vol 4, pp. 313.  <a href="https://doi.org/10.3389/fpsyg.2013.00313" style="text-decoration:inherit;">DOI: 10.3389/fpsyg.2013.00313</a></li><li><b>First Experiments with PowerPlay</b>   <a href="http://arxiv.org/pdf/1210.8385.pdf">[PDF]</a><br>Srivastava, R., Steunebrink, B. and Schmidhuber, J., 2012. ArXiv preprint. </li><li><b>Optimal Ordered Problem Solver</b>   <a href="http://arxiv.org/pdf/.pdf">[PDF]</a><br>Schmidhuber, J., 2002. ArXiv preprint. </li><li><b>A Dual Back-Propagation Scheme for Scalar Reinforcement Learning</b> <br>Munro, P.W., 1987. Proceedings of the Ninth Annual Conference of the Cognitive Science Society, Seattle, WA, pp. 165-176. </li><li><b>Dynamic Reinforcement Driven Error Propagation Networks with Application to Game Playing</b> <br>Robinson, T. and Fallside, F., 1989. CogSci 89. </li><li><b>Neural Networks for Control and System Identification</b> <br>Werbos, P.J., 1989. Proceedings of IEEE/CDC Tampa, Florida. </li><li><b>The truck backer-upper: An example of self learning in neural networks</b> <br>Nguyen, N. and Widrow, B., 1989. Proceedings of the International Joint Conference on Neural Networks, pp. 357-363. IEEE Press.</li><li><b>Lecture Slides on PILCO</b>   <a href="https://www.cs.toronto.edu/~duvenaud/courses/csc2541/slides/pilco.pdf">[PDF]</a><br>Duvenaud, D., 2016. CSC 2541 Course at University of Toronto. </li><li><b>Data-Efficient Reinforcement Learning in Continuous-State POMDPs</b>   <a href="http://arxiv.org/pdf/1602.02523.pdf">[PDF]</a><br>McAllister, R. and Rasmussen, C., 2016. ArXiv preprint. </li><li><b>Improving PILCO with Bayesian Neural Network Dynamics Models</b>   <a href="http://mlg.eng.cam.ac.uk/yarin/PDFs/DeepPILCO.pdf">[PDF]</a><br>Gal, Y., McAllister, R. and Rasmussen, C., 2016. ICML Workshop on Data-Efficient Machine Learning. </li><li><b>Learning and Policy Search in Stochastic Dynamical Systems with Bayesian Neural Networks</b>   <a href="http://arxiv.org/pdf/1605.07127.pdf">[PDF]</a><br>Depeweg, S., Hernandez-Lobato, J., Doshi-Velez, F. and Udluft, S., 2016. ArXiv preprint. </li><li><b>A Benchmark Environment Motivated by Industrial Control Problems</b>   <a href="http://arxiv.org/pdf/1709.09480.pdf">[PDF]</a><br>Hein, D., Depeweg, S., Tokic, M., Udluft, S., Hentschel, A., Runkler, T. and Sterzing, V., 2017. ArXiv preprint. </li><li><b>Learning to Generate Artificial Fovea Trajectories for Target Detection</b>   <a href="ftp://ftp.idsia.ch/pub/juergen/attention.pdf">[PDF]</a><br>Schmidhuber, J. and Huber, R., 1991. International Journal of Neural Systems, Vol 2(1-2), pp. 125—134.  <a href="https://doi.org/10.1142/S012906579100011X" style="text-decoration:inherit;">DOI: 10.1142/S012906579100011X</a></li><li><b>Learning deep dynamical models from image pixels</b>   <a href="http://arxiv.org/pdf/1410.7550.pdf">[PDF]</a><br>Wahlström, N., Schön, T. and Deisenroth, M., 2014. ArXiv preprint. </li><li><b>From Pixels to Torques: Policy Learning with Deep Dynamical Models</b>   <a href="http://arxiv.org/pdf/1502.02251.pdf">[PDF]</a><br>Wahlström, N., Schön, T. and Deisenroth, M., 2015. ArXiv preprint. </li><li><b>Deep Spatial Autoencoders for Visuomotor Learning</b>   <a href="http://arxiv.org/pdf/1509.06113.pdf">[PDF]</a><br>Finn, C., Tan, X., Duan, Y., Darrell, T., Levine, S. and Abbeel, P., 2015. ArXiv preprint. </li><li><b>Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images</b>   <a href="http://arxiv.org/pdf/1506.07365.pdf">[PDF]</a><br>Watter, M., Springenberg, J., Boedecker, J. and Riedmiller, M., 2015. ArXiv preprint. </li><li><b>Model-Based RL Lecture at Deep RL Bootcamp 2017</b>   <a href="https://youtu.be/iC2a7M9voYU?t=44m35s">[link]</a><br>Finn, C., 2017. </li><li><b>Game Engine Learning from Video</b>   <a href="https://doi.org/10.24963/ijcai.2017/518">[link]</a><br>Matthew Guzdial, B.L., 2017. Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17, pp. 3707—3713.  <a href="https://doi.org/10.24963/ijcai.2017/518" style="text-decoration:inherit;">DOI: 10.24963/ijcai.2017/518</a></li><li><b>Learning to Act by Predicting the Future</b>   <a href="http://arxiv.org/pdf/1611.01779.pdf">[PDF]</a><br>Dosovitskiy, A. and Koltun, V., 2016. ArXiv preprint. </li><li><b>Hallucination with Recurrent Neural Networks</b>   <a href="https://www.youtube.com/watch?v=-yX1SYeDHbg&amp;t=49m33s">[link]</a><br>Graves, A., 2015. </li><li><b>Unsupervised Learning of Disentangled Representations from Video</b>   <a href="http://arxiv.org/pdf/1705.10915.pdf">[PDF]</a><br>Denton, E. and Birodkar, V., 2017. ArXiv preprint. </li><li><b>The Predictron: End-To-End Learning and Planning</b>   <a href="http://arxiv.org/pdf/1612.08810.pdf">[PDF]</a><br>Silver, D., van Hasselt, H., Hessel, M., Schaul, T., Guez, A., Harley, T., Dulac-Arnold, G., Reichert, D., Rabinowitz, N., Barreto, A. and Degris, T., 2016. ArXiv preprint. </li><li><b>Imagination-Augmented Agents for Deep Reinforcement Learning</b>   <a href="http://arxiv.org/pdf/1707.06203.pdf">[PDF]</a><br>Weber, T., Racanière, S., Reichert, D., Buesing, L., Guez, A., Rezende, D., Badia, A., Vinyals, O., Heess, N., Li, Y., Pascanu, R., Battaglia, P., Silver, D. and Wierstra, D., 2017. ArXiv preprint. </li><li><b>Visual Interaction Networks</b>   <a href="http://arxiv.org/pdf/1706.01433.pdf">[PDF]</a><br>Watters, N., Tacchetti, A., Weber, T., Pascanu, R., Battaglia, P. and Zoran, D., 2017. ArXiv preprint. </li><li><b>PathNet: Evolution Channels Gradient Descent in Super Neural Networks</b>   <a href="http://arxiv.org/pdf/1701.08734.pdf">[PDF]</a><br>Fernando, C., Banarse, D., Blundell, C., Zwols, Y., Ha, D., Rusu, A., Pritzel, A. and Wierstra, D., 2017. ArXiv preprint. </li><li><b>Evolution Strategies as a Scalable Alternative to Reinforcement Learning</b>   <a href="http://arxiv.org/pdf/1703.03864.pdf">[PDF]</a><br>Salimans, T., Ho, J., Chen, X., Sidor, S. and Sutskever, I., 2017. ArXiv preprint. </li><li><b>Evolving Stable Strategies</b>   <a href="http://blog.otoro.net/2017/11/12/evolving-stable-strategies/">[link]</a><br>Ha, D., 2017. blog.otoro.net. </li><li><b>Welcoming the Era of Deep Neuroevolution</b>   <a href="https://eng.uber.com/deep-neuroevolution/">[link]</a><br>Stanley, K. and Clune, J., 2017. Uber AI Research.</li><li><b>Playing Atari with Deep Reinforcement Learning</b>   <a href="http://arxiv.org/pdf/1312.5602.pdf">[PDF]</a><br>Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D. and Riedmiller, M., 2013. ArXiv preprint. </li><li><b>Evolving Neural Networks Through Augmenting Topologies</b>   <a href="http://nn.cs.utexas.edu/?stanley:ec02">[link]</a><br>Stanley, K.O. and Miikkulainen, R., 2002. Evolutionary Computation, Vol 10(2), pp. 99-127. </li><li><b>Accelerated Neural Evolution Through Cooperatively Coevolved Synapses</b>   <a href="http://people.idsia.ch/~juergen/gomez08a.pdf">[PDF]</a><br>Gomez, F., Schmidhuber, J. and Miikkulainen, R., 2008. Journal of Machine Learning Research, Vol 9, pp. 937—965. JMLR.org.</li><li><b>Co-evolving Recurrent Neurons Learn Deep Memory POMDPs</b>   <a href="ftp://ftp.idsia.ch/pub/juergen/gecco05gomez.pdf">[PDF]</a><br>Gomez, F. and Schmidhuber, J., 2005. Proceedings of the 7th Annual Conference on Genetic and Evolutionary Computation, pp. 491—498. ACM. <a href="https://doi.org/10.1145/1068009.1068092" style="text-decoration:inherit;">DOI: 10.1145/1068009.1068092</a></li><li><b>Autonomous Evolution of Topographic Regularities in Artificial Neural Networks</b>   <a href="http://eplex.cs.ucf.edu/papers/gauci_nc10.pdf">[PDF]</a><br>Gauci, J. and Stanley, K.O., 2010. Neural Computation, Vol 22(7), pp. 1860—1898. MIT Press. <a href="https://doi.org/10.1162/neco.2010.06-09-1042" style="text-decoration:inherit;">DOI: 10.1162/neco.2010.06-09-1042</a></li><li><b>Parameter-exploring policy gradients</b>   <a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A64D1AE8313A364B814998E9E245B40A?doi=10.1.1.180.7104&amp;rep=rep1&amp;type=pdf">[link]</a><br>Sehnke, F., Osendorfer, C., Ruckstieb, T., Graves, A., Peters, J. and Schmidhuber, J., 2010. Neural Networks, Vol 23(4), pp. 551—559.  <a href="https://doi.org/10.1016/j.neunet.2009.12.004" style="text-decoration:inherit;">DOI: 10.1016/j.neunet.2009.12.004</a></li><li><b>Evolving Neural Networks</b>   <a href="http://nn.cs.utexas.edu/downloads/slides/miikkulainen.ijcnn13.pdf">[PDF]</a><br>Miikkulainen, R., 2013. IJCNN. </li><li><b>Evolving Large-scale Neural Networks for Vision-based Reinforcement Learning</b>   <a href="http://people.idsia.ch/~juergen/compressednetworksearch.html">[HTML]</a><br>Koutnik, J., Cuccu, G., Schmidhuber, J. and Gomez, F., 2013. Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation, pp. 1061—1068. ACM. <a href="https://doi.org/10.1145/2463372.2463509" style="text-decoration:inherit;">DOI: 10.1145/2463372.2463509</a></li><li><b>A Neuroevolution Approach to General Atari Game Playing</b>   <a href="http://www.cs.utexas.edu/~ai-lab/?atari">[link]</a><br>Hausknecht, M., Lehman, J., Miikkulainen, R. and Stone, P., 2013. IEEE Transactions on Computational Intelligence and AI in Games. </li><li><b>Neuro-Visual Control in the Quake II Environment</b>   <a href="https://www.cse.unr.edu/~bdbryant/papers/parker-2012-tciaig.pdf">[PDF]</a><br>Parker, M. and Bryant, B., 2012. IEEE Transactions on Computational Intelligence and AI in Games. </li><li><b>Autoencoder-augmented Neuroevolution for Visual Doom Playing</b>   <a href="http://arxiv.org/pdf/1707.03902.pdf">[PDF]</a><br>Alvernaz, S. and Togelius, J., 2017. ArXiv preprint. </li><li><b>Cortical interneurons that specialize in disinhibitory control</b>   <a href="http://dx.doi.org/10.1038/nature12676">[link]</a><br>Pi, H., Hangya, B., Kvitsiani, D., Sanders, J., Huang, Z. and Kepecs, A., 2013. Nature.  <a href="https://doi.org/10.1038/nature12676" style="text-decoration:inherit;">DOI: 10.1038/nature12676</a></li><li><b>SE3-Pose-Nets: Structured Deep Dynamics Models for Visuomotor Planning and Control</b>   <a href="http://arxiv.org/pdf/1710.00489.pdf">[PDF]</a><br>Byravan, A., Leeb, F., Meier, F. and Fox, D., 2017. ArXiv preprint. </li><li><b>Long short-term memory</b>   <a href="ftp://ftp.idsia.ch/pub/juergen/lstm.pdf">[PDF]</a><br>Hochreiter, S. and Schmidhuber, J., 1997. Neural Computation. MIT Press.</li><li><b>Learning to Forget: Continual Prediction with LSTM</b>   <a href="ftp://ftp.idsia.ch/pub/juergen/FgGates-NC.pdf">[PDF]</a><br>Gers, F., Schmidhuber, J. and Cummins, F., 2000. Neural Computation, Vol 12(10), pp. 2451—2471. MIT Press. <a href="https://doi.org/10.1162/089976600300015015" style="text-decoration:inherit;">DOI: 10.1162/089976600300015015</a></li><li><b>Nanoconnectomic upper bound on the variability of synaptic plasticity</b>   <a href="https://doi.org/10.7554/eLife.10778">[link]</a><br>Bartol, T.M., Bromer, C., Kinney, J., Chirillo, M.A., Bourne, J.N., Harris, K.M. and Sejnowski, T.J., 2015. eLife Sciences Publications, Ltd.  <a href="https://doi.org/10.7554/eLife.10778" style="text-decoration:inherit;">DOI: 10.7554/eLife.10778</a></li><li><b>Connectionist models of recognition memory: constraints imposed by learning and forgetting functions.</b> <br>Ratcliff, R.M., 1990. Psychological review, Vol 97 2, pp. 285-308. </li><li><b>Catastrophic interference in connectionist networks: Can It Be predicted, can It be prevented?</b>   <a href="http://papers.nips.cc/paper/799-catastrophic-interference-in-connectionist-networks-can-it-be-predicted-can-it-be-prevented.pdf">[PDF]</a><br>French, R.M., 1994. Advances in Neural Information Processing Systems 6, pp. 1176—1177. Morgan-Kaufmann.</li><li><b>Overcoming catastrophic forgetting in neural networks</b>   <a href="http://arxiv.org/pdf/1612.00796.pdf">[PDF]</a><br>Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A.M., Quan, J., Ramalho, T., Grabska-Barwinska, A., Hassabis, D., Clopath, C., Kumaran, D. and Hadsell, R., 2016. ArXiv preprint. </li><li><b>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</b>   <a href="http://arxiv.org/pdf/1701.06538.pdf">[PDF]</a><br>Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G. and Dean, J., 2017. ArXiv preprint. </li><li><b>HyperNetworks</b>   <a href="http://arxiv.org/pdf/1609.09106.pdf">[PDF]</a><br>Ha, D., Dai, A. and Le, Q., 2016. ArXiv preprint. </li><li><b>Language Modeling with Recurrent Highway Hypernetworks</b>   <a href="http://papers.nips.cc/paper/6919-language-modeling-with-recurrent-highway-hypernetworks.pdf">[PDF]</a><br>Suarez, J., 2017. Advances in Neural Information Processing Systems 30, pp. 3269—3278. Curran Associates, Inc.</li><li><b>WaveNet: A Generative Model for Raw Audio</b>   <a href="http://arxiv.org/pdf/1609.03499.pdf">[PDF]</a><br>van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A. and Kavukcuoglu, K., 2016. ArXiv preprint. </li><li><b>Attention Is All You Need</b>   <a href="http://arxiv.org/pdf/1706.03762.pdf">[PDF]</a><br>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L. and Polosukhin, I., 2017. ArXiv preprint. </li><li><b>Generative Temporal Models with Memory</b>   <a href="http://arxiv.org/pdf/1702.04649.pdf">[PDF]</a><br>Gemici, M., Hung, C., Santoro, A., Wayne, G., Mohamed, S., Rezende, D., Amos, D. and Lillicrap, T., 2017. ArXiv preprint. </li><li><b>One Big Net For Everything</b>   <a href="http://arxiv.org/pdf/1802.08864.pdf">[PDF]</a><br>Schmidhuber, J., 2018. Preprint arXiv:1802.08864 [cs.AI]. </li><li><b>Learning Complex, Extended Sequences Using the Principle of History Compression</b> <br>Schmidhuber, J., 1992. Neural Computation, Vol 4(2), pp. 234-242. </li></ol></dt-bibliography></div>
</dt-appendix>


<script type="text/bibliography">
  @online{ forrester,
    author = "Forrester, Jay Wright",
    title = "Counterintuitive behavior of social systems",
    publisher = "Technology Review",
    year = "1971",
    url = "https://en.wikipedia.org/wiki/Mental_model",
    note = "[Online; accessed 01-Nov-2017]"
  }
  @online{ video_game_exploits,
    author = "Wikipedia, Authors",
    publisher = "Wikipedia",
    title = "Video Game Exploits",
    year = "2017",
    url = "https://en.wikipedia.org/wiki/Video_game_exploits",
    note = "[Online; accessed 01-Nov-2017]"
  }
  @article{facial_identity_primate_brain,
    author = {Cheang, L. and Tsao, D.},
    title = "The Code for Facial Identity in the Primate Brain",
    journal = {Cell},
    year = {2017},
    url = "http://www.cell.com/cell/fulltext/S0092-8674%2817%2930538-X",
    doi = "10.1016/j.cell.2017.05.011",
  }
  @article{ single_neuron_viz,
    author = "Quiroga, R. and Reddy, L. and Kreiman, G. and Koch, C. and Fried, I.",
    title = "Invariant visual representation by single neurons in the human brain",
    year = "2005",
    journal = "Nature",
    url = "http://www.nature.com/nature/journal/v435/n7045/abs/nature03687.html",
    doi = "10.1038/nature03687",
  }
  @article{primary_viz_cortex_past_present,
    author = {Nortmann, Nora and Rekauzke, Sascha and Onat, Selim and König, Peter and Jancke, Dirk},
    title = {Primary Visual Cortex Represents the Difference Between Past and Present},
    journal = {Cerebral Cortex},
    volume = {25},
    number = {6},
    pages = {1427-1440},
    year = {2015},
    doi = {10.1093/cercor/bht318},
    URL = {http://dx.doi.org/10.1093/cercor/bht318},
  }
  @article{ mt_motion,
    author = "Gerrit, M. and Fischer, J. and Whitney, D.",
    journal = "Neuron",
    title = "Motion-Dependent Representation of Space in Area MT+",
    year = "2013",
    url = "http://dx.doi.org/10.1016/j.neuron.2013.03.010",
    doi = "10.1016/j.neuron.2013.03.010"
  }
  @misc{ mt_motion_article,
    author = "Hirshon, B.",
    title = "Tracking Fastballs",
    publisher = "Science Update Interview",
    year = "2013",
    url = "http://sciencenetlinks.com/science-news/science-updates/tracking-fastballs/",
  }
  @misc{ survival_optimization,
    author = "Mobbs, Dean and Hagan, Cindy C. and Dalgleish, Tim and Silston, Brian and Prévost, Charlotte",
    title = "The ecology of human fear: survival optimization and the nervous system.",
    journal = "Frontiers in Neuroscience",
    year = "2015",
    url = "https://www.frontiersin.org/article/10.3389/fnins.2015.00055",
    doi = "10.3389/fnins.2015.00055",
  }
  @online{tanaka_fastball,
    author = {Channel ESPN},
    title = {Sport Science: Masahiro Tanaka},
    year = 2017,
    url = {https://youtu.be/ZEtZGmMxwaw},
    urldate = {2017-11-01}
  }
  @misc{ flappy_bird,
    author = "Dong Nguyen",
    publisher = "dotGEARS",
    title = "Flappy Bird",
    year = "2013",
    url = "https://en.wikipedia.org/wiki/Flappy_Bird",
    note = "[Online; accessed 01-Nov-2017]"
  }
  @misc{ flappy_bird_clone,
    author = "Sourabh Verma",
    publisher = "GitHub",
    title = "A Flappy Bird Clone using python-pygame",
    year = "2013",
    url = "https://github.com/sourabhv/FlapPyBird",
    note = "[Online; accessed 01-Nov-2017]"
  }
  @misc{ flappy_bird_ne,
    author = "Vincent Bazia",
    publisher = "GitHub",
    title = "Program learning to play Flappy Bird by machine learning",
    year = "2016",
    url = "https://github.com/xviniette/FlappyLearning",
    note = "[Online; accessed 01-Nov-2017]"
  }
  @article{learning_to_think,
    title={On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models},
    author={Schmidhuber, Jürgen},
    journal={ArXiv preprint},
    year={2015},
    url={https://arxiv.org/abs/1511.09249}
  }
  @ARTICLE{rl_survey, 
  author={K. Arulkumaran and M. P. Deisenroth and M. Brundage and A. A. Bharath}, 
  journal={IEEE Signal Processing Magazine}, 
  title={Deep Reinforcement Learning: A Brief Survey}, 
  year={2017}, 
  volume={34}, 
  number={6}, 
  pages={26-38}, 
  keywords={Artificial intelligence;Learning (artificial intelligence);Machine learning;Neural networks;Signal processing algorithms;Visualization}, 
  doi={10.1109/MSP.2017.2743240}, 
  ISSN={1053-5888}, 
  month={Nov},
  url={https://arxiv.org/abs/1708.05866}
  }
  @online{carracing_v0,
    author = {Oleg Klimov},
    title = {CarRacing-v0},
    year = 2016,
    url = {https://gym.openai.com/envs/CarRacing-v0/},
    urldate = {2017-11-01}
  }
  @article{carracing_cs234,
    title={Reinforcement Car Racing with A3C},
    author = {Jang, S. and Min, J. and Lee, C.},
    year={2017},
    url="https://www.scribd.com/document/358019044/Reinforcement-Car-Racing-with-A3C"
  }
  @article{carracing_cs221,
    title={Car Racing using Reinforcement Learning},
    author = {Khan, M. and Elibol, O.},
    year={2016},
    url="https://web.stanford.edu/class/cs221/2017/restricted/p-final/elibol/final.pdf"
  }
  @article{mdnrnn_tutorial,
    title={Recurrent Neural Network Tutorial for Artists},
    author = {Ha, D.},
    journal={blog.otoro.net},
    year={2017},
    url="http://blog.otoro.net/2017/01/01/recurrent-neural-network-artist/"
  }
  @article{openai,
    title={Evolution Strategies as a Scalable Alternative to Reinforcement Learning},
   author = {Salimans, T. and Ho, J. and Chen, X. and Sidor, S. and Sutskever, I.},
    journal={ArXiv preprint},
    year={2017},
    url={https://arxiv.org/abs/1703.03864}
  }
  @article{visuales,
    title={A Visual Guide to Evolution Strategies},
    author = {Ha, D.},
    journal={blog.otoro.net},
    year={2017},
    url="http://blog.otoro.net/2017/10/29/visual-evolution-strategies/"
  }
  @article{stablees,
    title={Evolving Stable Strategies},
    author = {Ha, D.},
    journal={blog.otoro.net},
    year={2017},
    url="http://blog.otoro.net/2017/11/12/evolving-stable-strategies/"
  }
  @article{cmaes_original,
   author = {Hansen, Nikolaus and Ostermeier, Andreas},
   title = {Completely Derandomized Self-Adaptation in Evolution Strategies},
   journal = {Evolutionary Computation},
   issue_date = {June 2001},
   volume = {9},
   number = {2},
   month = jun,
   year = {2001},
   issn = {1063-6560},
   pages = {159--195},
   numpages = {37},
   url = {http://www.cmap.polytechnique.fr/~nikolaus.hansen/cmaartic.pdf},
   doi = {10.1162/106365601750190398},
   acmid = {1108843},
   publisher = {MIT Press},
   address = {Cambridge, MA, USA},
  }
  @article{cmaes,
    title={The CMA Evolution Strategy: A Tutorial},
   author = {Hansen, N.},
    journal={ArXiv preprint},
    year={2016},
    url={https://arxiv.org/abs/1604.00772}
  }
  @article{sampling_generative_networks,
    title={Sampling Generative Networks},
   author = {White, T.},
    journal={ArXiv preprint},
    year={2016},
    url={https://arxiv.org/abs/1609.04468}
  }
  @article{vae,
    title={Auto-Encoding Variational Bayes},
   author = {Kingma, D. and Welling, M.},
    journal={ArXiv preprint},
    year={2013},
    url={https://arxiv.org/abs/1312.6114}
  }
  @article{vae_dm,
    title={Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
   author = {Jimenez Rezende, D. and Mohamed, S. and Wierstra, D.},
    journal={ArXiv preprint},
    year={2014},
    url={https://arxiv.org/abs/1401.4082}
  }
  @BOOK{understandingcomics,
    TITLE = {Understanding Comics: The Invisible Art},
    AUTHOR = {McCloud, Scott},
    YEAR = {1993},
    PUBLISHER = {Tundra Publishing},
    url = "https://en.wikipedia.org/wiki/Understanding_Comics",
  }
  @online{understandingcomics_blog,
    TITLE = "More thoughts from Understanding Comics by Scott McCloud",
    AUTHOR = {E, M.},
    YEAR = {2012},
    PUBLISHER = {Tumblr},
    url = "http://memeengine.tumblr.com/post/28333277260/more-thoughts-from-understanding-comics-by-scott",
  }
  @online{browser_car,
    author = {Jan Hünermann},
    title = {Self-driving cars in the browser},
    year = 2017,
    url = {http://janhuenermann.com/projects/learning-to-drive},
    urldate = {2017-11-01}
  }
  @online{keras_car,
    author = {Ben Lau},
    title = {Using Keras and Deep Deterministic Policy Gradient to play TORCS},
    year = 2016,
    url = {https://yanpanlau.github.io/2016/10/11/Torcs-Keras.html},
    urldate = {2017-11-01}
  }
  @online{keras_flappybird,
    author = {Ben Lau},
    title = {Using Keras and Deep Q-Network to Play FlappyBird},
    year = 2016,
    url = {https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html},
    urldate = {2017-11-01}
  }
  @inproceedings{vizdoom,
    author    = {Michael Kempka and Marek Wydmuch and Grzegorz Runc and Jakub Toczek and Wojciech Jaskowski},
    title     = {ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning},
    booktitle = {IEEE Conference on Computational Intelligence and Games},  
    year      = {2016},
    url       = {http://arxiv.org/abs/1605.02097},
    address   = {Santorini, Greece},
    Month     = {Sep},
    Pages     = {341--348},
    Publisher = {IEEE},
    Note      = {The best paper award}
  }
  @online{takecover,
    author = {Philip Paquette},
    title = {DoomTakeCover-v0},
    year = 2016,
    url = {https://gym.openai.com/envs/DoomTakeCover-v0/},
    urldate = {2017-11-01}
  }
  @online{mar_io_kart,
    author = {Seth Bling},
    title = "Mar I/O Kart",
    year = 2015,
    url = {https://youtu.be/S9Y_I9vY8Qw},
    urldate = {2017-11-01}
  }
  @ARTICLE{bishop,
           journal = {Technical Report},
             title = {Mixture density networks},
            author = {Christopher M. Bishop},
           address = {Birmingham},
         publisher = {Aston University},
               url = {http://publications.aston.ac.uk/373/},
       year = 1994,
  }
  @article{mdntf,
    title={Mixture Density Networks with TensorFlow},
    author={Ha, D.},
    journal={blog.otoro.net},
    url = "http://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/",
    year={2015}
  }
  @article{graves_rnn,
    title={Generating sequences with recurrent neural networks},
    author={Graves, Alex},
    journal={ArXiv preprint},
    url = {https://arxiv.org/abs/1308.0850},
    year={2013}
  }
  @ARTICLE{sketchrnn,
     author = {Ha, D. and Eck, D.},
      title = "A Neural Representation of Sketch Drawings",
    journal={ArXiv preprint},
       year = 2017,
      month = apr,
     url = {https://magenta.tensorflow.org/sketch-rnn-demo},
  }
  @ARTICLE{sketchrnndemo,
     author = {Ha, D. and Jongejan, J. and Johnson, I.},
      title = "Draw Together with a Neural Network",
    journal={Google AI Experiments},
       year = 2017,
      month = jun,
     url = {https://magenta.tensorflow.org/sketch-rnn-demo},
  }
  @article{carter2016experiments,
    author = {Carter, Shan and Ha, David and Johnson, Ian and Olah, Chris},
    title = {Experiments in Handwriting with a Neural Network},
    journal = {Distill},
    year = {2016},
    url = {http://distill.pub/2016/handwriting},
    doi = {10.23915/distill.00004}
  }
  @ARTICLE{openai_gym,
     author = {Brockman, G. and Cheung, V. and Pettersson, L. and Schneider, J. and Schulman, J. and Tang, J. and Zaremba, W.},
    title={OpenAI Gym},
    journal={ArXiv preprint},
       year = 2016,
      month = jun,
     url = {https://arxiv.org/abs/1606.01540},
  }
  @ARTICLE{competitive_self_play,
     author = {Bansal, T. and Pachocki, J. and Sidor, S. and Sutskever, I. and Mordatch, I.},
    journal={ArXiv preprint},
    title={Emergent Complexity via Multi-Agent Competition},
       year = 2017,
      month = oct,
     url = {https://arxiv.org/abs/1710.03748},
  }
  @ARTICLE{continuous_adaptation_via_meta_learning,
     author = {Al-Shedivat, M. and Bansal, T. and Burda, Y. and Sutskever, I. and Mordatch, I. and Abbeel, P.},
    journal={ArXiv preprint},
    title={Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments},
       year = 2017,
      month = oct,
     url = {https://arxiv.org/abs/1710.03641},
  }
  @ARTICLE{asymmetric_self_play,
    author = {Sukhbaatar, S. and Lin, Z. and Kostrikov, I. and Synnaeve, G. and Szlam, A. and Fergus, R.},
    journal={ArXiv preprint},
    title={Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play},
       year = 2017,
      month = oct,
     url = {https://arxiv.org/abs/1703.05407},
  }
  @article{brain_capacity,
    author = {Bartol, Thomas M, Jr and Bromer, Cailey and Kinney, Justin and Chirillo, Michael A and Bourne, Jennifer N and Harris, Kristen M and Sejnowski, Terrence J},
    title = {Nanoconnectomic upper bound on the variability of synaptic plasticity},
    journal = {eLife Sciences Publications, Ltd},
    year = {2015},
    doi = {10.7554/eLife.10778},
    url = {https://doi.org/10.7554/eLife.10778}
  }
  @ARTICLE{outrageously_large_neural_nets,
     author = {Shazeer, N. and Mirhoseini, A. and Maziarz, K. and Davis, A. and Le, Q. and Hinton, G. and Dean, J.},
    journal={ArXiv preprint},
    title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
       year = 2017,
      month = jan,
     url = {https://arxiv.org/abs/1701.06538},
  }
  @ARTICLE{hypernetworks,
     author = "Ha, D. and Dai, A. and Le, Q.",
    journal={ArXiv preprint},
    title="HyperNetworks",
       year = 2016,
      month = sep,
     url = {https://arxiv.org/abs/1609.09106},
  }
  @incollection{suarez2017,
  title = {Language Modeling with Recurrent Highway Hypernetworks},
  author = {Suarez, Joseph},
  booktitle = {Advances in Neural Information Processing Systems 30},
  editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages = {3269--3278},
  year = {2017},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/6919-language-modeling-with-recurrent-highway-hypernetworks.pdf}
  }
  @article{lstm,
    title={Long short-term memory},
    author={Hochreiter, Sepp and Schmidhuber, Juergen},
    journal={Neural Computation},
    year={1997},
    publisher={MIT Press},
    url={ftp://ftp.idsia.ch/pub/juergen/lstm.pdf}
  }
  @ARTICLE{Kirkpatrick2016,
     author = {Kirkpatrick, J. and Pascanu, R. and Rabinowitz, N. and Veness, J. and Desjardins, G. and Rusu, A.and Milan, K. and Quan, J. and Ramalho, T. and Grabska-Barwinska, A. and Hassabis, D. and Clopath, C. and Kumaran, D. and Hadsell, R.},
      title = "Overcoming catastrophic forgetting in neural networks",
    journal={ArXiv preprint},
       year = 2016,
      month = dec,
     url = {https://arxiv.org/abs/1612.00796}
  }
  @article{Ratcliff1990,
    title={Connectionist models of recognition memory: constraints imposed by learning and forgetting functions.},
    author={Rodney Mark Ratcliff},
    journal={Psychological review},
    year={1990},
    volume={97 2},
    pages={285-308}
  }
  @incollection{French1994,
  title = {Catastrophic interference in connectionist networks: Can It Be predicted, can It be prevented?},
  author = {Robert M. French},
  booktitle = {Advances in Neural Information Processing Systems 6},
  editor = {J. D. Cowan and G. Tesauro and J. Alspector},
  pages = {1176--1177},
  year = {1994},
  publisher = {Morgan-Kaufmann},
  url = {http://papers.nips.cc/paper/799-catastrophic-interference-in-connectionist-networks-can-it-be-predicted-can-it-be-prevented.pdf}
  }
  @online{graves_lecture,
    author = {Alex Graves},
    title = {Hallucination with Recurrent Neural Networks},
    year = 2015,
    url = {https://www.youtube.com/watch?v=-yX1SYeDHbg&t=49m33s},
    urldate = {2017-11-01}
  }
  @online{dyna_slides,
    author = {David Silver},
    title = "David Silver's Lecture on Integrating Learning and Planning",
    year = 2017,
    url = {http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/dyna.pdf},
    urldate = {2017-12-13}
  }
  @online{fastball_video,
    author = {Josh Salwen},
    title = "2015 MLB Home Run Swings",
    publisher = "YouTube, Creative Commons License.",
    year = 2016,
    url = {https://youtu.be/FlHRHrt6fkk},
    urldate = {2017-11-01}
  }
  @online{finn_lecture,
    author = {Chelsea Finn},
    title = {Model-Based RL Lecture at Deep RL Bootcamp 2017},
    year = 2017,
    url = {https://youtu.be/iC2a7M9voYU?t=44m35s},
    urldate = {2017-11-01}
  }
  @inproceedings{game_engine_learning,
    author    = {Matthew Guzdial, Boyang Li, Mark O. Riedl},
    title     = {Game Engine Learning from Video},
    booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on
                 Artificial Intelligence, IJCAI-17},
    pages     = {3707--3713},
    year      = {2017},
    doi       = {10.24963/ijcai.2017/518},
    url       = {https://doi.org/10.24963/ijcai.2017/518},
  }
  @ARTICLE{learn_to_act_by_predicting_future,
     author = {Dosovitskiy, A. and Koltun, V.},
      title = "Learning to Act by Predicting the Future",
    journal={ArXiv preprint},
       year = 2016,
      month = nov,
     url = {https://arxiv.org/abs/1611.01779},
  }
  @ARTICLE{learning_multimodal_dynamics,
     author = {Moerland, T. and Broekens, J. and Jonker, C.},
      title = "Learning Multimodal Transition Dynamics for Model-Based Reinforcement Learning",
    journal={ArXiv preprint},
       year = 2017,
      month = may,
     url = {https://arxiv.org/abs/1705.00470},
  }
  @ARTICLE{learning_deep_dynamical_models_from_image_pixels,
     author = {Wahlström, N. and Schön, T. and Deisenroth, M.},
      title = "Learning deep dynamical models from image pixels",
    journal={ArXiv preprint},
       year = 2014,
      month = oct,
     url = {https://arxiv.org/abs/1410.7550},
  }
  @ARTICLE{from_pixels_to_torques,
     author = {Wahlström, N. and Schön, T. and Deisenroth, M.},
      title = "From Pixels to Torques: Policy Learning with Deep Dynamical Models",
    journal={ArXiv preprint},
       year = 2015,
      month = jun,
     url = {https://arxiv.org/abs/1502.02251},
  }
  @ARTICLE{embed_to_control,
     author = {Watter, M. and Springenberg, J. and Boedecker, J. and Riedmiller, M.},
      title = "Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images",
    journal={ArXiv preprint},
       year = 2015,
      month = jun,
     url = "https://arxiv.org/abs/1506.07365",
  }
  @ARTICLE{deep_spacial_autoencoders,
     author = {Finn, C. and Tan, X. and Duan, Y. and Darrell, T. and Levine, S. and Abbeel, P.},
      title = "Deep Spatial Autoencoders for Visuomotor Learning",
    journal={ArXiv preprint},
       year = 2015,
      month = sep,
     url = {https://arxiv.org/abs/1509.06113},
  }
  @ARTICLE{action_conditional_video_prediction,
     author = {Oh, J. and Guo, X. and Lee, H. and Lewis, R. and Singh, S.},
      title = "Action-Conditional Video Prediction using Deep Networks in Atari Games",
    journal={ArXiv preprint},
       year = 2015,
      month = jul,
     url = {https://arxiv.org/abs/1507.08750}
  }
  @ARTICLE{imagination_agent,
     author = {Weber, T. and Racanière, S. and Reichert, D. and Buesing, L. and Guez, A. and Rezende, D. and Badia, A. and Vinyals, O. and Heess, N. and Li, Y. and Pascanu, R. and Battaglia, P. and Silver, D. and Wierstra, D.},
      title = "Imagination-Augmented Agents for Deep Reinforcement Learning",
    journal={ArXiv preprint},
       year = 2017,
      month = jul,
     url = {https://arxiv.org/abs/1707.06203}
  }
  @article{intrinsic_motivation,
   author = "Oudeyer, P. and Kaplan, F. and Hafner, V.",
   title = "Intrinsic Motivation Systems for Autonomous Mental Development",
   journal = "Trans. Evol. Comp",
   year = 2007,
   month = "apr",
   url = "http://www.pyoudeyer.com/ims.pdf",
   doi = "10.1109/TEVC.2006.890271",
   publisher = "IEEE Press",
  } 
  @article{schmidhuber_creativity,
    author = "Schmidhuber, J.",
    journal = "IEEE Trans. Autonomous Mental Development",
    title = "Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990-2010).",
    url = "http://people.idsia.ch/~juergen/creativity.html",
    year = 2010,
  }
  @ARTICLE{dqn,
     author = {Mnih, V. and Kavukcuoglu, K. and Silver, D. and Graves, A. and Antonoglou, I. and Wierstra, D. and Riedmiller, M.},
      title = "Playing Atari with Deep Reinforcement Learning",
    journal={ArXiv preprint},
       year = 2013,
      month = dec,
     url = {https://arxiv.org/abs/1312.5602},
  }
  @article{hausknecht,
  title={A Neuroevolution Approach to General Atari Game Playing},
  author={Hausknecht, M. and Lehman, J. and Miikkulainen, R. and Stone, P.},
  journal={IEEE Transactions on Computational Intelligence and AI in Games},
  url="http://www.cs.utexas.edu/~ai-lab/?atari",
  year={2013}
  }
  @ARTICLE{pathnet,
     author = {Fernando, C. and Banarse, D. and Blundell, C. and Zwols, Y. and Ha, D. and Rusu, A. and Pritzel, A. and Wierstra, D.},
      title = "PathNet: Evolution Channels Gradient Descent in Super Neural Networks",
    journal={ArXiv preprint},
       year = 2017,
      month = jan,
     url = {https://arxiv.org/abs/1701.08734},
  }
@article{hyperneat,
 author = {Gauci, Jason and Stanley, Kenneth O.},
 title = {Autonomous Evolution of Topographic Regularities in Artificial Neural Networks},
 journal = {Neural Computation},
 issue_date = {July 2010},
 volume = {22},
 number = {7},
 month = jul,
 year = {2010},
 issn = {0899-7667},
 pages = {1860--1898},
 numpages = {39},
 url = {http://eplex.cs.ucf.edu/papers/gauci_nc10.pdf},
 doi = {10.1162/neco.2010.06-09-1042},
 acmid = {1810223},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}
@Article{neat,
title={Evolving Neural Networks Through Augmenting Topologies},
author={Kenneth O. Stanley and Risto Miikkulainen},
volume={10},
journal={Evolutionary Computation},
number={2},
pages={99-127},
url="http://nn.cs.utexas.edu/?stanley:ec02",
year={2002}
}
@article{pepg,
  author    = {Sehnke, F. and Osendorfer, C. and Ruckstieb, T. and Graves, A. and Peters, J. and Schmidhuber, J.},
  title     = {Parameter-exploring policy gradients},
  journal   = {Neural Networks},
  volume    = {23},
  number    = {4},
  pages     = {551--559},
  year      = {2010},
  url       = "http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A64D1AE8313A364B814998E9E245B40A?doi=10.1.1.180.7104&rep=rep1&type=pdf",
  doi       = {10.1016/j.neunet.2009.12.004},
}
  @article{parker2012,
  title={Neuro-Visual Control in the Quake II Environment},
  author={Parker, M. and Bryant, B.},
  journal={IEEE Transactions on Computational Intelligence and AI in Games},
  url="https://www.cse.unr.edu/~bdbryant/papers/parker-2012-tciaig.pdf",
  year={2012}
  }
  @article{evolving_neural_networks,
  title={Evolving Neural Networks},
  author={Miikkulainen, R.},
  journal={IJCNN},
  year=2013,
  month=aug,
  url={http://nn.cs.utexas.edu/downloads/slides/miikkulainen.ijcnn13.pdf},
  }
@ARTICLE{vae_evolution,
   author = {Alvernaz, S. and Togelius, J.},
    title = "Autoencoder-augmented Neuroevolution for Visual Doom Playing",
  journal = {ArXiv preprint},
 primaryClass = "cs.AI",
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
     year = 2017,
    month = jul,
    url = {https://arxiv.org/abs/1707.03902},
}
@ARTICLE{recurrent_env_sim,
   author = {Chiappa, S. and Racaniere, S. and Wierstra, D. and Mohamed, S.},
    title = "Recurrent Environment Simulators",
  journal = {ArXiv preprint},
 primaryClass = "cs.AI",
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Statistics - Machine Learning},
     year = 2017,
    month = apr,
    url = {https://arxiv.org/abs/1704.02254},
}
@book{Rechenberg1973,
    author = {Rechenberg, I.},
    keywords = {neurofitter},
    posted-at = {2007-10-03 02:05:07},
    priority = {2},
    publisher = {Frommann-Holzboog},
    title = {Evolutionsstrategie: optimierung technischer systeme nach prinzipien der biologischen evolution},
    year = {1973},
    url = {https://en.wikipedia.org/wiki/Ingo_Rechenberg}
}
@book{Schwefel1977,
    author = {Schwefel, H.},
    title = {Numerical Optimization of Computer Models},
    year = {1977},
    isbn = {0471099880},
    publisher = {John Wiley and Sons, Inc.},
    address = {New York, NY, USA},
    url = {https://en.wikipedia.org/wiki/Hans-Paul_Schwefel}
}
@article{s03_overview, 
    author = "J. Schmidhuber", 
    title = "Deep Learning in Neural Networks: An Overview", 
    journal = "Neural Networks", 
    pages = "85-117", 
    volume = "61", 
    doi = "10.1016/j.neunet.2014.09.003", 
    note = "Published online 2014; based on TR arXiv:1404.7828 [cs.NE]", 
    year = "2015"
}
@article{s04_trajectories,
  author    = {Schmidhuber, J. and Huber, R},
  title     = {Learning to Generate Artificial Fovea Trajectories for Target Detection},
  journal   = {International Journal of Neural Systems},
  volume    = {2},
  number    = {1-2},
  pages     = {125--134},
  year      = {1991},
  url       = {ftp://ftp.idsia.ch/pub/juergen/attention.pdf},
  doi       = {10.1142/S012906579100011X},
}
@article{s05a_cm, 
author={J. Schmidhuber}, 
journal={1990 IJCNN International Joint Conference on Neural Networks}, 
title={An on-line algorithm for dynamic reinforcement learning and planning in reactive environments}, 
year={1990}, 
pages={253-258 vol.2}, 
doi={10.1109/IJCNN.1990.137723}, 
month={June},
url="ftp://ftp.idsia.ch/pub/juergen/ijcnn90.ps.gz",
}
@article{s05b_rl,
title = {Reinforcement Learning in Markovian and Non-Markovian Environments},
author = {Schmidhuber, J.},
journal = {Advances in Neural Information Processing Systems 3},
editor = {R. P. Lippmann and J. E. Moody and D. S. Touretzky},
pages = {500--506},
year = {1991},
publisher = {Morgan-Kaufmann},
url = {http://papers.nips.cc/paper/393-reinforcement-learning-in-markovian-and-non-markovian-environments.pdf}
}
@article{s05c_boredom,
 author = {Schmidhuber, J.},
 title = {A Possibility for Implementing Curiosity and Boredom in Model-building Neural Controllers},
 journal = {Proceedings of the First International Conference on Simulation of Adaptive Behavior on From Animals to Animats},
 year = {1990},
 isbn = {0-262-63138-5},
 location = {Paris, France},
 pages = {222--227},
 numpages = {6},
 url = {ftp://ftp.idsia.ch/pub/juergen/curiositysab.pdf},
 acmid = {116542},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}
@article{s05_making_the_world_differentiable,
    author = {J. Schmidhuber},
    title = {Making the World Differentiable: On Using Self-Supervised Fully Recurrent Neural Networks for Dynamic Reinforcement Learning and Planning in Non-Stationary Environments},
    url = {http://people.idsia.ch/~juergen/FKI-126-90_(revised)bw_ocr.pdf},
    year = {1990}
}
@article{gom5_ne_accelerated,
 author = {Gomez, F. and Schmidhuber, J. and Miikkulainen, R.},
 title = {Accelerated Neural Evolution Through Cooperatively Coevolved Synapses},
 journal = {Journal of Machine Learning Research},
 issue_date = {6/1/2008},
 volume = {9},
 month = jun,
 year = {2008},
 issn = {1532-4435},
 pages = {937--965},
 numpages = {29},
 url = {http://people.idsia.ch/~juergen/gomez08a.pdf},
 acmid = {1390712},
 publisher = {JMLR.org},
}
@article{gom2_coevolve,
 author = {Gomez, F. and Schmidhuber, J.},
 title = {Co-evolving Recurrent Neurons Learn Deep Memory POMDPs},
 journal = {Proceedings of the 7th Annual Conference on Genetic and Evolutionary Computation},
 series = {GECCO 2005},
 year = {2005},
 isbn = {1-59593-010-8},
 location = {Washington DC, USA},
 pages = {491--498},
 numpages = {8},
 url = {ftp://ftp.idsia.ch/pub/juergen/gecco05gomez.pdf},
 doi = {10.1145/1068009.1068092},
 acmid = {1068092},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {POMDP, coevolution, recurrent neural networks},
}
@article{kou1_torcs,
 author = {Koutnik, J. and Cuccu, G. and Schmidhuber, J. and Gomez, F.},
 title = {Evolving Large-scale Neural Networks for Vision-based Reinforcement Learning},
 journal = {Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation},
 series = {GECCO 2013},
 year = {2013},
 isbn = {978-1-4503-1963-8},
 location = {Amsterdam, The Netherlands},
 pages = {1061--1068},
 numpages = {8},
 url = {http://people.idsia.ch/~juergen/compressednetworksearch.html},
 doi = {10.1145/2463372.2463509},
 acmid = {2463509},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computer vision, neuroevolution, recurrent neural networks, reinforcement learning},
}
@article{s08_curiousity,
    author = {J. Schmidhuber},
    title = {Curious Model-Building Control Systems},
    journal = {In Proc. International Joint Conference on Neural Networks, Singapore},
    year = {1991},
    pages = {1458--1463},
    publisher = {IEEE}
}
@article{s07_intrinsic,
  author = {Schmidhuber, J.},
  journal = {Connection Science},
  number = 2,
  pages = {173--187},
  priority = {2},
  title = {Developmental Robotics, Optimal Artificial Curiosity, Creativity, Music, and the Fine Arts},
  volume = 18,
  year = 2006,
}
@ARTICLE{s09_optimal_order,
   author = {Schmidhuber, J.},
    title = "Optimal Ordered Problem Solver",
  journal = {ArXiv preprint},
   eprint = {cs/0207097},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computational Complexity, Computer Science - Learning, I.2.2, I.2.6, I.2.8},
     year = 2002,
    month = jul,
   url = {https://arxiv.org/abs/cs/0207097},
}
@ARTICLE{s10_powerplay,
AUTHOR={Schmidhuber, J.},   
TITLE={PowerPlay: Training an Increasingly General Problem Solver by Continually Searching for the Simplest Still Unsolvable Problem},      
JOURNAL={Frontiers in Psychology},      
VOLUME={4},      
PAGES={313},     
YEAR={2013},      
URL={https://www.frontiersin.org/article/10.3389/fpsyg.2013.00313},       
DOI={10.3389/fpsyg.2013.00313},      
ISSN={1664-1078},   
}
@ARTICLE{s11_powerplay,
   author = {Srivastava, R. and Steunebrink, B. and Schmidhuber, J.},
    title = "First Experiments with PowerPlay",
  journal = {ArXiv preprint},
archivePrefix = "arXiv",
   eprint = {1210.8385},
 primaryClass = "cs.AI",
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning},
     year = 2012,
    month = oct,
   url = {https://arxiv.org/abs/1210.8385},
}
@article{s12_lstm_forget,
 author = {Gers, F. and Schmidhuber, J. and Cummins, F.},
 title = {Learning to Forget: Continual Prediction with LSTM},
 journal = {Neural Computation},
 issue_date = {October 2000},
 volume = {12},
 number = {10},
 month = oct,
 year = {2000},
 issn = {0899-7667},
 pages = {2451--2471},
 numpages = {21},
 url = {ftp://ftp.idsia.ch/pub/juergen/FgGates-NC.pdf},
 doi = {10.1162/089976600300015015},
 acmid = {1121915},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}
@ARTICLE{pathak2017,
   author = {Pathak, D. and Agrawal, P. and Efros A. and Darrell, T.},
    title = "Curiosity-driven Exploration by Self-supervised Prediction",
  journal = {ArXiv preprint},
archivePrefix = "arXiv",
   eprint = {1705.05363},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Statistics - Machine Learning},
     year = 2017,
    month = may,
   url = {https://pathak22.github.io/noreward-rl/},
}
@article{Gottlieb2013,
  author = {Gottlieb, J. and Oudeyer, P. and Lopes, M. and Baranes, A.},
  title = "Information-seeking, curiosity, and attention: computational and neural mechanisms",
  journal = "Cell",
  year = 2013,
  month = sep,
  doi = {10.1016/j.tics.2013.09.001},
  url = {http://www.pyoudeyer.com/TICSCuriosity2013.pdf},
}
@article{Lehman2011,
title = "Abandoning objectives: Evolution through the search for novelty alone",
author = "Lehman, Joel and Stanley, Kenneth",
year = "2011",
volume = "19",
pages = "189--223",
journal = "Evolutionary Computation",
issn = "1063-6560",
publisher = "M I T Press",
number = "2",
url = "http://eplex.cs.ucf.edu/noveltysearch/userspage/",
}
  @article{Keller2012,
  title = "Sensorimotor Mismatch Signals in Primary Visual Cortex of the Behaving Mouse",
  journal = "Neuron",
  volume = "74",
  number = "5",
  pages = "809 - 815",
  year = "2012",
  issn = "0896-6273",
  doi = "https://doi.org/10.1016/j.neuron.2012.03.040",
  url = "http://www.sciencedirect.com/science/article/pii/S0896627312003844",
  author = "Georg B. Keller and Tobias Bonhoeffer and Mark Hübener"
  }
  @article{Leinweber2017,
  title = "A Sensorimotor Circuit in Mouse Cortex for Visual Flow Predictions",
  journal = "Neuron",
  volume = "95",
  number = "6",
  pages = "1420 - 1432.e5",
  year = "2017",
  issn = "0896-6273",
  doi = "https://doi.org/10.1016/j.neuron.2017.08.036",
  url = "http://www.sciencedirect.com/science/article/pii/S0896627317307791",
  author = "Marcus Leinweber and Daniel R. Ward and Jan M. Sobczak and Alexander Attinger and Georg B. Keller",
  keywords = "predictive coding, anterior cingulate cortex, visual cortex, sensorimotor integration, A24b"
  }
  @article{Pi2013,
    title="Cortical interneurons that specialize in disinhibitory control",
    author={Pi, H. and Hangya, B. and Kvitsiani, D. and Sanders, J. and Huang, Z. and Kepecs, A.},
    journal="Nature",
    doi={10.1038/nature12676},
    url={http://dx.doi.org/10.1038/nature12676},
    year=2013,
    month=nov,
  }
  @ARTICLE{Gemici2017,
     author = {Gemici, M. and Hung, C. and Santoro, A. and Wayne, G. and  Mohamed, S. and Rezende, D. and Amos, D. and Lillicrap, T.},
      title = "Generative Temporal Models with Memory",
    journal = {ArXiv preprint},
  archivePrefix = "arXiv",
     eprint = {1702.04649},
   primaryClass = "cs.AI",
   keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
       year = 2017,
      month = feb,
     url = {https://arxiv.org/abs/1702.04649},
  }
  @article{Foster2017,
  author = {David J. Foster},
  title = {Replay Comes of Age},
  journal = {Annual Review of Neuroscience},
  volume = {40},
  number = {1},
  pages = {581-602},
  year = {2017},
  doi = {10.1146/annurev-neuro-072116-031538},
  url = {https://doi.org/10.1146/annurev-neuro-072116-031538},
  }
  @book{sutton_barto,
   author = {Sutton, Richard S. and Barto, Andrew G.},
   title = {Introduction to Reinforcement Learning},
   year = {1998},
   isbn = {0262193981},
   edition = {1st},
   publisher = {MIT Press},
   address = {Cambridge, MA, USA},
   url = {http://ufal.mff.cuni.cz/~straka/courses/npfl114/2016/sutton-bookdraft2016sep.pdf},
  }
  @online{memory_consolidation,
    TITLE = "Memory Consolidation",
    AUTHOR = {Wikipedia Authors},
    YEAR = {2017},
    PUBLISHER = {Wikipedia},
    url = "https://en.wikipedia.org/wiki/Memory_consolidation",
  }
  @online{fastball_image,
    TITLE = "Baseball Image (Creative Commons)",
    AUTHOR = {Aaron Hill},
    YEAR = {2013},
    PUBLISHER = {Wikipedia},
    url = "https://commons.wikimedia.org/wiki/File:Aaron_Hill_on_July_22,_2013.jpg",
  }
  @online{baseball_player_drawings,
    TITLE = "Baseball Icon Design (CC 3.0)",
    AUTHOR = {Gilad Sotil},
    YEAR = {2018},
    PUBLISHER = {The Noun Project},
    url = "https://thenounproject.com/gilad.sotil4231c9c47bce4f03/collection/ball-games/",
  }
  @ARTICLE{Silver2016,
    author = {Silver, D. and van Hasselt, H. and Hessel, M. and Schaul, T. and Guez, A. and Harley, T. and Dulac-Arnold, G. and Reichert, D. and Rabinowitz, N. and Barreto, A. and Degris, T.},
      title = "The Predictron: End-To-End Learning and Planning",
    journal = {ArXiv preprint},
  archivePrefix = "arXiv",
     eprint = {1612.08810},
   primaryClass = "cs.LG",
   keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
       year = 2016,
      month = dec,
     url = {https://arxiv.org/abs/1612.08810},
  }
  @ARTICLE{Watters2017,
   author = {Watters, N. and Tacchetti, A. and Weber, T. and Pascanu, R. and Battaglia, P. and Zoran, D.},
    title = "Visual Interaction Networks",
    journal = {ArXiv preprint},
  archivePrefix = "arXiv",
     eprint = {1706.01433},
 primaryClass = "cs.CV",
 keywords = {Computer Science - Computer Vision and Pattern Recognition},
     year = 2017,
    month = jun,
     url = {https://arxiv.org/abs/1706.01433},
  }
  @ARTICLE{Denton2017,
   author = {Denton, E. and Birodkar, V.},
    title = "Unsupervised Learning of Disentangled Representations from Video",
    journal = {ArXiv preprint},
  archivePrefix = "arXiv",
     eprint = {1705.10915},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
     year = 2017,
    month = may,
     url = {https://arxiv.org/abs/1705.10915},
  }
  @online{pdi,
    TITLE = "Peripheral drift illusion",
    AUTHOR = {Wikipedia Authors},
    YEAR = {2017},
    PUBLISHER = {Wikipedia},
    url = "https://en.wikipedia.org/wiki/Peripheral_drift_illusion",
  }
  @online{kitaoka,
    TITLE = "Akiyoshi's Illusion Pages",
    AUTHOR = {Akiyoshi Kitaoka},
    YEAR = {2002},
    PUBLISHER = {Kanzen},
    url = "http://www.ritsumei.ac.jp/~akitaoka/index-e.html",
  }
  @ARTICLE{Watanabe2018,
  AUTHOR={Watanabe, Eiji and Kitaoka, Akiyoshi and Sakamoto, Kiwako and Yasugi, Masaki and Tanaka, Kenta},   
  TITLE={Illusory Motion Reproduced by Deep Neural Networks Trained for Prediction},
  JOURNAL="Frontiers in Psychology",
  VOLUME={9},
  PAGES={345},
  YEAR={2018},      
  URL={https://www.frontiersin.org/article/10.3389/fpsyg.2018.00345},
  DOI={10.3389/fpsyg.2018.00345},
  ISSN={1664-1078},
  }
  @online{stanley2017,
    TITLE = "Welcoming the Era of Deep Neuroevolution",
    AUTHOR = {Kenneth Stanley and Jeff Clune},
    YEAR = {2017},
    PUBLISHER = {Uber AI Research},
    url = "https://eng.uber.com/deep-neuroevolution/",
  }
  @online{how_to_train_gan,
    author = "Chintala, S. and Denton, E. and Arjovsky, M. and Mathieu, M.",
    TITLE = "How to Train a GAN? Tips and tricks to make GANs work",
    YEAR = {2016},
    PUBLISHER = {"NIPS 2016 GAN Workshop"},
    url = "https://github.com/soumith/ganhacks/blob/master/README.md",
  }
  @online{dqn_racecar,
    author = "Luc Prieur",
    TITLE = "Deep-Q Learning for Box2D Racecar RL problem.",
    YEAR = {2017},
    PUBLISHER = {"GitHub"},
    url = "https://gist.github.com/lmclupr/b35c89b2f8f81b443166e88b787b03ab#file-race-car-cv2-nn-network-td0-15-possible-actions-ipynb",
  }
  @article{pilco,
      author = {Deisenroth, M. and Rasmussen, C.},
      title = {PILCO: A Model-Based and Data-Efficient Approach to Policy Search},
      booktitle = {In Proceedings of the International Conference on Machine Learning},
      year = {2011},
      url = "http://mlg.eng.cam.ac.uk/pub/pdf/DeiRas11.pdf",
  }
  @ARTICLE{Nagabandi2017,
    author = {Nagabandi, A. and Kahn, G. and Fearing, R. and Levine, S.},
      title = "Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning",
    journal = {ArXiv preprint},
  archivePrefix = "arXiv",
     eprint = {1708.02596},
   primaryClass = "cs.LG",
   keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics},
       year = 2017,
      month = aug,
     url = {https://arxiv.org/abs/1708.02596},
  }
  @ARTICLE{McAllister2017,
    author = {McAllister, R. and Rasmussen, C.},
      title = "Data-Efficient Reinforcement Learning in Continuous-State POMDPs",
    journal = {ArXiv preprint},
  archivePrefix = "arXiv",
     eprint = {1602.02523},
   primaryClass = "stat.ML",
   keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Systems and Control},
       year = 2016,
      month = feb,
     url = {https://arxiv.org/abs/1602.02523},
  }
  @article{pilco_tutorial,
      author = {David Duvenaud},
      title = {Lecture Slides on PILCO},
      journal = "CSC 2541 Course at University of Toronto",
      year = {2016},
      url = "https://www.cs.toronto.edu/~duvenaud/courses/csc2541/slides/pilco.pdf",
  }
  @ARTICLE{Depeweg2017,
    author = {Depeweg, S. and Hernandez-Lobato, J and Doshi-Velez, F. and Udluft, S.},
      title = "Learning and Policy Search in Stochastic Dynamical Systems with Bayesian Neural Networks",
    journal = {ArXiv preprint},
  archivePrefix = "arXiv",
     eprint = {1605.07127},
   primaryClass = "stat.ML",
   keywords = {Statistics - Machine Learning, Computer Science - Learning},
       year = 2016,
      month = may,
     url = {https://arxiv.org/abs/1605.07127},
  }
  @article{deep_pilco,
      author = {Gal, Y. and McAllister, R. and Rasmussen, C.},
      title = {Improving PILCO with Bayesian Neural Network Dynamics Models},
      booktitle = {ICML Workshop on Data-Efficient Machine Learning},
      year = {2016},
      month = apr,
      url = "http://mlg.eng.cam.ac.uk/yarin/PDFs/DeepPILCO.pdf",
  }
  @ARTICLE{Hein2017,
    author = {Hein, D. and Depeweg, S. and Tokic, M. and Udluft, S. and Hentschel, A. and Runkler, T. and Sterzing, V.},
      title = "A Benchmark Environment Motivated by Industrial Control Problems",
    journal = {ArXiv preprint},
  archivePrefix = "arXiv",
     eprint = {1709.09480},
   primaryClass = "cs.AI",
   keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Computer Science - Systems and Control},
       year = 2017,
      month = sep,
     url = {https://arxiv.org/abs/1709.09480},
  }
  @ARTICLE{wavenet,
    author = {van den Oord, A. and Dieleman, S. and Zen, H. and Simonyan, K. and 
  Vinyals, O. and Graves, A. and Kalchbrenner, N. and Senior, A. and 
  Kavukcuoglu, K.},
      title = "WaveNet: A Generative Model for Raw Audio",
    journal = {ArXiv preprint},
  archivePrefix = "arXiv",
     eprint = {1609.03499},
   primaryClass = "cs.SD",
   keywords = {Computer Science - Sound, Computer Science - Learning},
       year = 2016,
      month = sep,
     url = {https://arxiv.org/abs/1609.03499},
  }
  @ARTICLE{attention,
    author = {Vaswani, A. and Shazeer, N. and Parmar, N. and Uszkoreit, J. and 
  Jones, L. and Gomez, A and Kaiser, L. and Polosukhin, I.},
      title = "Attention Is All You Need",
    journal = {ArXiv preprint},
  archivePrefix = "arXiv",
     eprint = {1706.03762},
   primaryClass = "cs.CL",
   keywords = {Computer Science - Computation and Language, Computer Science - Learning},
       year = 2017,
      month = jun,
     url = {https://arxiv.org/abs/1706.03762},
  }
  @inproceedings{Werbos87specifications,
  author= {P. J. Werbos},
  title =  {Learning How the World Works: Specifications for Predictive Networks in Robots and Brains},
  booktitle = {Proceedings of IEEE International Conference on Systems, Man and Cybernetics, N.Y.},
  year   = 1987}
  @article{Munro87,
  author={P. W. Munro},
  title={A Dual Back-Propagation Scheme for Scalar Reinforcement Learning},
  journal={Proceedings of the Ninth Annual Conference of the Cognitive Science Society, Seattle, WA},
  pages={165-176},
  year = 1987}
  @inproceedings{RobinsonFallside89,
  author = {T. Robinson and F. Fallside},
  title  = {Dynamic Reinforcement Driven Error Propagation Networks with Application to Game Playing}, 
  booktitle = {CogSci 89},
  year = 1989}
  @inproceedings{Werbos89identification,
  author= {P. J. Werbos},
  title =  {Neural Networks for Control and System Identification},
  booktitle = {Proceedings of IEEE/CDC Tampa, Florida},
  year   = 1989}
  @inproceedings{NguyenWidrow89,
  author = {N. Nguyen and B. Widrow},
  title  = {The truck backer-upper: An example of self learning in neural networks},
  booktitle = {Proceedings of the International Joint Conference on Neural Networks},
  pages={357-363},
  publisher = {IEEE Press},
  place = {Piscataway, NU},
  year = 1989}
  @book{wiering2012,
    title={Reinforcement Learning},
    author={Wiering, Marco and van Otterlo, Martijn},
    year={2012},
    publisher={Springer}
  }
  @Article{Kaelbling:96,
  author =       "L. P. Kaelbling and M. L. Littman and A. W. Moore",
  title =        "Reinforcement learning: a survey",
  journal =      "Journal of AI research",
  volume =       "4",
  year =         "1996",
  pages =        "237--285",
  }
  @mastersthesis{Linnainmaa:1970,
  title = "The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors",
  author = "Linnainmaa, S.",
  year = "1970",
  school = "Univ. Helsinki"
  }
  @article{Kelley:1960,
    author="H. J. Kelley",
    title="Gradient Theory of Optimal Flight Paths",
    journal="ARS Journal",
    volume="30",
    number="10",
    pages="947-954",
    year="1960"
  }
  @inproceedings{Werbos:81sensitivity,
  author= {P. J. Werbos},
  title =  {Applications of Advances in Nonlinear Sensitivity Analysis},
  booktitle = {Proceedings of the 10th IFIP Conference, 31.8 - 4.9, NYC},
  pages = {762-770},
  year   = {1981}}
@incollection{werbos1982sensitivity,
  title={Applications of advances in nonlinear sensitivity analysis},
  author={Werbos, Paul J.},
  booktitle={System modeling and optimization},
  pages={762--770},
  year={1982},
  publisher={Springer}
}
  @techreport{SchmidhuberStorck:94,
  author = {J. Schmidhuber and J. Storck and S. Hochreiter},
  title =  {Reinforcement driven information acquisition in
  nondeterministic environments},
  number = {FKI- -94},
  institution = {TUM Department of Informatics},
  year   = {1994}}
  @inProceedings{Lin:91,
   author = {Lin, Long-Ji},
   title = {Programming Robots Using Reinforcement Learning and Teaching},
   booktitle = {Proceedings of the Ninth National Conference on Artificial Intelligence - Volume 2},
   series = {AAAI 1991},
   year = {1991},
   isbn = {0-262-51059-6},
   location = {Anaheim, California},
   pages = {781--786},
   numpages = {6},
   url = {http://dl.acm.org/citation.cfm?id=1865756.1865798},
   acmid = {1865798},
   publisher = {AAAI Press},
  }
@article{chunker91and92,
author = {J. Schmidhuber},
title={Learning Complex, Extended Sequences Using the Principle
of History Compression},
journal={Neural Computation},
volume = {4},
number = {2},
pages={234-242},
note={(Based on TR FKI-148-91, TUM, 1991)},
year   = {1992}}
@article{onebignet2018,
  title={One Big Net For Everything},
  author={J. Schmidhuber},
  journal={Preprint arXiv:1802.08864 [cs.AI]},
  month={February},
  year={2018},
  url = {https://arxiv.org/abs/1802.08864},
}
  @ARTICLE{Byravan2017,
    author = {Byravan, A. and Leeb, F. and Meier, F. and Fox, D.},
      title = "SE3-Pose-Nets: Structured Deep Dynamics Models for Visuomotor Planning and Control",
    journal = {ArXiv preprint},
  archivePrefix = "arXiv",
     eprint = {1710.00489},
   primaryClass = "cs.RO",
   keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Computer Science - Systems and Control},
       year = 2017,
      month = oct,
     url = {https://arxiv.org/abs/1710.00489},
  }
</script>
<script src="./World Models_files/p5.custom.js.download"></script>
<script src="./World Models_files/p5.dom.min.js.download"></script>
<script src="./World Models_files/deeplearn.0.3.10.js.download"></script>
<script src="./World Models_files/b64tool.js.download"></script>
<script src="./World Models_files/doomrnn.min.js.download"></script>
<script src="./World Models_files/doomvae.min.js.download"></script>
<script src="./World Models_files/carvae.min.js.download"></script>
<script src="./World Models_files/carrnn.min.js.download"></script>
<!--
<script src="demo/lib/p5.custom.js"></script>
<script src="demo/lib/p5.dom.min.js"></script>
<script src="demo/lib/deeplearn.0.3.10.js"></script>
<script src="demo/lib/b64tool.js"></script>
<script src="demo/models/doomrnn.min.js"></script>
<script src="demo/models/doomvae.min.js"></script>
<script src="demo/models/carvae.min.js"></script>
<script src="demo/models/carrnn.min.js"></script>
-->
<script src="./World Models_files/model.js.download"></script>
<script src="./World Models_files/carrnn_demo.js.download"></script>
<script src="./World Models_files/doomrnn_demo.js.download"></script>
<script src="./World Models_files/vae_demo.js.download"></script>
<script src="./World Models_files/start_demo.js.download"></script>
<script>
console.log("Everything is loaded.")
</script><style>
    dt-cite {
      color: hsla(206, 90%, 20%, 0.7);
    }
    dt-cite .citation-number {
      cursor: default;
      white-space: nowrap;
      font-family: -apple-system, BlinkMacSystemFont, "Roboto", Helvetica, sans-serif;
      font-size: 75%;
      color: hsla(206, 90%, 20%, 0.7);
      display: inline-block;
      line-height: 1.1em;
      text-align: center;
      position: relative;
      top: -2px;
      margin: 0 2px;
    }
    figcaption dt-cite .citation-number {
      font-size: 11px;
      font-weight: normal;
      top: -2px;
      line-height: 1em;
    }
  </style><div id="cite-hover-boxes-container"><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-0"><b>OpenAI Gym</b>  <a href="http://arxiv.org/pdf/1606.01540.pdf">[PDF]</a><br>G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, W. Zaremba.<br>ArXiv preprint. 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-1"><b>Understanding Comics: The Invisible Art</b>  <a href="https://en.wikipedia.org/wiki/Understanding_Comics">[link]</a><br>S. McCloud. Tundra Publishing. 1993. <br><br><b>More thoughts from Understanding Comics by Scott McCloud</b>  <a href="http://memeengine.tumblr.com/post/28333277260/more-thoughts-from-understanding-comics-by-scott">[link]</a><br>M. E. Tumblr. 2012. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-2"><b>Counterintuitive behavior of social systems</b>  <a href="https://en.wikipedia.org/wiki/Mental_model">[link]</a><br>J.W. Forrester.<br>Technology Review. 1971. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-3"><b>The Code for Facial Identity in the Primate Brain</b>  <a href="http://www.cell.com/cell/fulltext/S0092-8674%2817%2930538-X">[link]</a><br>L. Cheang, D. Tsao.<br>Cell. 2017. <br> <a href="https://doi.org/10.1016/j.cell.2017.05.011" style="text-decoration:inherit;">DOI: 10.1016/j.cell.2017.05.011</a><br><br><b>Invariant visual representation by single neurons in the human brain</b>  <a href="http://www.nature.com/nature/journal/v435/n7045/abs/nature03687.html">[HTML]</a><br>R. Quiroga, L. Reddy, G. Kreiman, C. Koch, I. Fried.<br>Nature. 2005. <br> <a href="https://doi.org/10.1038/nature03687" style="text-decoration:inherit;">DOI: 10.1038/nature03687</a></div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-4"><b>Primary Visual Cortex Represents the Difference Between Past and Present</b>  <a href="http://dx.doi.org/10.1093/cercor/bht318">[link]</a><br>N. Nortmann, S. Rekauzke, S. Onat, P. König, D. Jancke.<br>Cerebral Cortex, Vol 25(6), pp. 1427-1440. 2015. <br> <a href="https://doi.org/10.1093/cercor/bht318" style="text-decoration:inherit;">DOI: 10.1093/cercor/bht318</a><br><br><b>Motion-Dependent Representation of Space in Area MT+</b>  <a href="http://dx.doi.org/10.1016/j.neuron.2013.03.010">[link]</a><br>M. Gerrit, J. Fischer, D. Whitney.<br>Neuron. 2013. <br> <a href="https://doi.org/10.1016/j.neuron.2013.03.010" style="text-decoration:inherit;">DOI: 10.1016/j.neuron.2013.03.010</a></div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-5"><b>Akiyoshi’s Illusion Pages</b>  <a href="http://www.ritsumei.ac.jp/~akitaoka/index-e.html">[HTML]</a><br>A. Kitaoka.<br>Kanzen. 2002. <br><br><b>Peripheral drift illusion</b>  <a href="https://en.wikipedia.org/wiki/Peripheral_drift_illusion">[link]</a><br>W. Authors.<br>Wikipedia. 2017. <br><br><b>Illusory Motion Reproduced by Deep Neural Networks Trained for Prediction</b>  <a href="https://www.frontiersin.org/article/10.3389/fpsyg.2018.00345">[link]</a><br>E. Watanabe, A. Kitaoka, K. Sakamoto, M. Yasugi, K. Tanaka.<br>Frontiers in Psychology, Vol 9, pp. 345. 2018. <br> <a href="https://doi.org/10.3389/fpsyg.2018.00345" style="text-decoration:inherit;">DOI: 10.3389/fpsyg.2018.00345</a></div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-6"><b>Sensorimotor Mismatch Signals in Primary Visual Cortex of the Behaving Mouse</b>  <a href="http://www.sciencedirect.com/science/article/pii/S0896627312003844">[link]</a><br>G. Keller, T. Bonhoeffer, M. Hübener.<br>Neuron, Vol 74(5), pp. 809 - 815. 2012. <br> <a href="https://doi.org/https://doi.org/10.1016/j.neuron.2012.03.040" style="text-decoration:inherit;">DOI: https://doi.org/10.1016/j.neuron.2012.03.040</a><br><br><b>A Sensorimotor Circuit in Mouse Cortex for Visual Flow Predictions</b>  <a href="http://www.sciencedirect.com/science/article/pii/S0896627317307791">[link]</a><br>M. Leinweber, D.R. Ward, J.M. Sobczak, A. Attinger, G.B. Keller.<br>Neuron, Vol 95(6), pp. 1420 - 1432.e5. 2017. <br> <a href="https://doi.org/https://doi.org/10.1016/j.neuron.2017.08.036" style="text-decoration:inherit;">DOI: https://doi.org/10.1016/j.neuron.2017.08.036</a></div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-7"><b>The ecology of human fear: survival optimization and the nervous system.</b>  <a href="https://www.frontiersin.org/article/10.3389/fnins.2015.00055">[link]</a><br>D. Mobbs, C.C. Hagan, T. Dalgleish, B. Silston, C. Prévost.<br>Frontiers in Neuroscience. 2015. <br> <a href="https://doi.org/10.3389/fnins.2015.00055" style="text-decoration:inherit;">DOI: 10.3389/fnins.2015.00055</a></div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-8"><b>Baseball Icon Design (CC 3.0)</b>  <a href="https://thenounproject.com/gilad.sotil4231c9c47bce4f03/collection/ball-games/">[link]</a><br>G. Sotil.<br>The Noun Project. 2018. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-9"><b>Motion-Dependent Representation of Space in Area MT+</b>  <a href="http://dx.doi.org/10.1016/j.neuron.2013.03.010">[link]</a><br>M. Gerrit, J. Fischer, D. Whitney.<br>Neuron. 2013. <br> <a href="https://doi.org/10.1016/j.neuron.2013.03.010" style="text-decoration:inherit;">DOI: 10.1016/j.neuron.2013.03.010</a></div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-10"><b>Tracking Fastballs</b>  <a href="http://sciencenetlinks.com/science-news/science-updates/tracking-fastballs/">[link]</a><br>B. Hirshon.<br>Science Update Interview. 2013. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-11"><b>Reinforcement learning: a survey</b><br>L.P. Kaelbling, M.L. Littman, A.W. Moore.<br>Journal of AI research, Vol 4, pp. 237—285. 1996. <br><br><b>Introduction to Reinforcement Learning</b>  <a href="http://ufal.mff.cuni.cz/~straka/courses/npfl114/2016/sutton-bookdraft2016sep.pdf">[PDF]</a><br>R.S. Sutton, A.G. Barto.<br>MIT Press. 1998. <br><br><b>Reinforcement Learning</b><br>M. Wiering, M. van Otterlo.<br>Springer. 2012. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-12"><b>Learning How the World Works: Specifications for Predictive Networks in Robots and Brains</b><br>P.J. Werbos.<br>Proceedings of IEEE International Conference on Systems, Man and Cybernetics, N.Y.. 1987. <br><br><b>David Silver’s Lecture on Integrating Learning and Planning</b>  <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/dyna.pdf">[PDF]</a><br>D. Silver.  2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-13"><b>Making the World Differentiable: On Using Self-Supervised Fully Recurrent Neural Networks for Dynamic Reinforcement Learning and Planning in Non-Stationary Environments</b>  <a href="http://people.idsia.ch/~juergen/FKI-126-90_(revised)bw_ocr.pdf">[PDF]</a><br>J. Schmidhuber.  1990. <br><br><b>An on-line algorithm for dynamic reinforcement learning and planning in reactive environments</b>  <a href="ftp://ftp.idsia.ch/pub/juergen/ijcnn90.ps.gz">[link]</a><br>J. Schmidhuber.<br>1990 IJCNN International Joint Conference on Neural Networks, pp. 253-258 vol.2. 1990. <br> <a href="https://doi.org/10.1109/IJCNN.1990.137723" style="text-decoration:inherit;">DOI: 10.1109/IJCNN.1990.137723</a><br><br><b>Reinforcement Learning in Markovian and Non-Markovian Environments</b>  <a href="http://papers.nips.cc/paper/393-reinforcement-learning-in-markovian-and-non-markovian-environments.pdf">[PDF]</a><br>J. Schmidhuber.<br>Advances in Neural Information Processing Systems 3, pp. 500—506. Morgan-Kaufmann. 1991. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-14"><b>The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors</b><br>S. Linnainmaa.  1970. <br><br><b>Gradient Theory of Optimal Flight Paths</b><br>H.J. Kelley.<br>ARS Journal, Vol 30(10), pp. 947-954. 1960. <br><br><b>Applications of advances in nonlinear sensitivity analysis</b><br>P.J. Werbos.<br>System modeling and optimization, pp. 762—770. Springer. 1982. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-15"><b>Deep Reinforcement Learning: A Brief Survey</b>  <a href="http://arxiv.org/pdf/1708.05866.pdf">[PDF]</a><br>K. Arulkumaran, M.P. Deisenroth, M. Brundage, A.A. Bharath.<br>IEEE Signal Processing Magazine, Vol 34(6), pp. 26-38. 2017. <br> <a href="https://doi.org/10.1109/MSP.2017.2743240" style="text-decoration:inherit;">DOI: 10.1109/MSP.2017.2743240</a><br><br><b>Deep Learning in Neural Networks: An Overview</b><br>J. Schmidhuber.<br>Neural Networks, Vol 61, pp. 85-117. 2015. <br> <a href="https://doi.org/10.1016/j.neunet.2014.09.003" style="text-decoration:inherit;">DOI: 10.1016/j.neunet.2014.09.003</a></div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-16"><b>Making the World Differentiable: On Using Self-Supervised Fully Recurrent Neural Networks for Dynamic Reinforcement Learning and Planning in Non-Stationary Environments</b>  <a href="http://people.idsia.ch/~juergen/FKI-126-90_(revised)bw_ocr.pdf">[PDF]</a><br>J. Schmidhuber.  1990. <br><br><b>An on-line algorithm for dynamic reinforcement learning and planning in reactive environments</b>  <a href="ftp://ftp.idsia.ch/pub/juergen/ijcnn90.ps.gz">[link]</a><br>J. Schmidhuber.<br>1990 IJCNN International Joint Conference on Neural Networks, pp. 253-258 vol.2. 1990. <br> <a href="https://doi.org/10.1109/IJCNN.1990.137723" style="text-decoration:inherit;">DOI: 10.1109/IJCNN.1990.137723</a><br><br><b>Reinforcement Learning in Markovian and Non-Markovian Environments</b>  <a href="http://papers.nips.cc/paper/393-reinforcement-learning-in-markovian-and-non-markovian-environments.pdf">[PDF]</a><br>J. Schmidhuber.<br>Advances in Neural Information Processing Systems 3, pp. 500—506. Morgan-Kaufmann. 1991. <br><br><b>A Possibility for Implementing Curiosity and Boredom in Model-building Neural Controllers</b>  <a href="ftp://ftp.idsia.ch/pub/juergen/curiositysab.pdf">[PDF]</a><br>J. Schmidhuber.<br>Proceedings of the First International Conference on Simulation of Adaptive Behavior on From Animals to Animats, pp. 222—227. MIT Press. 1990. <br><br><b>On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models</b>  <a href="http://arxiv.org/pdf/1511.09249.pdf">[PDF]</a><br>J. Schmidhuber. ArXiv preprint. 2015. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-17"><b>On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models</b>  <a href="http://arxiv.org/pdf/1511.09249.pdf">[PDF]</a><br>J. Schmidhuber. ArXiv preprint. 2015. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-18"><b>Auto-Encoding Variational Bayes</b>  <a href="http://arxiv.org/pdf/1312.6114.pdf">[PDF]</a><br>D. Kingma, M. Welling.<br>ArXiv preprint. 2013. <br><br><b>Stochastic Backpropagation and Approximate Inference in Deep Generative Models</b>  <a href="http://arxiv.org/pdf/1401.4082.pdf">[PDF]</a><br>D. Jimenez Rezende, S. Mohamed, D. Wierstra.<br>ArXiv preprint. 2014. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-19"><b>Auto-Encoding Variational Bayes</b>  <a href="http://arxiv.org/pdf/1312.6114.pdf">[PDF]</a><br>D. Kingma, M. Welling.<br>ArXiv preprint. 2013. <br><br><b>Stochastic Backpropagation and Approximate Inference in Deep Generative Models</b>  <a href="http://arxiv.org/pdf/1401.4082.pdf">[PDF]</a><br>D. Jimenez Rezende, S. Mohamed, D. Wierstra.<br>ArXiv preprint. 2014. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-20"><b>ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning</b>  <a href="http://arxiv.org/pdf/1605.02097.pdf">[PDF]</a><br>M. Kempka, M. Wydmuch, G. Runc, J. Toczek, W. Jaskowski.<br>IEEE Conference on Computational Intelligence and Games, pp. 341—348. IEEE. 2016. <br><br><b>DoomTakeCover-v0</b>  <a href="https://gym.openai.com/envs/DoomTakeCover-v0/">[link]</a><br>P. Paquette.<br> 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-21"><b>A Neural Representation of Sketch Drawings</b>  <a href="https://magenta.tensorflow.org/sketch-rnn-demo">[link]</a><br>D. Ha, D. Eck. ArXiv preprint. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-22"><b>Draw Together with a Neural Network</b>  <a href="https://magenta.tensorflow.org/sketch-rnn-demo">[link]</a><br>D. Ha, J. Jongejan, I. Johnson.<br>Google AI Experiments. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-23"><b>Mixture density networks</b>  <a href="http://publications.aston.ac.uk/373/">[link]</a><br>C.M. Bishop.<br>Technical Report. Aston University. 1994. <br><br><b>Mixture Density Networks with TensorFlow</b>  <a href="http://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/">[link]</a><br>D. Ha. blog.otoro.net. 2015. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-24"><b>Generating sequences with recurrent neural networks</b>  <a href="http://arxiv.org/pdf/1308.0850.pdf">[PDF]</a><br>A. Graves. ArXiv preprint. 2013. <br><br><b>Recurrent Neural Network Tutorial for Artists</b>  <a href="http://blog.otoro.net/2017/01/01/recurrent-neural-network-artist/">[link]</a><br>D. Ha. blog.otoro.net. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-25"><b>Generating sequences with recurrent neural networks</b>  <a href="http://arxiv.org/pdf/1308.0850.pdf">[PDF]</a><br>A. Graves. ArXiv preprint. 2013. <br><br><b>Experiments in Handwriting with a Neural Network</b>  <a href="http://distill.pub/2016/handwriting">[link]</a><br>S. Carter, D. Ha, I. Johnson, C. Olah.<br>Distill. 2016. <br> <a href="https://doi.org/10.23915/distill.00004" style="text-decoration:inherit;">DOI: 10.23915/distill.00004</a></div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-26"><b>A Neural Representation of Sketch Drawings</b>  <a href="https://magenta.tensorflow.org/sketch-rnn-demo">[link]</a><br>D. Ha, D. Eck. ArXiv preprint. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-27"><b>OpenAI Gym</b>  <a href="http://arxiv.org/pdf/1606.01540.pdf">[PDF]</a><br>G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, W. Zaremba.<br>ArXiv preprint. 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-28"><b>Evolutionsstrategie: optimierung technischer systeme nach prinzipien der biologischen evolution</b>  <a href="https://en.wikipedia.org/wiki/Ingo_Rechenberg">[link]</a><br>I. Rechenberg. Frommann-Holzboog. 1973. <br><br><b>Numerical Optimization of Computer Models</b>  <a href="https://en.wikipedia.org/wiki/Hans-Paul_Schwefel">[link]</a><br>H. Schwefel.<br>John Wiley and Sons, Inc. 1977. <br><br><b>A Visual Guide to Evolution Strategies</b>  <a href="http://blog.otoro.net/2017/10/29/visual-evolution-strategies/">[link]</a><br>D. Ha. blog.otoro.net. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-29"><b>The CMA Evolution Strategy: A Tutorial</b>  <a href="http://arxiv.org/pdf/1604.00772.pdf">[PDF]</a><br>N. Hansen. ArXiv preprint. 2016. <br><br><b>Completely Derandomized Self-Adaptation in Evolution Strategies</b>  <a href="http://www.cmap.polytechnique.fr/~nikolaus.hansen/cmaartic.pdf">[PDF]</a><br>N. Hansen, A. Ostermeier.<br>Evolutionary Computation, Vol 9(2), pp. 159—195. MIT Press. 2001. <br> <a href="https://doi.org/10.1162/106365601750190398" style="text-decoration:inherit;">DOI: 10.1162/106365601750190398</a><br><br><b>A Visual Guide to Evolution Strategies</b>  <a href="http://blog.otoro.net/2017/10/29/visual-evolution-strategies/">[link]</a><br>D. Ha. blog.otoro.net. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-30"><b>CarRacing-v0</b>  <a href="https://gym.openai.com/envs/CarRacing-v0/">[link]</a><br>O. Klimov.<br> 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-31"><b>CarRacing-v0</b>  <a href="https://gym.openai.com/envs/CarRacing-v0/">[link]</a><br>O. Klimov.<br> 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-32"><b>CarRacing-v0</b>  <a href="https://gym.openai.com/envs/CarRacing-v0/">[link]</a><br>O. Klimov.<br> 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-33"><b>Self-driving cars in the browser</b>  <a href="http://janhuenermann.com/projects/learning-to-drive">[link]</a><br>J. Hünermann.  2017. <br><br><b>Mar I/O Kart</b>  <a href="https://youtu.be/S9Y_I9vY8Qw">[link]</a><br>S. Bling.<br> 2015. <br><br><b>Using Keras and Deep Deterministic Policy Gradient to play TORCS</b>  <a href="https://yanpanlau.github.io/2016/10/11/Torcs-Keras.html">[HTML]</a><br>B. Lau.  2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-34"><b>CarRacing-v0</b>  <a href="https://gym.openai.com/envs/CarRacing-v0/">[link]</a><br>O. Klimov.<br> 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-35"><b>Car Racing using Reinforcement Learning</b>  <a href="https://web.stanford.edu/class/cs221/2017/restricted/p-final/elibol/final.pdf">[PDF]</a><br>M. Khan, O. Elibol.  2016. <br><br><b>Reinforcement Car Racing with A3C</b>  <a href="https://www.scribd.com/document/358019044/Reinforcement-Car-Racing-with-A3C">[link]</a><br>S. Jang, J. Min, C. Lee.  2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-36"><b>Deep-Q Learning for Box2D Racecar RL problem.</b>  <a href="https://gist.github.com/lmclupr/b35c89b2f8f81b443166e88b787b03ab#file-race-car-cv2-nn-network-td0-15-possible-actions-ipynb">[link]</a><br>L. Prieur. “GitHub”. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-37"><b>Reinforcement Car Racing with A3C</b>  <a href="https://www.scribd.com/document/358019044/Reinforcement-Car-Racing-with-A3C">[link]</a><br>S. Jang, J. Min, C. Lee.  2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-38"><b>Car Racing using Reinforcement Learning</b>  <a href="https://web.stanford.edu/class/cs221/2017/restricted/p-final/elibol/final.pdf">[PDF]</a><br>M. Khan, O. Elibol.  2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-39"><b>CarRacing-v0</b>  <a href="https://gym.openai.com/envs/CarRacing-v0/">[link]</a><br>O. Klimov.<br> 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-40"><b>Car Racing using Reinforcement Learning</b>  <a href="https://web.stanford.edu/class/cs221/2017/restricted/p-final/elibol/final.pdf">[PDF]</a><br>M. Khan, O. Elibol.  2016. <br><br><b>Reinforcement Car Racing with A3C</b>  <a href="https://www.scribd.com/document/358019044/Reinforcement-Car-Racing-with-A3C">[link]</a><br>S. Jang, J. Min, C. Lee.  2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-41"><b>CarRacing-v0</b>  <a href="https://gym.openai.com/envs/CarRacing-v0/">[link]</a><br>O. Klimov.<br> 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-42"><b>Reinforcement Car Racing with A3C</b>  <a href="https://www.scribd.com/document/358019044/Reinforcement-Car-Racing-with-A3C">[link]</a><br>S. Jang, J. Min, C. Lee.  2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-43"><b>Car Racing using Reinforcement Learning</b>  <a href="https://web.stanford.edu/class/cs221/2017/restricted/p-final/elibol/final.pdf">[PDF]</a><br>M. Khan, O. Elibol.  2016. <br><br><b>Reinforcement Car Racing with A3C</b>  <a href="https://www.scribd.com/document/358019044/Reinforcement-Car-Racing-with-A3C">[link]</a><br>S. Jang, J. Min, C. Lee.  2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-44"><b>ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning</b>  <a href="http://arxiv.org/pdf/1605.02097.pdf">[PDF]</a><br>M. Kempka, M. Wydmuch, G. Runc, J. Toczek, W. Jaskowski.<br>IEEE Conference on Computational Intelligence and Games, pp. 341—348. IEEE. 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-45"><b>ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning</b>  <a href="http://arxiv.org/pdf/1605.02097.pdf">[PDF]</a><br>M. Kempka, M. Wydmuch, G. Runc, J. Toczek, W. Jaskowski.<br>IEEE Conference on Computational Intelligence and Games, pp. 341—348. IEEE. 2016. <br><br><b>DoomTakeCover-v0</b>  <a href="https://gym.openai.com/envs/DoomTakeCover-v0/">[link]</a><br>P. Paquette.<br> 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-46"><b>DoomTakeCover-v0</b>  <a href="https://gym.openai.com/envs/DoomTakeCover-v0/">[link]</a><br>P. Paquette.<br> 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-47"><b>OpenAI Gym</b>  <a href="http://arxiv.org/pdf/1606.01540.pdf">[PDF]</a><br>G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, W. Zaremba.<br>ArXiv preprint. 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-48"><b>A Neural Representation of Sketch Drawings</b>  <a href="https://magenta.tensorflow.org/sketch-rnn-demo">[link]</a><br>D. Ha, D. Eck. ArXiv preprint. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-49"><b>Video Game Exploits</b>  <a href="https://en.wikipedia.org/wiki/Video_game_exploits">[link]</a><br>A. Wikipedia.<br>Wikipedia. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-50"><b>Action-Conditional Video Prediction using Deep Networks in Atari Games</b>  <a href="http://arxiv.org/pdf/1507.08750.pdf">[PDF]</a><br>J. Oh, X. Guo, H. Lee, R. Lewis, S. Singh.<br>ArXiv preprint. 2015. <br><br><b>Recurrent Environment Simulators</b>  <a href="http://arxiv.org/pdf/1704.02254.pdf">[PDF]</a><br>S. Chiappa, S. Racaniere, D. Wierstra, S. Mohamed.<br>ArXiv preprint. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-51"><b>Making the World Differentiable: On Using Self-Supervised Fully Recurrent Neural Networks for Dynamic Reinforcement Learning and Planning in Non-Stationary Environments</b>  <a href="http://people.idsia.ch/~juergen/FKI-126-90_(revised)bw_ocr.pdf">[PDF]</a><br>J. Schmidhuber.  1990. <br><br><b>An on-line algorithm for dynamic reinforcement learning and planning in reactive environments</b>  <a href="ftp://ftp.idsia.ch/pub/juergen/ijcnn90.ps.gz">[link]</a><br>J. Schmidhuber.<br>1990 IJCNN International Joint Conference on Neural Networks, pp. 253-258 vol.2. 1990. <br> <a href="https://doi.org/10.1109/IJCNN.1990.137723" style="text-decoration:inherit;">DOI: 10.1109/IJCNN.1990.137723</a><br><br><b>Reinforcement Learning in Markovian and Non-Markovian Environments</b>  <a href="http://papers.nips.cc/paper/393-reinforcement-learning-in-markovian-and-non-markovian-environments.pdf">[PDF]</a><br>J. Schmidhuber.<br>Advances in Neural Information Processing Systems 3, pp. 500—506. Morgan-Kaufmann. 1991. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-52"><b>PILCO: A Model-Based and Data-Efficient Approach to Policy Search</b>  <a href="http://mlg.eng.cam.ac.uk/pub/pdf/DeiRas11.pdf">[PDF]</a><br>M. Deisenroth, C. Rasmussen.<br>In Proceedings of the International Conference on Machine Learning. 2011. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-53"><b>Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning</b>  <a href="http://arxiv.org/pdf/1708.02596.pdf">[PDF]</a><br>A. Nagabandi, G. Kahn, R. Fearing, S. Levine.<br>ArXiv preprint. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-54"><b>On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models</b>  <a href="http://arxiv.org/pdf/1511.09249.pdf">[PDF]</a><br>J. Schmidhuber. ArXiv preprint. 2015. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-55"><b>Making the World Differentiable: On Using Self-Supervised Fully Recurrent Neural Networks for Dynamic Reinforcement Learning and Planning in Non-Stationary Environments</b>  <a href="http://people.idsia.ch/~juergen/FKI-126-90_(revised)bw_ocr.pdf">[PDF]</a><br>J. Schmidhuber.  1990. <br><br><b>An on-line algorithm for dynamic reinforcement learning and planning in reactive environments</b>  <a href="ftp://ftp.idsia.ch/pub/juergen/ijcnn90.ps.gz">[link]</a><br>J. Schmidhuber.<br>1990 IJCNN International Joint Conference on Neural Networks, pp. 253-258 vol.2. 1990. <br> <a href="https://doi.org/10.1109/IJCNN.1990.137723" style="text-decoration:inherit;">DOI: 10.1109/IJCNN.1990.137723</a><br><br><b>Reinforcement Learning in Markovian and Non-Markovian Environments</b>  <a href="http://papers.nips.cc/paper/393-reinforcement-learning-in-markovian-and-non-markovian-environments.pdf">[PDF]</a><br>J. Schmidhuber.<br>Advances in Neural Information Processing Systems 3, pp. 500—506. Morgan-Kaufmann. 1991. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-56"><b>DoomTakeCover-v0</b>  <a href="https://gym.openai.com/envs/DoomTakeCover-v0/">[link]</a><br>P. Paquette.<br> 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-57"><b>DoomTakeCover-v0</b>  <a href="https://gym.openai.com/envs/DoomTakeCover-v0/">[link]</a><br>P. Paquette.<br> 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-58"><b>On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models</b>  <a href="http://arxiv.org/pdf/1511.09249.pdf">[PDF]</a><br>J. Schmidhuber. ArXiv preprint. 2015. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-59"><b>Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990-2010).</b>  <a href="http://people.idsia.ch/~juergen/creativity.html">[HTML]</a><br>J. Schmidhuber.<br>IEEE Trans. Autonomous Mental Development. 2010. <br><br><b>Developmental Robotics, Optimal Artificial Curiosity, Creativity, Music, and the Fine Arts</b><br>J. Schmidhuber.<br>Connection Science, Vol 18(2), pp. 173—187. 2006. <br><br><b>Curious Model-Building Control Systems</b><br>J. Schmidhuber.<br>In Proc. International Joint Conference on Neural Networks, Singapore, pp. 1458—1463. IEEE. 1991. <br><br><b>Curiosity-driven Exploration by Self-supervised Prediction</b>  <a href="https://pathak22.github.io/noreward-rl/">[link]</a><br>D. Pathak, P. Agrawal, E. A., T. Darrell.<br>ArXiv preprint. 2017. <br><br><b>Intrinsic Motivation Systems for Autonomous Mental Development</b>  <a href="http://www.pyoudeyer.com/ims.pdf">[PDF]</a><br>P. Oudeyer, F. Kaplan, V. Hafner.<br>Trans. Evol. Comp. IEEE Press. 2007. <br> <a href="https://doi.org/10.1109/TEVC.2006.890271" style="text-decoration:inherit;">DOI: 10.1109/TEVC.2006.890271</a></div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-60"><b>Reinforcement driven information acquisition in nondeterministic environments</b><br>J. Schmidhuber, J. Storck, S. Hochreiter.<br> 1994. <br><br><b>Information-seeking, curiosity, and attention: computational and neural mechanisms</b>  <a href="http://www.pyoudeyer.com/TICSCuriosity2013.pdf">[PDF]</a><br>J. Gottlieb, P. Oudeyer, M. Lopes, A. Baranes.<br>Cell. 2013. <br> <a href="https://doi.org/10.1016/j.tics.2013.09.001" style="text-decoration:inherit;">DOI: 10.1016/j.tics.2013.09.001</a></div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-61"><b>Abandoning objectives: Evolution through the search for novelty alone</b>  <a href="http://eplex.cs.ucf.edu/noveltysearch/userspage/">[link]</a><br>J. Lehman, K. Stanley.<br>Evolutionary Computation, Vol 19(2), pp. 189—223. M I T Press. 2011. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-62"><b>Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990-2010).</b>  <a href="http://people.idsia.ch/~juergen/creativity.html">[HTML]</a><br>J. Schmidhuber.<br>IEEE Trans. Autonomous Mental Development. 2010. <br><br><b>Developmental Robotics, Optimal Artificial Curiosity, Creativity, Music, and the Fine Arts</b><br>J. Schmidhuber.<br>Connection Science, Vol 18(2), pp. 173—187. 2006. <br><br><b>Curious Model-Building Control Systems</b><br>J. Schmidhuber.<br>In Proc. International Joint Conference on Neural Networks, Singapore, pp. 1458—1463. IEEE. 1991. <br><br><b>On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models</b>  <a href="http://arxiv.org/pdf/1511.09249.pdf">[PDF]</a><br>J. Schmidhuber. ArXiv preprint. 2015. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-63"><b>Memory Consolidation</b>  <a href="https://en.wikipedia.org/wiki/Memory_consolidation">[link]</a><br>W. Authors.<br>Wikipedia. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-64"><b>Replay Comes of Age</b>  <a href="https://doi.org/10.1146/annurev-neuro-072116-031538">[link]</a><br>D.J. Foster.<br>Annual Review of Neuroscience, Vol 40(1), pp. 581-602. 2017. <br> <a href="https://doi.org/10.1146/annurev-neuro-072116-031538" style="text-decoration:inherit;">DOI: 10.1146/annurev-neuro-072116-031538</a></div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-65"><b>Memory Consolidation</b>  <a href="https://en.wikipedia.org/wiki/Memory_consolidation">[link]</a><br>W. Authors.<br>Wikipedia. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-66"><b>Replay Comes of Age</b>  <a href="https://doi.org/10.1146/annurev-neuro-072116-031538">[link]</a><br>D.J. Foster.<br>Annual Review of Neuroscience, Vol 40(1), pp. 581-602. 2017. <br> <a href="https://doi.org/10.1146/annurev-neuro-072116-031538" style="text-decoration:inherit;">DOI: 10.1146/annurev-neuro-072116-031538</a></div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-67"><b>Replay Comes of Age</b>  <a href="https://doi.org/10.1146/annurev-neuro-072116-031538">[link]</a><br>D.J. Foster.<br>Annual Review of Neuroscience, Vol 40(1), pp. 581-602. 2017. <br> <a href="https://doi.org/10.1146/annurev-neuro-072116-031538" style="text-decoration:inherit;">DOI: 10.1146/annurev-neuro-072116-031538</a></div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-68"><b>Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play</b>  <a href="http://arxiv.org/pdf/1703.05407.pdf">[PDF]</a><br>S. Sukhbaatar, Z. Lin, I. Kostrikov, G. Synnaeve, A. Szlam, R. Fergus.<br>ArXiv preprint. 2017. <br><br><b>Emergent Complexity via Multi-Agent Competition</b>  <a href="http://arxiv.org/pdf/1710.03748.pdf">[PDF]</a><br>T. Bansal, J. Pachocki, S. Sidor, I. Sutskever, I. Mordatch.<br>ArXiv preprint. 2017. <br><br><b>Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments</b>  <a href="http://arxiv.org/pdf/1710.03641.pdf">[PDF]</a><br>M. Al-Shedivat, T. Bansal, Y. Burda, I. Sutskever, I. Mordatch, P. Abbeel.<br>ArXiv preprint. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-69"><b>PowerPlay: Training an Increasingly General Problem Solver by Continually Searching for the Simplest Still Unsolvable Problem</b>  <a href="https://www.frontiersin.org/article/10.3389/fpsyg.2013.00313">[link]</a><br>J. Schmidhuber.<br>Frontiers in Psychology, Vol 4, pp. 313. 2013. <br> <a href="https://doi.org/10.3389/fpsyg.2013.00313" style="text-decoration:inherit;">DOI: 10.3389/fpsyg.2013.00313</a><br><br><b>First Experiments with PowerPlay</b>  <a href="http://arxiv.org/pdf/1210.8385.pdf">[PDF]</a><br>R. Srivastava, B. Steunebrink, J. Schmidhuber.<br>ArXiv preprint. 2012. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-70"><b>Optimal Ordered Problem Solver</b>  <a href="http://arxiv.org/pdf/.pdf">[PDF]</a><br>J. Schmidhuber.<br>ArXiv preprint. 2002. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-71"><b>Learning How the World Works: Specifications for Predictive Networks in Robots and Brains</b><br>P.J. Werbos.<br>Proceedings of IEEE International Conference on Systems, Man and Cybernetics, N.Y.. 1987. <br><br><b>A Dual Back-Propagation Scheme for Scalar Reinforcement Learning</b><br>P.W. Munro.<br>Proceedings of the Ninth Annual Conference of the Cognitive Science Society, Seattle, WA, pp. 165-176. 1987. <br><br><b>Dynamic Reinforcement Driven Error Propagation Networks with Application to Game Playing</b><br>T. Robinson, F. Fallside.<br>CogSci 89. 1989. <br><br><b>Neural Networks for Control and System Identification</b><br>P.J. Werbos.<br>Proceedings of IEEE/CDC Tampa, Florida. 1989. <br><br><b>The truck backer-upper: An example of self learning in neural networks</b><br>N. Nguyen, B. Widrow.<br>Proceedings of the International Joint Conference on Neural Networks, pp. 357-363. IEEE Press. 1989. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-72"><b>Making the World Differentiable: On Using Self-Supervised Fully Recurrent Neural Networks for Dynamic Reinforcement Learning and Planning in Non-Stationary Environments</b>  <a href="http://people.idsia.ch/~juergen/FKI-126-90_(revised)bw_ocr.pdf">[PDF]</a><br>J. Schmidhuber.  1990. <br><br><b>An on-line algorithm for dynamic reinforcement learning and planning in reactive environments</b>  <a href="ftp://ftp.idsia.ch/pub/juergen/ijcnn90.ps.gz">[link]</a><br>J. Schmidhuber.<br>1990 IJCNN International Joint Conference on Neural Networks, pp. 253-258 vol.2. 1990. <br> <a href="https://doi.org/10.1109/IJCNN.1990.137723" style="text-decoration:inherit;">DOI: 10.1109/IJCNN.1990.137723</a><br><br><b>Reinforcement Learning in Markovian and Non-Markovian Environments</b>  <a href="http://papers.nips.cc/paper/393-reinforcement-learning-in-markovian-and-non-markovian-environments.pdf">[PDF]</a><br>J. Schmidhuber.<br>Advances in Neural Information Processing Systems 3, pp. 500—506. Morgan-Kaufmann. 1991. <br><br><b>A Possibility for Implementing Curiosity and Boredom in Model-building Neural Controllers</b>  <a href="ftp://ftp.idsia.ch/pub/juergen/curiositysab.pdf">[PDF]</a><br>J. Schmidhuber.<br>Proceedings of the First International Conference on Simulation of Adaptive Behavior on From Animals to Animats, pp. 222—227. MIT Press. 1990. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-73"><b>On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models</b>  <a href="http://arxiv.org/pdf/1511.09249.pdf">[PDF]</a><br>J. Schmidhuber. ArXiv preprint. 2015. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-74"><b>PILCO: A Model-Based and Data-Efficient Approach to Policy Search</b>  <a href="http://mlg.eng.cam.ac.uk/pub/pdf/DeiRas11.pdf">[PDF]</a><br>M. Deisenroth, C. Rasmussen.<br>In Proceedings of the International Conference on Machine Learning. 2011. <br><br><b>Lecture Slides on PILCO</b>  <a href="https://www.cs.toronto.edu/~duvenaud/courses/csc2541/slides/pilco.pdf">[PDF]</a><br>D. Duvenaud.<br>CSC 2541 Course at University of Toronto. 2016. <br><br><b>Data-Efficient Reinforcement Learning in Continuous-State POMDPs</b>  <a href="http://arxiv.org/pdf/1602.02523.pdf">[PDF]</a><br>R. McAllister, C. Rasmussen.<br>ArXiv preprint. 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-75"><b>Improving PILCO with Bayesian Neural Network Dynamics Models</b>  <a href="http://mlg.eng.cam.ac.uk/yarin/PDFs/DeepPILCO.pdf">[PDF]</a><br>Y. Gal, R. McAllister, C. Rasmussen.<br>ICML Workshop on Data-Efficient Machine Learning. 2016. <br><br><b>Learning and Policy Search in Stochastic Dynamical Systems with Bayesian Neural Networks</b>  <a href="http://arxiv.org/pdf/1605.07127.pdf">[PDF]</a><br>S. Depeweg, J. Hernandez-Lobato, F. Doshi-Velez, S. Udluft.<br>ArXiv preprint. 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-76"><b>A Benchmark Environment Motivated by Industrial Control Problems</b>  <a href="http://arxiv.org/pdf/1709.09480.pdf">[PDF]</a><br>D. Hein, S. Depeweg, M. Tokic, S. Udluft, A. Hentschel, T. Runkler, V. Sterzing.<br>ArXiv preprint. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-77"><b>Learning to Generate Artificial Fovea Trajectories for Target Detection</b>  <a href="ftp://ftp.idsia.ch/pub/juergen/attention.pdf">[PDF]</a><br>J. Schmidhuber, R. Huber.<br>International Journal of Neural Systems, Vol 2(1-2), pp. 125—134. 1991. <br> <a href="https://doi.org/10.1142/S012906579100011X" style="text-decoration:inherit;">DOI: 10.1142/S012906579100011X</a></div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-78"><b>Learning deep dynamical models from image pixels</b>  <a href="http://arxiv.org/pdf/1410.7550.pdf">[PDF]</a><br>N. Wahlström, T. Schön, M. Deisenroth.<br>ArXiv preprint. 2014. <br><br><b>From Pixels to Torques: Policy Learning with Deep Dynamical Models</b>  <a href="http://arxiv.org/pdf/1502.02251.pdf">[PDF]</a><br>N. Wahlström, T. Schön, M. Deisenroth.<br>ArXiv preprint. 2015. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-79"><b>Deep Spatial Autoencoders for Visuomotor Learning</b>  <a href="http://arxiv.org/pdf/1509.06113.pdf">[PDF]</a><br>C. Finn, X. Tan, Y. Duan, T. Darrell, S. Levine, P. Abbeel.<br>ArXiv preprint. 2015. <br><br><b>Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images</b>  <a href="http://arxiv.org/pdf/1506.07365.pdf">[PDF]</a><br>M. Watter, J. Springenberg, J. Boedecker, M. Riedmiller.<br>ArXiv preprint. 2015. <br><br><b>Model-Based RL Lecture at Deep RL Bootcamp 2017</b>  <a href="https://youtu.be/iC2a7M9voYU?t=44m35s">[link]</a><br>C. Finn.  2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-80"><b>Model-Based RL Lecture at Deep RL Bootcamp 2017</b>  <a href="https://youtu.be/iC2a7M9voYU?t=44m35s">[link]</a><br>C. Finn.  2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-81"><b>Game Engine Learning from Video</b>  <a href="https://doi.org/10.24963/ijcai.2017/518">[link]</a><br>B.L. Matthew Guzdial.<br>Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17, pp. 3707—3713. 2017. <br> <a href="https://doi.org/10.24963/ijcai.2017/518" style="text-decoration:inherit;">DOI: 10.24963/ijcai.2017/518</a></div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-82"><b>The truck backer-upper: An example of self learning in neural networks</b><br>N. Nguyen, B. Widrow.<br>Proceedings of the International Joint Conference on Neural Networks, pp. 357-363. IEEE Press. 1989. <br><br><b>Learning to Generate Artificial Fovea Trajectories for Target Detection</b>  <a href="ftp://ftp.idsia.ch/pub/juergen/attention.pdf">[PDF]</a><br>J. Schmidhuber, R. Huber.<br>International Journal of Neural Systems, Vol 2(1-2), pp. 125—134. 1991. <br> <a href="https://doi.org/10.1142/S012906579100011X" style="text-decoration:inherit;">DOI: 10.1142/S012906579100011X</a></div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-83"><b>Learning to Act by Predicting the Future</b>  <a href="http://arxiv.org/pdf/1611.01779.pdf">[PDF]</a><br>A. Dosovitskiy, V. Koltun.<br>ArXiv preprint. 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-84"><b>ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning</b>  <a href="http://arxiv.org/pdf/1605.02097.pdf">[PDF]</a><br>M. Kempka, M. Wydmuch, G. Runc, J. Toczek, W. Jaskowski.<br>IEEE Conference on Computational Intelligence and Games, pp. 341—348. IEEE. 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-85"><b>Generating sequences with recurrent neural networks</b>  <a href="http://arxiv.org/pdf/1308.0850.pdf">[PDF]</a><br>A. Graves. ArXiv preprint. 2013. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-86"><b>Hallucination with Recurrent Neural Networks</b>  <a href="https://www.youtube.com/watch?v=-yX1SYeDHbg&amp;t=49m33s">[link]</a><br>A. Graves.  2015. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-87"><b>Making the World Differentiable: On Using Self-Supervised Fully Recurrent Neural Networks for Dynamic Reinforcement Learning and Planning in Non-Stationary Environments</b>  <a href="http://people.idsia.ch/~juergen/FKI-126-90_(revised)bw_ocr.pdf">[PDF]</a><br>J. Schmidhuber.  1990. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-88"><b>Making the World Differentiable: On Using Self-Supervised Fully Recurrent Neural Networks for Dynamic Reinforcement Learning and Planning in Non-Stationary Environments</b>  <a href="http://people.idsia.ch/~juergen/FKI-126-90_(revised)bw_ocr.pdf">[PDF]</a><br>J. Schmidhuber.  1990. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-89"><b>An on-line algorithm for dynamic reinforcement learning and planning in reactive environments</b>  <a href="ftp://ftp.idsia.ch/pub/juergen/ijcnn90.ps.gz">[link]</a><br>J. Schmidhuber.<br>1990 IJCNN International Joint Conference on Neural Networks, pp. 253-258 vol.2. 1990. <br> <a href="https://doi.org/10.1109/IJCNN.1990.137723" style="text-decoration:inherit;">DOI: 10.1109/IJCNN.1990.137723</a><br><br><b>Reinforcement Learning in Markovian and Non-Markovian Environments</b>  <a href="http://papers.nips.cc/paper/393-reinforcement-learning-in-markovian-and-non-markovian-environments.pdf">[PDF]</a><br>J. Schmidhuber.<br>Advances in Neural Information Processing Systems 3, pp. 500—506. Morgan-Kaufmann. 1991. <br><br><b>A Possibility for Implementing Curiosity and Boredom in Model-building Neural Controllers</b>  <a href="ftp://ftp.idsia.ch/pub/juergen/curiositysab.pdf">[PDF]</a><br>J. Schmidhuber.<br>Proceedings of the First International Conference on Simulation of Adaptive Behavior on From Animals to Animats, pp. 222—227. MIT Press. 1990. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-90"><b>On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models</b>  <a href="http://arxiv.org/pdf/1511.09249.pdf">[PDF]</a><br>J. Schmidhuber. ArXiv preprint. 2015. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-91"><b>Recurrent Environment Simulators</b>  <a href="http://arxiv.org/pdf/1704.02254.pdf">[PDF]</a><br>S. Chiappa, S. Racaniere, D. Wierstra, S. Mohamed.<br>ArXiv preprint. 2017. <br><br><b>Action-Conditional Video Prediction using Deep Networks in Atari Games</b>  <a href="http://arxiv.org/pdf/1507.08750.pdf">[PDF]</a><br>J. Oh, X. Guo, H. Lee, R. Lewis, S. Singh.<br>ArXiv preprint. 2015. <br><br><b>Unsupervised Learning of Disentangled Representations from Video</b>  <a href="http://arxiv.org/pdf/1705.10915.pdf">[PDF]</a><br>E. Denton, V. Birodkar.<br>ArXiv preprint. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-92"><b>The Predictron: End-To-End Learning and Planning</b>  <a href="http://arxiv.org/pdf/1612.08810.pdf">[PDF]</a><br>D. Silver, H. van Hasselt, M. Hessel, T. Schaul, A. Guez, T. Harley, G. Dulac-Arnold, D. Reichert, N. Rabinowitz, A. Barreto, T. Degris.<br>ArXiv preprint. 2016. <br><br><b>Imagination-Augmented Agents for Deep Reinforcement Learning</b>  <a href="http://arxiv.org/pdf/1707.06203.pdf">[PDF]</a><br>T. Weber, S. Racanière, D. Reichert, L. Buesing, A. Guez, D. Rezende, A. Badia, O. Vinyals, N. Heess, Y. Li, R. Pascanu, P. Battaglia, D. Silver, D. Wierstra.<br>ArXiv preprint. 2017. <br><br><b>Visual Interaction Networks</b>  <a href="http://arxiv.org/pdf/1706.01433.pdf">[PDF]</a><br>N. Watters, A. Tacchetti, T. Weber, R. Pascanu, P. Battaglia, D. Zoran.<br>ArXiv preprint. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-93"><b>PathNet: Evolution Channels Gradient Descent in Super Neural Networks</b>  <a href="http://arxiv.org/pdf/1701.08734.pdf">[PDF]</a><br>C. Fernando, D. Banarse, C. Blundell, Y. Zwols, D. Ha, A. Rusu, A. Pritzel, D. Wierstra.<br>ArXiv preprint. 2017. <br><br><b>Evolution Strategies as a Scalable Alternative to Reinforcement Learning</b>  <a href="http://arxiv.org/pdf/1703.03864.pdf">[PDF]</a><br>T. Salimans, J. Ho, X. Chen, S. Sidor, I. Sutskever.<br>ArXiv preprint. 2017. <br><br><b>Evolving Stable Strategies</b>  <a href="http://blog.otoro.net/2017/11/12/evolving-stable-strategies/">[link]</a><br>D. Ha.<br>blog.otoro.net. 2017. <br><br><b>Welcoming the Era of Deep Neuroevolution</b>  <a href="https://eng.uber.com/deep-neuroevolution/">[link]</a><br>K. Stanley, J. Clune.<br>Uber AI Research. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-94"><b>Playing Atari with Deep Reinforcement Learning</b>  <a href="http://arxiv.org/pdf/1312.5602.pdf">[PDF]</a><br>V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, M. Riedmiller.<br>ArXiv preprint. 2013. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-95"><b>Evolving Neural Networks Through Augmenting Topologies</b>  <a href="http://nn.cs.utexas.edu/?stanley:ec02">[link]</a><br>K.O. Stanley, R. Miikkulainen.<br>Evolutionary Computation, Vol 10(2), pp. 99-127. 2002. <br><br><b>Accelerated Neural Evolution Through Cooperatively Coevolved Synapses</b>  <a href="http://people.idsia.ch/~juergen/gomez08a.pdf">[PDF]</a><br>F. Gomez, J. Schmidhuber, R. Miikkulainen.<br>Journal of Machine Learning Research, Vol 9, pp. 937—965. JMLR.org. 2008. <br><br><b>Co-evolving Recurrent Neurons Learn Deep Memory POMDPs</b>  <a href="ftp://ftp.idsia.ch/pub/juergen/gecco05gomez.pdf">[PDF]</a><br>F. Gomez, J. Schmidhuber.<br>Proceedings of the 7th Annual Conference on Genetic and Evolutionary Computation, pp. 491—498. ACM. 2005. <br> <a href="https://doi.org/10.1145/1068009.1068092" style="text-decoration:inherit;">DOI: 10.1145/1068009.1068092</a><br><br><b>Autonomous Evolution of Topographic Regularities in Artificial Neural Networks</b>  <a href="http://eplex.cs.ucf.edu/papers/gauci_nc10.pdf">[PDF]</a><br>J. Gauci, K.O. Stanley.<br>Neural Computation, Vol 22(7), pp. 1860—1898. MIT Press. 2010. <br> <a href="https://doi.org/10.1162/neco.2010.06-09-1042" style="text-decoration:inherit;">DOI: 10.1162/neco.2010.06-09-1042</a><br><br><b>Parameter-exploring policy gradients</b>  <a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A64D1AE8313A364B814998E9E245B40A?doi=10.1.1.180.7104&amp;rep=rep1&amp;type=pdf">[link]</a><br>F. Sehnke, C. Osendorfer, T. Ruckstieb, A. Graves, J. Peters, J. Schmidhuber.<br>Neural Networks, Vol 23(4), pp. 551—559. 2010. <br> <a href="https://doi.org/10.1016/j.neunet.2009.12.004" style="text-decoration:inherit;">DOI: 10.1016/j.neunet.2009.12.004</a><br><br><b>Evolving Neural Networks</b>  <a href="http://nn.cs.utexas.edu/downloads/slides/miikkulainen.ijcnn13.pdf">[PDF]</a><br>R. Miikkulainen.<br>IJCNN. 2013. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-96"><b>Evolving Large-scale Neural Networks for Vision-based Reinforcement Learning</b>  <a href="http://people.idsia.ch/~juergen/compressednetworksearch.html">[HTML]</a><br>J. Koutnik, G. Cuccu, J. Schmidhuber, F. Gomez.<br>Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation, pp. 1061—1068. ACM. 2013. <br> <a href="https://doi.org/10.1145/2463372.2463509" style="text-decoration:inherit;">DOI: 10.1145/2463372.2463509</a><br><br><b>A Neuroevolution Approach to General Atari Game Playing</b>  <a href="http://www.cs.utexas.edu/~ai-lab/?atari">[link]</a><br>M. Hausknecht, J. Lehman, R. Miikkulainen, P. Stone.<br>IEEE Transactions on Computational Intelligence and AI in Games. 2013. <br><br><b>Neuro-Visual Control in the Quake II Environment</b>  <a href="https://www.cse.unr.edu/~bdbryant/papers/parker-2012-tciaig.pdf">[PDF]</a><br>M. Parker, B. Bryant.<br>IEEE Transactions on Computational Intelligence and AI in Games. 2012. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-97"><b>Autoencoder-augmented Neuroevolution for Visual Doom Playing</b>  <a href="http://arxiv.org/pdf/1707.03902.pdf">[PDF]</a><br>S. Alvernaz, J. Togelius.<br>ArXiv preprint. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-98"><b>Making the World Differentiable: On Using Self-Supervised Fully Recurrent Neural Networks for Dynamic Reinforcement Learning and Planning in Non-Stationary Environments</b>  <a href="http://people.idsia.ch/~juergen/FKI-126-90_(revised)bw_ocr.pdf">[PDF]</a><br>J. Schmidhuber.  1990. <br><br><b>An on-line algorithm for dynamic reinforcement learning and planning in reactive environments</b>  <a href="ftp://ftp.idsia.ch/pub/juergen/ijcnn90.ps.gz">[link]</a><br>J. Schmidhuber.<br>1990 IJCNN International Joint Conference on Neural Networks, pp. 253-258 vol.2. 1990. <br> <a href="https://doi.org/10.1109/IJCNN.1990.137723" style="text-decoration:inherit;">DOI: 10.1109/IJCNN.1990.137723</a><br><br><b>Reinforcement Learning in Markovian and Non-Markovian Environments</b>  <a href="http://papers.nips.cc/paper/393-reinforcement-learning-in-markovian-and-non-markovian-environments.pdf">[PDF]</a><br>J. Schmidhuber.<br>Advances in Neural Information Processing Systems 3, pp. 500—506. Morgan-Kaufmann. 1991. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-99"><b>Cortical interneurons that specialize in disinhibitory control</b>  <a href="http://dx.doi.org/10.1038/nature12676">[link]</a><br>H. Pi, B. Hangya, D. Kvitsiani, J. Sanders, Z. Huang, A. Kepecs.<br>Nature. 2013. <br> <a href="https://doi.org/10.1038/nature12676" style="text-decoration:inherit;">DOI: 10.1038/nature12676</a></div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-100"><b>SE3-Pose-Nets: Structured Deep Dynamics Models for Visuomotor Planning and Control</b>  <a href="http://arxiv.org/pdf/1710.00489.pdf">[PDF]</a><br>A. Byravan, F. Leeb, F. Meier, D. Fox.<br>ArXiv preprint. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-101"><b>Long short-term memory</b>  <a href="ftp://ftp.idsia.ch/pub/juergen/lstm.pdf">[PDF]</a><br>S. Hochreiter, J. Schmidhuber.<br>Neural Computation. MIT Press. 1997. <br><br><b>Learning to Forget: Continual Prediction with LSTM</b>  <a href="ftp://ftp.idsia.ch/pub/juergen/FgGates-NC.pdf">[PDF]</a><br>F. Gers, J. Schmidhuber, F. Cummins.<br>Neural Computation, Vol 12(10), pp. 2451—2471. MIT Press. 2000. <br> <a href="https://doi.org/10.1162/089976600300015015" style="text-decoration:inherit;">DOI: 10.1162/089976600300015015</a></div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-102"><b>Nanoconnectomic upper bound on the variability of synaptic plasticity</b>  <a href="https://doi.org/10.7554/eLife.10778">[link]</a><br>T.M. Bartol, C. Bromer, J. Kinney, M.A. Chirillo, J.N. Bourne, K.M. Harris, T.J. Sejnowski.<br>eLife Sciences Publications, Ltd. 2015. <br> <a href="https://doi.org/10.7554/eLife.10778" style="text-decoration:inherit;">DOI: 10.7554/eLife.10778</a></div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-103"><b>Connectionist models of recognition memory: constraints imposed by learning and forgetting functions.</b><br>R.M. Ratcliff.<br>Psychological review, Vol 97 2, pp. 285-308. 1990. <br><br><b>Catastrophic interference in connectionist networks: Can It Be predicted, can It be prevented?</b>  <a href="http://papers.nips.cc/paper/799-catastrophic-interference-in-connectionist-networks-can-it-be-predicted-can-it-be-prevented.pdf">[PDF]</a><br>R.M. French.<br>Advances in Neural Information Processing Systems 6, pp. 1176—1177. Morgan-Kaufmann. 1994. <br><br><b>Overcoming catastrophic forgetting in neural networks</b>  <a href="http://arxiv.org/pdf/1612.00796.pdf">[PDF]</a><br>J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A.M. Rusu, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R. Hadsell.<br>ArXiv preprint. 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-104"><b>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</b>  <a href="http://arxiv.org/pdf/1701.06538.pdf">[PDF]</a><br>N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, J. Dean.<br>ArXiv preprint. 2017. <br><br><b>HyperNetworks</b>  <a href="http://arxiv.org/pdf/1609.09106.pdf">[PDF]</a><br>D. Ha, A. Dai, Q. Le.<br>ArXiv preprint. 2016. <br><br><b>Language Modeling with Recurrent Highway Hypernetworks</b>  <a href="http://papers.nips.cc/paper/6919-language-modeling-with-recurrent-highway-hypernetworks.pdf">[PDF]</a><br>J. Suarez.<br>Advances in Neural Information Processing Systems 30, pp. 3269—3278. Curran Associates, Inc. 2017. <br><br><b>WaveNet: A Generative Model for Raw Audio</b>  <a href="http://arxiv.org/pdf/1609.03499.pdf">[PDF]</a><br>A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, K. Kavukcuoglu.<br>ArXiv preprint. 2016. <br><br><b>Attention Is All You Need</b>  <a href="http://arxiv.org/pdf/1706.03762.pdf">[PDF]</a><br>A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, L. Kaiser, I. Polosukhin.<br>ArXiv preprint. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-105"><b>Generative Temporal Models with Memory</b>  <a href="http://arxiv.org/pdf/1702.04649.pdf">[PDF]</a><br>M. Gemici, C. Hung, A. Santoro, G. Wayne, S. Mohamed, D. Rezende, D. Amos, T. Lillicrap.<br>ArXiv preprint. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-106"><b>Making the World Differentiable: On Using Self-Supervised Fully Recurrent Neural Networks for Dynamic Reinforcement Learning and Planning in Non-Stationary Environments</b>  <a href="http://people.idsia.ch/~juergen/FKI-126-90_(revised)bw_ocr.pdf">[PDF]</a><br>J. Schmidhuber.  1990. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-107"><b>Making the World Differentiable: On Using Self-Supervised Fully Recurrent Neural Networks for Dynamic Reinforcement Learning and Planning in Non-Stationary Environments</b>  <a href="http://people.idsia.ch/~juergen/FKI-126-90_(revised)bw_ocr.pdf">[PDF]</a><br>J. Schmidhuber.  1990. <br><br><b>An on-line algorithm for dynamic reinforcement learning and planning in reactive environments</b>  <a href="ftp://ftp.idsia.ch/pub/juergen/ijcnn90.ps.gz">[link]</a><br>J. Schmidhuber.<br>1990 IJCNN International Joint Conference on Neural Networks, pp. 253-258 vol.2. 1990. <br> <a href="https://doi.org/10.1109/IJCNN.1990.137723" style="text-decoration:inherit;">DOI: 10.1109/IJCNN.1990.137723</a><br><br><b>Reinforcement Learning in Markovian and Non-Markovian Environments</b>  <a href="http://papers.nips.cc/paper/393-reinforcement-learning-in-markovian-and-non-markovian-environments.pdf">[PDF]</a><br>J. Schmidhuber.<br>Advances in Neural Information Processing Systems 3, pp. 500—506. Morgan-Kaufmann. 1991. <br><br><b>A Possibility for Implementing Curiosity and Boredom in Model-building Neural Controllers</b>  <a href="ftp://ftp.idsia.ch/pub/juergen/curiositysab.pdf">[PDF]</a><br>J. Schmidhuber.<br>Proceedings of the First International Conference on Simulation of Adaptive Behavior on From Animals to Animats, pp. 222—227. MIT Press. 1990. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-108"><b>On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models</b>  <a href="http://arxiv.org/pdf/1511.09249.pdf">[PDF]</a><br>J. Schmidhuber. ArXiv preprint. 2015. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-109"><b>One Big Net For Everything</b>  <a href="http://arxiv.org/pdf/1802.08864.pdf">[PDF]</a><br>J. Schmidhuber.<br>Preprint arXiv:1802.08864 [cs.AI]. 2018. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-110"><b>PowerPlay: Training an Increasingly General Problem Solver by Continually Searching for the Simplest Still Unsolvable Problem</b>  <a href="https://www.frontiersin.org/article/10.3389/fpsyg.2013.00313">[link]</a><br>J. Schmidhuber.<br>Frontiers in Psychology, Vol 4, pp. 313. 2013. <br> <a href="https://doi.org/10.3389/fpsyg.2013.00313" style="text-decoration:inherit;">DOI: 10.3389/fpsyg.2013.00313</a><br><br><b>First Experiments with PowerPlay</b>  <a href="http://arxiv.org/pdf/1210.8385.pdf">[PDF]</a><br>R. Srivastava, B. Steunebrink, J. Schmidhuber.<br>ArXiv preprint. 2012. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-111"><b>Learning Complex, Extended Sequences Using the Principle of History Compression</b><br>J. Schmidhuber.<br>Neural Computation, Vol 4(2), pp. 234-242. 1992. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-112"><b>CarRacing-v0</b>  <a href="https://gym.openai.com/envs/CarRacing-v0/">[link]</a><br>O. Klimov.<br> 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-113"><b>Long short-term memory</b>  <a href="ftp://ftp.idsia.ch/pub/juergen/lstm.pdf">[PDF]</a><br>S. Hochreiter, J. Schmidhuber.<br>Neural Computation. MIT Press. 1997. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-114"><b>Mixture density networks</b>  <a href="http://publications.aston.ac.uk/373/">[link]</a><br>C.M. Bishop.<br>Technical Report. Aston University. 1994. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-115"><b>Mixture Density Networks with TensorFlow</b>  <a href="http://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/">[link]</a><br>D. Ha. blog.otoro.net. 2015. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-116"><b>Generating sequences with recurrent neural networks</b>  <a href="http://arxiv.org/pdf/1308.0850.pdf">[PDF]</a><br>A. Graves. ArXiv preprint. 2013. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-117"><b>A Neural Representation of Sketch Drawings</b>  <a href="https://magenta.tensorflow.org/sketch-rnn-demo">[link]</a><br>D. Ha, D. Eck. ArXiv preprint. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-118"><b>A Neural Representation of Sketch Drawings</b>  <a href="https://magenta.tensorflow.org/sketch-rnn-demo">[link]</a><br>D. Ha, D. Eck. ArXiv preprint. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-119"><b>The CMA Evolution Strategy: A Tutorial</b>  <a href="http://arxiv.org/pdf/1604.00772.pdf">[PDF]</a><br>N. Hansen. ArXiv preprint. 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-120"><b>A Visual Guide to Evolution Strategies</b>  <a href="http://blog.otoro.net/2017/10/29/visual-evolution-strategies/">[link]</a><br>D. Ha. blog.otoro.net. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-121"><b>Evolving Stable Strategies</b>  <a href="http://blog.otoro.net/2017/11/12/evolving-stable-strategies/">[link]</a><br>D. Ha.<br>blog.otoro.net. 2017. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-122"><b>CarRacing-v0</b>  <a href="https://gym.openai.com/envs/CarRacing-v0/">[link]</a><br>O. Klimov.<br> 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-123"><b>ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning</b>  <a href="http://arxiv.org/pdf/1605.02097.pdf">[PDF]</a><br>M. Kempka, M. Wydmuch, G. Runc, J. Toczek, W. Jaskowski.<br>IEEE Conference on Computational Intelligence and Games, pp. 341—348. IEEE. 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-124"><b>DoomTakeCover-v0</b>  <a href="https://gym.openai.com/envs/DoomTakeCover-v0/">[link]</a><br>P. Paquette.<br> 2016. </div></div><div id="footnote-hover-boxes-container"><div style="display:none;" class="dt-hover-box" id="dt-fn-hover-box-0">In many RL problems, the feedback (positive or negative reward) is given at end of a sequence of steps. The credit assignment problem tackles the problem of figuring out which steps caused the resulting feedback—which steps should receive credit or blame for a final result?</div><div style="display:none;" class="dt-hover-box" id="dt-fn-hover-box-1">Typical model-free RL models have in the order of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><msup><mn>0</mn><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">10^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span><span class="mord"><span class="mord mathrm">0</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">3</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><msup><mn>0</mn><mn>6</mn></msup></mrow><annotation encoding="application/x-tex">10^6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span><span class="mord"><span class="mord mathrm">0</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">6</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> model parameters. We look at training models in the order of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><msup><mn>0</mn><mn>7</mn></msup></mrow><annotation encoding="application/x-tex">10^7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span><span class="mord"><span class="mord mathrm">0</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">7</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> parameters, which is still rather small compared to state-of-the-art deep learning models with <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><msup><mn>0</mn><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">10^8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span><span class="mord"><span class="mord mathrm">0</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">8</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> to even <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><msup><mn>0</mn><mrow><mn>9</mn></mrow></msup></mrow><annotation encoding="application/x-tex">10^{9}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span><span class="mord"><span class="mord mathrm">0</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">9</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> parameters. In principle, the procedure described in this article can take advantage of these larger networks if we wanted to use them.</div><div style="display:none;" class="dt-hover-box" id="dt-fn-hover-box-2">To be clear, the prediction of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">z_{t+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> is not fed into the controller C directly — just the hidden state <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>. This is because <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> has all the information needed to generate the parameters of a mixture of Gaussian distribution, if we want to sample <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">z_{t+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> to make a prediction.</div><div style="display:none;" class="dt-hover-box" id="dt-fn-hover-box-3">We find this task interesting because although it is not difficult to train an agent to wobble around randomly generated tracks and obtain a mediocre score, CarRacing-v0 defines “solving” as getting average reward of 900 over 100 consecutive trials, which means the agent can only afford very few driving mistakes.</div><div style="display:none;" class="dt-hover-box" id="dt-fn-hover-box-4">We will discuss an iterative training procedure later on for more complicated environments where a random policy is not sufficient.</div><div style="display:none;" class="dt-hover-box" id="dt-fn-hover-box-5">In principle, we can train both models together in an end-to-end manner, although we found that training each separately is more practical, and also achieves satisfactory results. Training each model only required less than an hour of computation time using a single NVIDIA P100 GPU. We can also train individual VAE and MDN-RNN models without having to exhaustively tune hyperparameters.</div><div style="display:none;" class="dt-hover-box" id="dt-fn-hover-box-6">We will discuss how this score compares to other models later on.</div><div style="display:none;" class="dt-hover-box" id="dt-fn-hover-box-7">In <em>Learning to Think</em>, it is acceptable that the RNN M isn’t always a reliable predictor. A (potentially evolution-based) RNN C can in principle learn to ignore a flawed M, or exploit certain useful parts of M for arbitrary computational purposes including hierarchical planning etc. This is not what we do here though — our present approach is still closer to some of the old systems, where a RNN M is used to predict and plan ahead step by step. Unlike this early work, however, we use evolution for C (like in <em>Learning to Think</em>) rather than traditional RL combined with RNNs, which has the advantage of both simplicity and generality.</div><div style="display:none;" class="dt-hover-box" id="dt-fn-hover-box-8">Another related connection is to muscle memory. For instance, as you learn to do something like play the piano, you no longer have to spend working memory capacity on translating individual notes to finger motions — this all becomes encoded at a subconscious level.</div></div><script>

function nodeFromString(str) {
  var div = document.createElement("div");
  div.innerHTML = str;
  return div.firstChild;
}

function make_hover_css(pos) {
  var pretty = window.innerWidth > 600;
  var padding = pretty? 18 : 12;
  var outer_padding = pretty ? 18 : 0;
  var bbox = document.querySelector("body").getBoundingClientRect();
  var left = pos[0] - bbox.left, top = pos[1] - bbox.top;
  var width = Math.min(window.innerWidth-2*outer_padding, 648);
  left = Math.min(left, window.innerWidth-width-outer_padding);
  width = width - 2*padding;
  return (`position: absolute;
     background-color: #FFF;
     opacity: 0.95;
     max-width: ${width}px;
     top: ${top}px;
     left: ${left}px;
     border: 1px solid rgba(0, 0, 0, 0.25);
     padding: ${padding}px;
     border-radius: ${pretty? 3 : 0}px;
     box-shadow: 0px 2px 10px 2px rgba(0, 0, 0, 0.2);
     z-index: ${1e6};`);
}


function DtHoverBox(div_id) {
  this.div = document.querySelector("#"+div_id);
  this.visible = false;
  this.bindDivEvents();
  DtHoverBox.box_map[div_id] = this;
}

DtHoverBox.box_map = {};

DtHoverBox.get_box = function get_box(div_id) {
  if (div_id in DtHoverBox.box_map) {
    return DtHoverBox.box_map[div_id];
  } else {
    return new DtHoverBox(div_id);
  }
}

DtHoverBox.prototype.show = function show(pos){
  this.visible = true;
  this.div.setAttribute("style", make_hover_css(pos) );
  for (var box_id in DtHoverBox.box_map) {
    var box = DtHoverBox.box_map[box_id];
    if (box != this) box.hide();
  }
}

DtHoverBox.prototype.showAtNode = function showAtNode(node){
    var bbox = node.getBoundingClientRect();
    this.show([bbox.right, bbox.bottom]);
}

DtHoverBox.prototype.hide = function hide(){
  this.visible = false;
  if (this.div) this.div.setAttribute("style", "display:none");
  if (this.timeout) clearTimeout(this.timeout);
}

DtHoverBox.prototype.stopTimeout = function stopTimeout() {
  if (this.timeout) clearTimeout(this.timeout);
}

DtHoverBox.prototype.extendTimeout = function extendTimeout(T) {
  //console.log("extend", T)
  var this_ = this;
  this.stopTimeout();
  this.timeout = setTimeout(function(){this_.hide();}.bind(this), T);
}

// Bind events to a link to open this box
DtHoverBox.prototype.bind = function bind(node) {
  if (typeof node == "string"){
    node = document.querySelector(node);
  }

  node.addEventListener("mouseover", function(){
    if (!this.visible) this.showAtNode(node);
    this.stopTimeout();
  }.bind(this));

  node.addEventListener("mouseout", function(){this.extendTimeout(250);}.bind(this));

  node.addEventListener("touchstart", function(e) {
    if (this.visible) {
      this.hide();
    } else {
      this.showAtNode(node);
    }
    // Don't trigger body touchstart event when touching link
    e.stopPropagation();
  }.bind(this));
}

DtHoverBox.prototype.bindDivEvents = function bindDivEvents(){
  // For mice, same behavior as hovering on links
  this.div.addEventListener("mouseover", function(){
    if (!this.visible) this.showAtNode(node);
    this.stopTimeout();
  }.bind(this));
  this.div.addEventListener("mouseout", function(){this.extendTimeout(250);}.bind(this));

  // Don't trigger body touchstart event when touching within box
  this.div.addEventListener("touchstart", function(e){e.stopPropagation();});
  // Close box when touching outside box
  document.body.addEventListener("touchstart", function(){this.hide();}.bind(this));
}

var hover_es = document.querySelectorAll("span[data-hover-ref]");
hover_es = [].slice.apply(hover_es);
hover_es.forEach(function(e,n){
  var ref_id = e.getAttribute("data-hover-ref");
  DtHoverBox.get_box(ref_id).bind(e);
})
</script><div id="CWSE-overlay-initial"></div>	<div id="CWSE-overlay-primary"><div class="optimizer-container">	<i id="optimizer-close">X</i>	<div class="content"></div>	</div></div></body></html>